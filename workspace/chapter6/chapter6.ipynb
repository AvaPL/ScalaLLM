{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e73e9304-0b46-4a79-81f5-af4e977bf44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $file.^.Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c87b69a7-66f9-475b-9cf1-b8e88f67ec29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mzipName\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"sms+spam+collection.zip\"\u001b[39m\n",
       "\u001b[36mdatasetUrl\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\u001b[39m\n",
       "\u001b[36moutputDir\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"data/sms-spam-raw\"\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val zipName = \"sms+spam+collection.zip\"\n",
    "val datasetUrl = s\"https://archive.ics.uci.edu/static/public/228/$zipName\"\n",
    "val outputDir = \"data/sms-spam-raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "048673b9-9ae6-4d7c-8d60-60a1a383f667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 65461    0 65461    0     0  69798      0 --:--:-- --:--:-- --:--:-- 69787\n",
      "100  198k    0  198k    0     0   175k      0 --:--:--  0:00:01 --:--:--  175k\n"
     ]
    }
   ],
   "source": [
    "Magic.!(\"curl\", \"--create-dirs\", \"-O\", \"--output-dir\", outputDir, datasetUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fdc00a5-7737-4164-90b6-7914a4332ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/sms-spam-raw/sms+spam+collection.zip\n",
      "  inflating: data/sms-spam-raw/SMSSpamCollection  \n",
      "  inflating: data/sms-spam-raw/readme  \n"
     ]
    }
   ],
   "source": [
    "Magic.!(\"unzip\", \"-o\", s\"$outputDir/$zipName\", \"-d\", outputDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28347282-363e-45e5-93c9-4385a6631506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam count: 747\n",
      "Not spam count: 4827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mscala.io.Source\u001b[39m\n",
       "\u001b[36mdatasetRaw\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
       "ham\tOk lar... Joking wif u oni...\n",
       "spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
       "ham\tU dun say so early hor... U c already then say...\n",
       "ham\tNah I don't think he goes to usf, he lives around here though\n",
       "spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n",
       "ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
       "ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
       "spam\tWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
       "spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n",
       "ham\tI'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\n",
       "spam\tSIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info\n",
       "spam\tURGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\n",
       "ham\tI've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\n",
       "ham\tI HAVE A DATE ON SUNDAY WITH WILL!!\n",
       "spam\tXXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL\n",
       "ham\tOh k...i'm watching here:)\n",
       "ham\tEh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet.\n",
       "ham\tFine if thats the way u feel. Thats the way its gota b\n",
       "spam\tEngland v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/ú1.20 POBOXox36504W45WQ 16+\n",
       "ham\tIs that seriously how you spell his name?\n",
       "ham\tI‘m going to try for 2 months ha ha only joking\n",
       "\u001b[39m...\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mSmsSpamRecord\u001b[39m\n",
       "defined \u001b[32mtype\u001b[39m \u001b[36mDataset\u001b[39m\n",
       "\u001b[36msmsSpamRecords\u001b[39m: \u001b[32mDataset\u001b[39m = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\"\u001b[39m,\n",
       "    isSpam = \u001b[32mfalse\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(text = \u001b[32m\"Ok lar... Joking wif u oni...\"\u001b[39m, isSpam = \u001b[32mfalse\u001b[39m),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"\u001b[39m,\n",
       "    isSpam = \u001b[32mtrue\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"U dun say so early hor... U c already then say...\"\u001b[39m,\n",
       "    isSpam = \u001b[32mfalse\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Nah I don't think he goes to usf, he lives around here though\"\u001b[39m,\n",
       "    isSpam = \u001b[32mfalse\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\"\u001b[39m,\n",
       "    isSpam = \u001b[32mtrue\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Even my brother is not like to speak with me. They treat me like aids patent.\"\u001b[39m,\n",
       "    isSpam = \u001b[32mfalse\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\"\u001b[39m,\n",
       "    isSpam = \u001b[32mfalse\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "...\n",
       "\u001b[36mspamRecords\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mSmsSpamRecord\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"\u001b[39m,\n",
       "    isSpam = \u001b[32mtrue\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\"\u001b[39m,\n",
       "    isSpam = \u001b[32mtrue\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\"\u001b[39m,\n",
       "    isSpam = \u001b[32mtrue\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\"\u001b[39m,\n",
       "    isSpam = \u001b[32mtrue\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info\"\u001b[39m,\n",
       "    isSpam = \u001b[32mtrue\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\"\u001b[39m,\n",
       "    isSpam = \u001b[32mtrue\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCB\u001b[39m...\n",
       "\u001b[36mnotSpamRecords\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mSmsSpamRecord\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\"\u001b[39m,\n",
       "    isSpam = \u001b[32mfalse\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(text = \u001b[32m\"Ok lar... Joking wif u oni...\"\u001b[39m, isSpam = \u001b[32mfalse\u001b[39m),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"U dun say so early hor... U c already then say...\"\u001b[39m,\n",
       "    isSpam = \u001b[32mfalse\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Nah I don't think he goes to usf, he lives around here though\"\u001b[39m,\n",
       "    isSpam = \u001b[32mfalse\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Even my brother is not like to speak with me. They treat me like aids patent.\"\u001b[39m,\n",
       "    isSpam = \u001b[32mfalse\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\"\u001b[39m,\n",
       "    isSpam = \u001b[32mfalse\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\"\u001b[39m,\n",
       "    isSpam = \u001b[32mfalse\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\"\u001b[39m,\n",
       "    isSpam = \u001b[32mfalse\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(text = \u001b[32m\"I HAVE A DATE ON SUNDAY WITH WILL!!\"\u001b[39m, isSpam = \u001b[32mfalse\u001b[39m),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(text = \u001b[32m\"Oh k...i'm watching here:)\"\u001b[39m, isSpam = \u001b[32mfalse\u001b[39m),\n",
       "..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.io.Source\n",
    "\n",
    "val datasetRaw = Source.fromFile(s\"$outputDir/SMSSpamCollection\").mkString\n",
    "\n",
    "case class SmsSpamRecord(\n",
    "  text: String,\n",
    "  isSpam: Boolean\n",
    ")\n",
    "\n",
    "type Dataset = Vector[SmsSpamRecord]\n",
    "\n",
    "val smsSpamRecords: Dataset = datasetRaw.split(\"\\n\").map {\n",
    "  case s\"spam\\t$text\" => SmsSpamRecord(text, isSpam = true)\n",
    "  case s\"ham\\t$text\" => SmsSpamRecord(text, isSpam = false)\n",
    "}.toVector\n",
    "\n",
    "val (spamRecords, notSpamRecords) = smsSpamRecords.partition(_.isSpam)\n",
    "println(s\"Spam count: ${spamRecords.size}\")\n",
    "println(s\"Not spam count: ${notSpamRecords.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e244291-9733-4821-a534-f6cd331a4ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.mutable\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Random\u001b[39m\n",
       "\u001b[36mbalancedDataset\u001b[39m: \u001b[32mDataset\u001b[39m = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"\u001b[39m,\n",
       "    isSpam = \u001b[32mtrue\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\"\u001b[39m,\n",
       "    isSpam = \u001b[32mtrue\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\"\u001b[39m,\n",
       "    isSpam = \u001b[32mtrue\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\"\u001b[39m,\n",
       "    isSpam = \u001b[32mtrue\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info\"\u001b[39m,\n",
       "    isSpam = \u001b[32mtrue\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\"\u001b[39m,\n",
       "    isSpam = \u001b[32mtrue\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCB\u001b[39m..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable\n",
    "import scala.util.Random\n",
    "\n",
    "val balancedDataset: Dataset = {\n",
    "\n",
    "  def sample(records: Vector[SmsSpamRecord], targetSize: Int): Vector[SmsSpamRecord] = {\n",
    "    val balancedDatasetSpam = mutable.Map[String, SmsSpamRecord]()\n",
    "    while (balancedDatasetSpam.size < targetSize) {\n",
    "      val randomRecord = records(Random.nextInt(records.size))\n",
    "      if (!balancedDatasetSpam.contains(randomRecord.text))\n",
    "        balancedDatasetSpam += randomRecord.text -> randomRecord\n",
    "    }\n",
    "    balancedDatasetSpam.values.toVector\n",
    "  }\n",
    "\n",
    "  if (spamRecords.size < notSpamRecords.size)\n",
    "    spamRecords ++ sample(notSpamRecords, targetSize = spamRecords.size)\n",
    "  else\n",
    "    notSpamRecords ++ sample(spamRecords, targetSize = notSpamRecords.size)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cba96609-d9fa-440e-9556-b3d0f3b104d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mtype\u001b[39m \u001b[36mTraining\u001b[39m\n",
       "defined \u001b[32mtype\u001b[39m \u001b[36mValidation\u001b[39m\n",
       "defined \u001b[32mtype\u001b[39m \u001b[36mTest\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mrandomSplit\u001b[39m\n",
       "\u001b[36mtraining\u001b[39m: \u001b[32mTraining\u001b[39m = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(text = \u001b[32m\"Ugh just got outta class\"\u001b[39m, isSpam = \u001b[32mfalse\u001b[39m),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Sorry da thangam, very very sorry i am held up with prasad.\"\u001b[39m,\n",
       "    isSpam = \u001b[32mfalse\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"As a valued customer, I am pleased to advise you that following recent review of your Mob No. you are awarded with a £1500 Bonus Prize, call 09066368470\"\u001b[39m,\n",
       "    isSpam = \u001b[32mtrue\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(text = \u001b[32m\"Bugis oso near wat... \"\u001b[39m, isSpam = \u001b[32mfalse\u001b[39m),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Hi I'm sue. I am 20 years old and work as a lapdancer. I love sex. Text me live - I'm i my bedroom now. text SUE to 89555. By TextOperator G2 1DA 150ppmsg 18+\"\u001b[39m,\n",
       "    isSpam = \u001b[32mtrue\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Please CALL 08712402578 immediately as there is an urgent message waiting for you\"\u001b[39m,\n",
       "    isSpam = \u001b[32mtrue\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(text = \u001b[32m\"No my mum went 2 dentist.\"\u001b[39m, isSpam = \u001b[32mfalse\u001b[39m),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"\\\"AH POOR BABY!HOPE URFEELING BETTERSN LUV! PROBTHAT OVERDOSE OF WORK HEY GO CAREFUL SPK 2 U SN LOTS OF LOVEJEN XXX.\\\"\"\u001b[39m,\n",
       "    isSpam = \u001b[32mfalse\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(text = \u001b[32m\"I need you to be in my strong arms...\"\u001b[39m, isSpam = \u001b[32mfalse\u001b[39m),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Storming msg: Wen u lift d phne, u say \\\"HELLO\\\" Do u knw wt is d real meaning of HELLO?? . . . It's d name of a girl..! . . . Yes.. And u knw who is dat girl?? \\\"Margaret Hello\\\" She is d girlfrnd f Grahmbell who invnted telphone... . . . . Moral:One can 4get d name of a person, bt not his girlfrnd... G o o d n i g h t . . .@\"\u001b[39m,\n",
       "    isSpam = \u001b[32mfalse\u001b[39m\n",
       "  ),\n",
       "...\n",
       "\u001b[36mvalidation\u001b[39m: \u001b[32mValidation\u001b[39m = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Last Chance! Claim ur £150 worth of discount vouchers today! Text SHOP to 85023 now! SavaMob, offers mobile! T Cs SavaMob POBOX84, M263UZ. £3.00 Sub. 16\"\u001b[39m,\n",
       "    isSpam = \u001b[32mtrue\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Ffffffffff. Alright no way I can meet up with you sooner?\"\u001b[39m,\n",
       "    isSpam = \u001b[32mfalse\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Promotion Number: 8714714 - UR awarded a City Break and could WIN a £200 Summer Shopping spree every WK. Txt STORE to 88039 . SkilGme. TsCs087147403231Winawk!Age16 £1.50perWKsub\"\u001b[39m,\n",
       "    isSpam = \u001b[32mtrue\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Can you please send me my aunty's number\"\u001b[39m,\n",
       "    isSpam = \u001b[32mfalse\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(text = \u001b[32m\"COME BACK TO TAMPA FFFFUUUUUUU\"\u001b[39m, isSpam = \u001b[32mfalse\u001b[39m),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(text = \u001b[32m\"Can you talk with me..\"\u001b[39m, isSpam = \u001b[32mfalse\u001b[39m),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Thanks for your Ringtone Order, Reference T91. You will be charged GBP 4 per week. You can unsubscribe at anytime by calling customer services on 09057039994\"\u001b[39m,\n",
       "    isSpam = \u001b[32mtrue\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"What I'm saying is if you haven't explicitly told nora I know someone I'm probably just not gonna bother\"\u001b[39m,\n",
       "    isSpam = \u001b[32mfalse\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Is ur changes 2 da report big? Cos i've already made changes 2 da previous report.\"\u001b[39m,\n",
       "    isSpam = \u001b[32mfalse\u001b[39m\n",
       "  ),\n",
       "...\n",
       "\u001b[36mtest\u001b[39m: \u001b[32mTest\u001b[39m = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Sunshine Quiz Wkly Q! Win a top Sony DVD player if u know which country Liverpool played in mid week? Txt ansr to 82277. £1.50 SP:Tyrone\"\u001b[39m,\n",
       "    isSpam = \u001b[32mtrue\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(text = \u001b[32m\"Depends on where u going lor.\"\u001b[39m, isSpam = \u001b[32mfalse\u001b[39m),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(text = \u001b[32m\"Doing project w frens lor. \"\u001b[39m, isSpam = \u001b[32mfalse\u001b[39m),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"I haven't forgotten you, i might have a couple bucks to send you tomorrow, k? I love ya too\"\u001b[39m,\n",
       "    isSpam = \u001b[32mfalse\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Thanks for your ringtone order, reference number X49.Your mobile will be charged 4.50. Should your tone not arrive please call customer services 09065989182\"\u001b[39m,\n",
       "    isSpam = \u001b[32mtrue\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Well done ENGLAND! Get the official poly ringtone or colour flag on yer mobile! text TONE or FLAG to 84199 NOW! Opt-out txt ENG STOP. Box39822 W111WX £1.50\"\u001b[39m,\n",
       "    isSpam = \u001b[32mtrue\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"Lol you won't feel bad when I use her money to take you out to a steak dinner =D\"\u001b[39m,\n",
       "    isSpam = \u001b[32mfalse\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(text = \u001b[32m\"U sick still can go shopping?\"\u001b[39m, isSpam = \u001b[32mfalse\u001b[39m),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"PRIVATE! Your 2003 Account Statement for <fone no> shows 800 un-redeemed S. I. M. points. Call 08715203656 Identifier Code: 42049 Expires 26/10/04\"\u001b[39m,\n",
       "    isSpam = \u001b[32mtrue\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mSmsSpamRecord\u001b[39m(\n",
       "    text = \u001b[32m\"URGENT! Your Mobile number has been awarded with a £2000 prize GUARANTEED. Call 09058094455 from land line. Claim 3030. Valid 12hrs only\"\u001b[39m,\n",
       "..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type Training = Dataset\n",
    "type Validation = Dataset\n",
    "type Test = Dataset\n",
    "\n",
    "def randomSplit(dataset: Vector[SmsSpamRecord], trainingFraction: Double, validationFraction: Double): (Training, Validation, Test) = {\n",
    "  val shuffledDataset = Random.shuffle(dataset)\n",
    "  val trainingSize = (shuffledDataset.size * trainingFraction).floor.toInt\n",
    "  val validationSize = (shuffledDataset.size * validationFraction).floor.toInt\n",
    "\n",
    "  val (training, remainingRecords) = shuffledDataset.splitAt(trainingSize)\n",
    "  val (validation, test) = remainingRecords.splitAt(validationSize)\n",
    "  (training, validation, test)\n",
    "}\n",
    "\n",
    "val (training, validation, test) = randomSplit(balancedDataset, trainingFraction = 0.7, validationFraction = 0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c33bb0b-ea20-4a02-8606-a28dab16f9b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Using\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.github.tototoshi.csv.CSVWriter\u001b[39m\n",
       "\u001b[36mtextHeader\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Text\"\u001b[39m\n",
       "\u001b[36mlabelHeader\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Label\"\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mwriteToCsv\u001b[39m\n",
       "\u001b[36mtrainingCsv\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"data/training.csv\"\u001b[39m\n",
       "\u001b[36mvalidationCsv\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"data/validation.csv\"\u001b[39m\n",
       "\u001b[36mtestCsv\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"data/test.csv\"\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`com.github.tototoshi::scala-csv:2.0.0`\n",
    "\n",
    "import scala.util.Using\n",
    "import com.github.tototoshi.csv.CSVWriter\n",
    "\n",
    "val textHeader = \"Text\"\n",
    "val labelHeader = \"Label\"\n",
    "\n",
    "def writeToCsv(path: String, dataset: Dataset): Unit = {\n",
    "  val headers = Vector(textHeader, labelHeader)\n",
    "\n",
    "  Using.resource(CSVWriter.open(path)) { writer =>\n",
    "    val rows = dataset.map {\n",
    "      case SmsSpamRecord(text, isSpam) => Vector(text, if (isSpam) \"1\" else \"0\")\n",
    "    }\n",
    "    writer.writeAll(headers +: rows)\n",
    "  }\n",
    "}\n",
    "\n",
    "val trainingCsv = \"data/training.csv\"\n",
    "writeToCsv(trainingCsv, training)\n",
    "val validationCsv = \"data/validation.csv\"\n",
    "writeToCsv(validationCsv, validation)\n",
    "val testCsv = \"data/test.csv\"\n",
    "writeToCsv(testCsv, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2ba9697-4121-41ec-ad86-4ca468d8caad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken==0.7.* in /usr/local/lib/python3.12/site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/site-packages (from tiktoken==0.7.*) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/site-packages (from tiktoken==0.7.*) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.7.*) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.7.*) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.7.*) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.7.*) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "Magic.!(\"pip\", \"install\", \"tiktoken==0.7.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ac863ee-648b-40f4-bb1d-275a7b2657bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mme.shadaj.scalapy.py\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mpy.SeqConverters\u001b[39m\n",
       "\u001b[36mtiktoken\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mModule\u001b[39m = <module 'tiktoken' from '/usr/local/lib/python3.12/site-packages/tiktoken/__init__.py'>\n",
       "\u001b[36mtokenizer\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <Encoding 'gpt2'>\n",
       "\u001b[36mendOfTextToken\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"<|endoftext|>\"\u001b[39m\n",
       "\u001b[36mencodedEndOfTextToken\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = [50256]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`dev.scalapy::scalapy-core:0.5.3`\n",
    "\n",
    "import me.shadaj.scalapy.py\n",
    "import py.SeqConverters\n",
    "\n",
    "val tiktoken = py.module(\"tiktoken\")\n",
    "\n",
    "val tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "val endOfTextToken = \"<|endoftext|>\"\n",
    "val encodedEndOfTextToken = tokenizer.encode(endOfTextToken, allowed_special = py.Dynamic.global.set(Seq(endOfTextToken).toPythonProxy))\n",
    "println(encodedEndOfTextToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67d18733-4b85-41ed-8717-07f261b5ab86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.4.* in /usr/local/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (75.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->torch==2.4.*) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/site-packages (from sympy->torch==2.4.*) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "Magic.!(\"pip\", \"install\", \"torch==2.4.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "910a813f-f2c6-4410-9c73-8687f41294ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mcom.github.tototoshi.csv.CSVReader\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mpy.PyQuote\u001b[39m\n",
       "\u001b[36mtorch\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mModule\u001b[39m = <module 'torch' from '/usr/local/lib/python3.12/site-packages/torch/__init__.py'>\n",
       "defined \u001b[32mtype\u001b[39m \u001b[36mTokenizer\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mSpamDataset\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.github.tototoshi.csv.CSVReader\n",
    "import py.PyQuote\n",
    "\n",
    "val torch = py.module(\"torch\")\n",
    "\n",
    "type Tokenizer = py.Dynamic\n",
    "\n",
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"from torch.utils.data import Dataset\n",
    "     |\n",
    "     |class SpamDataset(Dataset):\n",
    "     |  def __init__(self, init):\n",
    "     |    init(self)\n",
    "     |\n",
    "     |  def __getitem__(self, index):\n",
    "     |    return self.getItem(index)\n",
    "     |  \n",
    "     |  def __len__(self):\n",
    "     |    return self.len()\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def SpamDataset(\n",
    "  csvPath: String,\n",
    "  tokenizer: Tokenizer,\n",
    "  maxLength: Option[Int] = None,\n",
    "  paddingTokenId: Int = 50_256\n",
    "): py.Dynamic = {\n",
    "  val smsSpamRecords = Using.resource(CSVReader.open(csvPath)) { csvReader =>\n",
    "    csvReader.iteratorWithHeaders.map { row =>\n",
    "      SmsSpamRecord(text = row(textHeader), isSpam = row(labelHeader).toInt > 0)\n",
    "    }.toVector\n",
    "  }\n",
    "  val encodedTexts = {\n",
    "    val encodedTexts = smsSpamRecords.map(_.text).map(tokenizer.encode(_).as[Seq[Int]].toVector)\n",
    "    val padToLength = maxLength.getOrElse(encodedTexts.map(_.length).max)\n",
    "    encodedTexts.map(_.padTo(padToLength, paddingTokenId))\n",
    "  }\n",
    "    \n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.maxLength = encodedTexts.head.length\n",
    "    \n",
    "    val getItem = (index: Int) => {\n",
    "      val textTensor = torch.tensor(encodedTexts(index).toPythonProxy, dtype = torch.long)\n",
    "      val labelTensor = torch.tensor(if (smsSpamRecords(index).isSpam) 1 else 0, dtype = torch.long)\n",
    "      (textTensor, labelTensor)\n",
    "    }\n",
    "    self.getItem = getItem\n",
    "\n",
    "    val len = () => smsSpamRecords.size\n",
    "    self.len = len\n",
    "  }\n",
    "  py.Dynamic.global.SpamDataset(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3db6947e-949f-465d-90f3-2588cd796fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainingDataset\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <SpamDataset object at 0xffff4c3ad280>\n",
       "\u001b[36mvalidationDataset\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <SpamDataset object at 0xffff3d31c200>\n",
       "\u001b[36mtestDataset\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <SpamDataset object at 0xffff3d31f5f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainingDataset = SpamDataset(trainingCsv, tokenizer)\n",
    "val validationDataset = SpamDataset(validationCsv, tokenizer, maxLength = Some(trainingDataset.maxLength.as[Int]))\n",
    "val testDataset = SpamDataset(testCsv, tokenizer, maxLength = Some(trainingDataset.maxLength.as[Int]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9939eb92-a199-4019-91c4-67833cc92e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mbatchSize\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m8\u001b[39m\n",
       "\u001b[36mres14_1\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch._C.Generator object at 0xffff4ff2c790>\n",
       "\u001b[36mtrainingDataLoader\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch.utils.data.dataloader.DataLoader object at 0xffff3f3f3e90>\n",
       "\u001b[36mvalidationDataLoader\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch.utils.data.dataloader.DataLoader object at 0xffff58175cd0>\n",
       "\u001b[36mtestDataLoader\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch.utils.data.dataloader.DataLoader object at 0xffff3d78bd40>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val batchSize = 8\n",
    "torch.manual_seed(123)\n",
    "\n",
    "val trainingDataLoader = torch.utils.data.DataLoader(\n",
    "  dataset = trainingDataset, \n",
    "  batch_size = batchSize,\n",
    "  shuffle = true,\n",
    "  num_workers = 0,\n",
    "  drop_last = true\n",
    ")\n",
    "val validationDataLoader = torch.utils.data.DataLoader(\n",
    "  dataset = validationDataset, \n",
    "  batch_size = batchSize,\n",
    "  num_workers = 0,\n",
    "  drop_last = false\n",
    ")\n",
    "val testDataLoader = torch.utils.data.DataLoader(\n",
    "  dataset = testDataset, \n",
    "  batch_size = batchSize,\n",
    "  num_workers = 0,\n",
    "  drop_last = false\n",
    ")\n",
    "\n",
    "println(s\"${py.Dynamic.global.len(trainingDataLoader)} training batches\")\n",
    "println(s\"${py.Dynamic.global.len(validationDataLoader)} validation batches\")\n",
    "println(s\"${py.Dynamic.global.len(testDataLoader)} test batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26d34644-7ac9-4277-9ae4-b6aaa38bc6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mGPTConfig\u001b[39m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class GPTConfig(\n",
    "  vocabularySize: Int,\n",
    "  contextLength: Int,\n",
    "  embeddingDimension: Int,\n",
    "  attentionHeadsCount: Int,\n",
    "  layersCount: Int,\n",
    "  dropoutRate: Double,\n",
    "  queryKeyValueBias: Boolean\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2dfd43d-f02c-4c1a-8eb3-817c92b539a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mtype\u001b[39m \u001b[36mTorchTensor\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mMultiHeadAttention\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type TorchTensor = py.Dynamic\n",
    "\n",
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class MultiHeadAttention(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def MultiHeadAttention(\n",
    "  inputDimension: Int,\n",
    "  outputDimension: Int,\n",
    "  dropoutProbability: Double,\n",
    "  contextLength: Int,\n",
    "  headsCount: Int,\n",
    "  queryKeyValueBias: Boolean\n",
    "): py.Dynamic = {\n",
    "  assert(outputDimension % headsCount == 0, \"Output dimension must be a multiple of heads count\")\n",
    "  val headDimension = outputDimension / headsCount\n",
    "    \n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.weightsQuery = torch.nn.Linear(inputDimension, outputDimension, bias = queryKeyValueBias)\n",
    "    self.weightsKey = torch.nn.Linear(inputDimension, outputDimension, bias = queryKeyValueBias)\n",
    "    self.weightsValue = torch.nn.Linear(inputDimension, outputDimension, bias = queryKeyValueBias)\n",
    "    self.outputProjection = torch.nn.Linear(outputDimension, outputDimension)\n",
    "    self.dropout = torch.nn.Dropout(dropoutProbability)\n",
    "    self.register_buffer(\"mask\", torch.triu(torch.ones(contextLength, contextLength), diagonal = 1))\n",
    "      \n",
    "    val forward = (batchedInputs: TorchTensor) => {\n",
    "      val (batchesCount, tokensCount, tokenDimension) = batchedInputs.shape.as[(Int, Int, Int)]\n",
    "      val queries = self.weightsQuery(batchedInputs)\n",
    "        .view(batchesCount, tokensCount, headsCount, headDimension)\n",
    "        .transpose(1, 2)\n",
    "      val keys = self.weightsKey(batchedInputs)\n",
    "        .view(batchesCount, tokensCount, headsCount, headDimension)\n",
    "        .transpose(1, 2)\n",
    "      val values = self.weightsValue(batchedInputs)\n",
    "        .view(batchesCount, tokensCount, headsCount, headDimension)\n",
    "        .transpose(1, 2)\n",
    "      val attentionScores = py\"$queries @ $keys.transpose(2, 3)\"\n",
    "      attentionScores.masked_fill_(py\"${self.mask}.bool()[:$tokensCount, :$tokensCount]\", -torch.inf)\n",
    "      val attentionWeights = self.dropout(torch.softmax(py\"$attentionScores / $headDimension**0.5\", dim = -1))\n",
    "      self.outputProjection(\n",
    "        py\"$attentionWeights @ $values\"\n",
    "          .transpose(1, 2)\n",
    "          .reshape(batchesCount, tokensCount, outputDimension)\n",
    "      )\n",
    "    }\n",
    "    self.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.MultiHeadAttention(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7799670d-e179-4188-b8c9-d9393033d141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mGELU\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "// Because it mostly uses Python operators, it's implemented fully in Python\n",
    "py.exec {\n",
    "  s\"\"\"import torch\n",
    "     |import torch.nn as nn\n",
    "     |\n",
    "     |class GELU(nn.Module):\n",
    "     |  def __init__(self):\n",
    "     |    super().__init__()\n",
    "     |\n",
    "     |  def forward(self, inputs):\n",
    "     |    return 0.5 * inputs * (\n",
    "     |      1 + torch.tanh(\n",
    "     |        torch.sqrt(torch.tensor(2.0 / torch.pi)) * (inputs + 0.044715 * torch.pow(inputs, 3))\n",
    "     |      )\n",
    "     |    )\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def GELU() = py.Dynamic.global.GELU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cb61e38-8be9-4729-9e86-6c90eb8de235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mFeedForward\u001b[39m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class FeedForward(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def FeedForward(\n",
    "  embeddingDimension: Int\n",
    "): py.Dynamic = {\n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.layers = torch.nn.Sequential(\n",
    "      torch.nn.Linear(embeddingDimension, 4 * embeddingDimension),\n",
    "      GELU(),\n",
    "      torch.nn.Linear(4 * embeddingDimension, embeddingDimension)\n",
    "    )\n",
    "      \n",
    "    val forward = (inputs: TorchTensor) => self.layers(inputs)\n",
    "    self.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.FeedForward(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3a80736-2a99-4310-b59f-20edb7bbac85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mNormalizationLayer\u001b[39m"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class NormalizationLayer(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def NormalizationLayer(\n",
    "  embeddingDimension: Int\n",
    "): py.Dynamic = {\n",
    "  val epsilon = 1e-5\n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.scale = torch.nn.Parameter(torch.ones(embeddingDimension))\n",
    "    self.shift = torch.nn.Parameter(torch.zeros(embeddingDimension))\n",
    "      \n",
    "    val forward = (inputs: TorchTensor) => {\n",
    "      val mean = inputs.mean(dim = -1, keepdim = true)\n",
    "      val variance = inputs.`var`(dim = -1, keepdim = true, unbiased = false)\n",
    "      val normalizedInputs = py\"($inputs - $mean) / torch.sqrt($variance + $epsilon)\"\n",
    "      py\"${self.scale} * $normalizedInputs + ${self.shift}\"\n",
    "    }\n",
    "    self.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.NormalizationLayer(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1b07cba-e201-4209-9980-21948569c2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.chaining._\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mTransformerBlock\u001b[39m"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.chaining._\n",
    "\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class TransformerBlock(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def TransformerBlock(\n",
    "  config: GPTConfig\n",
    "): py.Dynamic = {\n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.multiHeadAttention = MultiHeadAttention(\n",
    "      inputDimension = config.embeddingDimension,\n",
    "      outputDimension = config.embeddingDimension,\n",
    "      dropoutProbability = config.dropoutRate,\n",
    "      contextLength = config.contextLength,\n",
    "      headsCount = config.attentionHeadsCount,\n",
    "      queryKeyValueBias = config.queryKeyValueBias\n",
    "    )\n",
    "    self.feedForward = FeedForward(config.embeddingDimension)\n",
    "    self.normalization1 = NormalizationLayer(config.embeddingDimension)\n",
    "    self.normalization2 = NormalizationLayer(config.embeddingDimension)\n",
    "    self.dropoutShortcut = torch.nn.Dropout(config.dropoutRate)\n",
    "    \n",
    "    val forward = (inputs: TorchTensor) => {\n",
    "      val shortcut = inputs\n",
    "      val newShortcut = inputs\n",
    "        .pipe(self.normalization1(_))\n",
    "        .pipe(self.multiHeadAttention(_))\n",
    "        .pipe(self.dropoutShortcut(_))\n",
    "        .pipe(o => py\"$o + $shortcut\")\n",
    "      newShortcut\n",
    "        .pipe(self.normalization2(_))\n",
    "        .pipe(self.feedForward(_))\n",
    "        .pipe(self.dropoutShortcut(_))\n",
    "        .pipe(o => py\"$o + $newShortcut\")\n",
    "    }\n",
    "    self.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.TransformerBlock(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2974f19c-5b76-4ed2-a6d7-5bca875ddaea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mtype\u001b[39m \u001b[36mModel\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mGPTModel\u001b[39m"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class GPTModel(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "type Model = py.Dynamic\n",
    "def GPTModel(\n",
    "  config: GPTConfig\n",
    "): Model = {\n",
    "  val transformerBlocks = Seq.fill(config.layersCount)(TransformerBlock(config))\n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.tokenEmbeddingLayer = torch.nn.Embedding(config.vocabularySize, config.embeddingDimension)\n",
    "    self.positionEmbeddingLayer = torch.nn.Embedding(config.contextLength, config.embeddingDimension)\n",
    "    self.dropoutEmbeddingLayer = torch.nn.Dropout(config.dropoutRate)\n",
    "    self.transformerBlocksLayer = py\"nn.Sequential(*${transformerBlocks.toPythonProxy})\"\n",
    "    self.finalNormalizationLayer = NormalizationLayer(config.embeddingDimension)\n",
    "    self.outputLayer = torch.nn.Linear(config.embeddingDimension, config.vocabularySize, bias = false)\n",
    "      \n",
    "    val forward = (batchedInputs: TorchTensor) => {\n",
    "      val (_, sequenceLength) = batchedInputs.shape.as[(Int, Int)]\n",
    "      val tokenEmbeddings = self.tokenEmbeddingLayer(batchedInputs)\n",
    "      val positionEmbeddings = self.positionEmbeddingLayer(torch.arange(sequenceLength, device = batchedInputs.device))\n",
    "      py\"$tokenEmbeddings + $positionEmbeddings\"\n",
    "        .pipe(self.dropoutEmbeddingLayer(_))\n",
    "        .pipe(self.transformerBlocksLayer(_))\n",
    "        .pipe(self.finalNormalizationLayer(_))\n",
    "        .pipe(self.outputLayer(_))\n",
    "    }\n",
    "    self.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.GPTModel(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e2bb752-7eb2-4fcb-9e0d-425bbb51f8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mbaseUrl\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2/124M\"\u001b[39m\n",
       "\u001b[36mhparamsFilename\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"hparams.json\"\u001b[39m\n",
       "\u001b[36mfilenames\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[32m\"checkpoint\"\u001b[39m,\n",
       "  \u001b[32m\"encoder.json\"\u001b[39m,\n",
       "  \u001b[32m\"hparams.json\"\u001b[39m,\n",
       "  \u001b[32m\"model.ckpt.data-00000-of-00001\"\u001b[39m,\n",
       "  \u001b[32m\"model.ckpt.index\"\u001b[39m,\n",
       "  \u001b[32m\"model.ckpt.meta\"\u001b[39m,\n",
       "  \u001b[32m\"vocab.bpe\"\u001b[39m\n",
       ")\n",
       "\u001b[36moutputDir\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"data/openai124M\"\u001b[39m"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val baseUrl = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2/124M\" // backup\n",
    "// val baseUrl = \"https://openaipublic.blob.core.windows.net/gpt-2/models/124M\" // backup\n",
    "val hparamsFilename = \"hparams.json\"\n",
    "val filenames = List(\"checkpoint\", \"encoder.json\", hparamsFilename, \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\", \"model.ckpt.meta\", \"vocab.bpe\")\n",
    "\n",
    "val outputDir = \"data/openai124M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff93551d-b4f8-42e8-8ff7-9394cb75125b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100    77  100    77    0     0    112      0 --:--:-- --:--:-- --:--:--   112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading encoder.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  4 1017k    4 48714    0     0  48908      0  0:00:21 --:--:--  0:00:21 48909\n",
      "100 1017k  100 1017k    0     0   565k      0  0:00:01  0:00:01 --:--:--  566k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading hparams.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100    90  100    90    0     0    138      0 --:--:-- --:--:-- --:--:--   138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model.ckpt.data-00000-of-00001...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0  474M    0  439k    0     0   292k      0  0:27:41  0:00:01  0:27:40  292k\n",
      "  2  474M    2 10.0M    0     0  4095k      0  0:01:58  0:00:02  0:01:56 4095k\n",
      "  5  474M    5 25.5M    0     0  7437k      0  0:01:05  0:00:03  0:01:02 7435k\n",
      "  8  474M    8 41.6M    0     0  9405k      0  0:00:51  0:00:04  0:00:47 9404k\n",
      " 12  474M   12 58.1M    0     0  10.4M      0  0:00:45  0:00:05  0:00:40 11.6M\n",
      " 15  474M   15 74.0M    0     0  11.2M      0  0:00:42  0:00:06  0:00:36 14.5M\n",
      " 19  474M   19 90.5M    0     0  11.9M      0  0:00:39  0:00:07  0:00:32 15.9M\n",
      " 22  474M   22  106M    0     0  12.4M      0  0:00:38  0:00:08  0:00:30 16.1M\n",
      " 25  474M   25  121M    0     0  12.7M      0  0:00:37  0:00:09  0:00:28 16.0M\n",
      " 28  474M   28  136M    0     0  12.9M      0  0:00:36  0:00:10  0:00:26 15.8M\n",
      " 32  474M   32  152M    0     0  13.2M      0  0:00:35  0:00:11  0:00:24 15.8M\n",
      " 34  474M   34  163M    0     0  12.9M      0  0:00:36  0:00:12  0:00:24 14.3M\n",
      " 37  474M   37  178M    0     0  13.2M      0  0:00:35  0:00:13  0:00:22 14.4M\n",
      " 40  474M   40  194M    0     0  13.3M      0  0:00:35  0:00:14  0:00:21 14.4M\n",
      " 44  474M   44  211M    0     0  13.5M      0  0:00:34  0:00:15  0:00:19 14.8M\n",
      " 48  474M   48  228M    0     0  13.7M      0  0:00:34  0:00:16  0:00:18 15.0M\n",
      " 51  474M   51  244M    0     0  13.9M      0  0:00:34  0:00:17  0:00:17 16.6M\n",
      " 55  474M   55  261M    0     0  14.1M      0  0:00:33  0:00:18  0:00:15 16.5M\n",
      " 58  474M   58  276M    0     0  14.1M      0  0:00:33  0:00:19  0:00:14 16.4M\n",
      " 61  474M   61  291M    0     0  14.1M      0  0:00:33  0:00:20  0:00:13 16.1M\n",
      " 64  474M   64  307M    0     0  14.2M      0  0:00:33  0:00:21  0:00:12 16.0M\n",
      " 68  474M   68  323M    0     0  14.3M      0  0:00:32  0:00:22  0:00:10 15.8M\n",
      " 72  474M   72  343M    0     0  14.5M      0  0:00:32  0:00:23  0:00:09 16.0M\n",
      " 75  474M   75  358M    0     0  14.5M      0  0:00:32  0:00:24  0:00:08 16.0M\n",
      " 78  474M   78  373M    0     0  14.5M      0  0:00:32  0:00:25  0:00:07 16.1M\n",
      " 81  474M   81  388M    0     0  14.6M      0  0:00:32  0:00:26  0:00:06 16.1M\n",
      " 85  474M   85  405M    0     0  14.7M      0  0:00:32  0:00:27  0:00:05 16.3M\n",
      " 88  474M   88  421M    0     0  14.7M      0  0:00:32  0:00:28  0:00:04 16.0M\n",
      " 92  474M   92  438M    0     0  14.8M      0  0:00:31  0:00:29  0:00:02 16.3M\n",
      " 95  474M   95  454M    0     0  14.8M      0  0:00:31  0:00:30  0:00:01 16.4M\n",
      " 99  474M   99  472M    0     0  14.9M      0  0:00:31  0:00:31 --:--:-- 16.6M\n",
      "100  474M  100  474M    0     0  14.9M      0  0:00:31  0:00:31 --:--:-- 16.5M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model.ckpt.index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  5215  100  5215    0     0   8799      0 --:--:-- --:--:-- --:--:--  8809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model.ckpt.meta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      " 19  460k   19 89664    0     0  73720      0  0:00:06  0:00:01  0:00:05 73676\n",
      "100  460k  100  460k    0     0   270k      0  0:00:01  0:00:01 --:--:--  270k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading vocab.bpe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      " 78  445k   78  351k    0     0   235k      0  0:00:01  0:00:01 --:--:--  235k\n",
      "100  445k  100  445k    0     0   279k      0  0:00:01  0:00:01 --:--:--  279k\n"
     ]
    }
   ],
   "source": [
    "filenames.foreach { filename =>\n",
    "  println(s\"Downloading $filename...\")\n",
    "  Magic.!(\"curl\", \"--create-dirs\", \"-O\", \"--output-dir\", outputDir, s\"$baseUrl/$filename\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba140fe2-fc98-46cb-b3aa-5f76b94e332c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.16.* in /usr/local/lib/python3.12/site-packages (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (25.1.24)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (4.25.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.16.*) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow==2.16.*) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow==2.16.*) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow==2.16.*) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.*) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.*) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.*) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.*) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.*) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.*) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.*) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.*) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.*) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.*) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.*) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "Magic.!(\"pip\", \"install\", \"tensorflow==2.16.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17d95f90-6312-4cab-9434-73d340ee1a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.io.Source\u001b[39m\n",
       "\u001b[36mhparamsMap\u001b[39m: \u001b[32mujson\u001b[39m.\u001b[32mValue\u001b[39m.\u001b[32mValue\u001b[39m = \u001b[33mObj\u001b[39m(\n",
       "  value = \u001b[33mMap\u001b[39m(\n",
       "    \u001b[32m\"n_vocab\"\u001b[39m -> \u001b[33mNum\u001b[39m(value = \u001b[32m50257.0\u001b[39m),\n",
       "    \u001b[32m\"n_ctx\"\u001b[39m -> \u001b[33mNum\u001b[39m(value = \u001b[32m1024.0\u001b[39m),\n",
       "    \u001b[32m\"n_embd\"\u001b[39m -> \u001b[33mNum\u001b[39m(value = \u001b[32m768.0\u001b[39m),\n",
       "    \u001b[32m\"n_head\"\u001b[39m -> \u001b[33mNum\u001b[39m(value = \u001b[32m12.0\u001b[39m),\n",
       "    \u001b[32m\"n_layer\"\u001b[39m -> \u001b[33mNum\u001b[39m(value = \u001b[32m12.0\u001b[39m)\n",
       "  )\n",
       ")\n",
       "\u001b[36mgptConfig\u001b[39m: \u001b[32mGPTConfig\u001b[39m = \u001b[33mGPTConfig\u001b[39m(\n",
       "  vocabularySize = \u001b[32m50257\u001b[39m,\n",
       "  contextLength = \u001b[32m1024\u001b[39m,\n",
       "  embeddingDimension = \u001b[32m768\u001b[39m,\n",
       "  attentionHeadsCount = \u001b[32m12\u001b[39m,\n",
       "  layersCount = \u001b[32m12\u001b[39m,\n",
       "  dropoutRate = \u001b[32m0.1\u001b[39m,\n",
       "  queryKeyValueBias = \u001b[32mtrue\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`com.lihaoyi::ujson:4.1.0`\n",
    "\n",
    "import scala.io.Source\n",
    "\n",
    "val hparamsMap = ujson.read(Source.fromFile(s\"$outputDir/$hparamsFilename\").mkString)\n",
    "\n",
    "val gptConfig = GPTConfig(\n",
    "  vocabularySize = hparamsMap(\"n_vocab\").num.toInt,\n",
    "  contextLength = hparamsMap(\"n_ctx\").num.toInt,\n",
    "  embeddingDimension = hparamsMap(\"n_embd\").num.toInt,\n",
    "  attentionHeadsCount = hparamsMap(\"n_head\").num.toInt,\n",
    "  layersCount = hparamsMap(\"n_layer\").num.toInt,\n",
    "  dropoutRate = 0.1,\n",
    "  queryKeyValueBias = true\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc8b8a66-3bbe-41d2-a719-8fbe8c1023cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtf\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mModule\u001b[39m = <module 'tensorflow' from '/usr/local/lib/python3.12/site-packages/tensorflow/__init__.py'>\n",
       "\u001b[36mnp\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mModule\u001b[39m = <module 'numpy' from '/usr/local/lib/python3.12/site-packages/numpy/__init__.py'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tf = py.module(\"tensorflow\")\n",
    "val np = py.module(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a353053b-7a30-45b9-8c63-6166a708a832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/h0/attn/c_attn/b\n",
      "model/h0/attn/c_attn/w\n",
      "model/h0/attn/c_proj/b\n",
      "model/h0/attn/c_proj/w\n",
      "model/h0/ln_1/b\n",
      "model/h0/ln_1/g\n",
      "model/h0/ln_2/b\n",
      "model/h0/ln_2/g\n",
      "model/h0/mlp/c_fc/b\n",
      "model/h0/mlp/c_fc/w\n",
      "model/h0/mlp/c_proj/b\n",
      "model/h0/mlp/c_proj/w\n",
      "model/h1/attn/c_attn/b\n",
      "model/h1/attn/c_attn/w\n",
      "model/h1/attn/c_proj/b\n",
      "model/h1/attn/c_proj/w\n",
      "model/h1/ln_1/b\n",
      "model/h1/ln_1/g\n",
      "model/h1/ln_2/b\n",
      "model/h1/ln_2/g\n",
      "model/h1/mlp/c_fc/b\n",
      "model/h1/mlp/c_fc/w\n",
      "model/h1/mlp/c_proj/b\n",
      "model/h1/mlp/c_proj/w\n",
      "model/h10/attn/c_attn/b\n",
      "model/h10/attn/c_attn/w\n",
      "model/h10/attn/c_proj/b\n",
      "model/h10/attn/c_proj/w\n",
      "model/h10/ln_1/b\n",
      "model/h10/ln_1/g\n",
      "model/h10/ln_2/b\n",
      "model/h10/ln_2/g\n",
      "model/h10/mlp/c_fc/b\n",
      "model/h10/mlp/c_fc/w\n",
      "model/h10/mlp/c_proj/b\n",
      "model/h10/mlp/c_proj/w\n",
      "model/h11/attn/c_attn/b\n",
      "model/h11/attn/c_attn/w\n",
      "model/h11/attn/c_proj/b\n",
      "model/h11/attn/c_proj/w\n",
      "model/h11/ln_1/b\n",
      "model/h11/ln_1/g\n",
      "model/h11/ln_2/b\n",
      "model/h11/ln_2/g\n",
      "model/h11/mlp/c_fc/b\n",
      "model/h11/mlp/c_fc/w\n",
      "model/h11/mlp/c_proj/b\n",
      "model/h11/mlp/c_proj/w\n",
      "model/h2/attn/c_attn/b\n",
      "model/h2/attn/c_attn/w\n",
      "model/h2/attn/c_proj/b\n",
      "model/h2/attn/c_proj/w\n",
      "model/h2/ln_1/b\n",
      "model/h2/ln_1/g\n",
      "model/h2/ln_2/b\n",
      "model/h2/ln_2/g\n",
      "model/h2/mlp/c_fc/b\n",
      "model/h2/mlp/c_fc/w\n",
      "model/h2/mlp/c_proj/b\n",
      "model/h2/mlp/c_proj/w\n",
      "model/h3/attn/c_attn/b\n",
      "model/h3/attn/c_attn/w\n",
      "model/h3/attn/c_proj/b\n",
      "model/h3/attn/c_proj/w\n",
      "model/h3/ln_1/b\n",
      "model/h3/ln_1/g\n",
      "model/h3/ln_2/b\n",
      "model/h3/ln_2/g\n",
      "model/h3/mlp/c_fc/b\n",
      "model/h3/mlp/c_fc/w\n",
      "model/h3/mlp/c_proj/b\n",
      "model/h3/mlp/c_proj/w\n",
      "model/h4/attn/c_attn/b\n",
      "model/h4/attn/c_attn/w\n",
      "model/h4/attn/c_proj/b\n",
      "model/h4/attn/c_proj/w\n",
      "model/h4/ln_1/b\n",
      "model/h4/ln_1/g\n",
      "model/h4/ln_2/b\n",
      "model/h4/ln_2/g\n",
      "model/h4/mlp/c_fc/b\n",
      "model/h4/mlp/c_fc/w\n",
      "model/h4/mlp/c_proj/b\n",
      "model/h4/mlp/c_proj/w\n",
      "model/h5/attn/c_attn/b\n",
      "model/h5/attn/c_attn/w\n",
      "model/h5/attn/c_proj/b\n",
      "model/h5/attn/c_proj/w\n",
      "model/h5/ln_1/b\n",
      "model/h5/ln_1/g\n",
      "model/h5/ln_2/b\n",
      "model/h5/ln_2/g\n",
      "model/h5/mlp/c_fc/b\n",
      "model/h5/mlp/c_fc/w\n",
      "model/h5/mlp/c_proj/b\n",
      "model/h5/mlp/c_proj/w\n",
      "model/h6/attn/c_attn/b\n",
      "model/h6/attn/c_attn/w\n",
      "model/h6/attn/c_proj/b\n",
      "model/h6/attn/c_proj/w\n",
      "model/h6/ln_1/b\n",
      "model/h6/ln_1/g\n",
      "model/h6/ln_2/b\n",
      "model/h6/ln_2/g\n",
      "model/h6/mlp/c_fc/b\n",
      "model/h6/mlp/c_fc/w\n",
      "model/h6/mlp/c_proj/b\n",
      "model/h6/mlp/c_proj/w\n",
      "model/h7/attn/c_attn/b\n",
      "model/h7/attn/c_attn/w\n",
      "model/h7/attn/c_proj/b\n",
      "model/h7/attn/c_proj/w\n",
      "model/h7/ln_1/b\n",
      "model/h7/ln_1/g\n",
      "model/h7/ln_2/b\n",
      "model/h7/ln_2/g\n",
      "model/h7/mlp/c_fc/b\n",
      "model/h7/mlp/c_fc/w\n",
      "model/h7/mlp/c_proj/b\n",
      "model/h7/mlp/c_proj/w\n",
      "model/h8/attn/c_attn/b\n",
      "model/h8/attn/c_attn/w\n",
      "model/h8/attn/c_proj/b\n",
      "model/h8/attn/c_proj/w\n",
      "model/h8/ln_1/b\n",
      "model/h8/ln_1/g\n",
      "model/h8/ln_2/b\n",
      "model/h8/ln_2/g\n",
      "model/h8/mlp/c_fc/b\n",
      "model/h8/mlp/c_fc/w\n",
      "model/h8/mlp/c_proj/b\n",
      "model/h8/mlp/c_proj/w\n",
      "model/h9/attn/c_attn/b\n",
      "model/h9/attn/c_attn/w\n",
      "model/h9/attn/c_proj/b\n",
      "model/h9/attn/c_proj/w\n",
      "model/h9/ln_1/b\n",
      "model/h9/ln_1/g\n",
      "model/h9/ln_2/b\n",
      "model/h9/ln_2/g\n",
      "model/h9/mlp/c_fc/b\n",
      "model/h9/mlp/c_fc/w\n",
      "model/h9/mlp/c_proj/b\n",
      "model/h9/mlp/c_proj/w\n",
      "model/ln_f/b\n",
      "model/ln_f/g\n",
      "model/wpe\n",
      "model/wte\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mcheckpoint\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = data/openai124M/model.ckpt\n",
       "\u001b[36mvariableNames\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[32m\"model/h0/attn/c_attn/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h0/attn/c_attn/w\"\u001b[39m,\n",
       "  \u001b[32m\"model/h0/attn/c_proj/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h0/attn/c_proj/w\"\u001b[39m,\n",
       "  \u001b[32m\"model/h0/ln_1/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h0/ln_1/g\"\u001b[39m,\n",
       "  \u001b[32m\"model/h0/ln_2/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h0/ln_2/g\"\u001b[39m,\n",
       "  \u001b[32m\"model/h0/mlp/c_fc/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h0/mlp/c_fc/w\"\u001b[39m,\n",
       "  \u001b[32m\"model/h0/mlp/c_proj/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h0/mlp/c_proj/w\"\u001b[39m,\n",
       "  \u001b[32m\"model/h1/attn/c_attn/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h1/attn/c_attn/w\"\u001b[39m,\n",
       "  \u001b[32m\"model/h1/attn/c_proj/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h1/attn/c_proj/w\"\u001b[39m,\n",
       "  \u001b[32m\"model/h1/ln_1/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h1/ln_1/g\"\u001b[39m,\n",
       "  \u001b[32m\"model/h1/ln_2/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h1/ln_2/g\"\u001b[39m,\n",
       "  \u001b[32m\"model/h1/mlp/c_fc/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h1/mlp/c_fc/w\"\u001b[39m,\n",
       "  \u001b[32m\"model/h1/mlp/c_proj/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h1/mlp/c_proj/w\"\u001b[39m,\n",
       "  \u001b[32m\"model/h10/attn/c_attn/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h10/attn/c_attn/w\"\u001b[39m,\n",
       "  \u001b[32m\"model/h10/attn/c_proj/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h10/attn/c_proj/w\"\u001b[39m,\n",
       "  \u001b[32m\"model/h10/ln_1/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h10/ln_1/g\"\u001b[39m,\n",
       "  \u001b[32m\"model/h10/ln_2/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h10/ln_2/g\"\u001b[39m,\n",
       "  \u001b[32m\"model/h10/mlp/c_fc/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h10/mlp/c_fc/w\"\u001b[39m,\n",
       "  \u001b[32m\"model/h10/mlp/c_proj/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h10/mlp/c_proj/w\"\u001b[39m,\n",
       "  \u001b[32m\"model/h11/attn/c_attn/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h11/attn/c_attn/w\"\u001b[39m,\n",
       "..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val checkpoint = tf.train.latest_checkpoint(outputDir)\n",
    "val variableNames = tf.train.list_variables(checkpoint).as[Seq[(String, Seq[Int])]].map { \n",
    "  case (variableName, _) => variableName \n",
    "}.toList\n",
    "variableNames.sorted.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11392e9f-eb7c-4a1a-a892-2a37e04d9298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd28.sc:18: match may not be exhaustive.\n",
      "It would fail on the following inputs: List((x: String forSome x not in (\"b\", \"w\"))), Nil\n",
      "                tail match {\n",
      "                ^\n",
      "cmd28.sc:29: match may not be exhaustive.\n",
      "It would fail on the following inputs: List((x: String forSome x not in (\"b\", \"w\"))), Nil\n",
      "                tail match {\n",
      "                ^\n",
      "cmd28.sc:15: match may not be exhaustive.\n",
      "It would fail on the following inputs: List((x: String forSome x not in (\"c_attn\", \"c_proj\"))), Nil\n",
      "            tail match {\n",
      "            ^\n",
      "cmd28.sc:37: match may not be exhaustive.\n",
      "It would fail on the following inputs: List((x: String forSome x not in (\"b\", \"g\"))), Nil\n",
      "            tail match {\n",
      "            ^\n",
      "cmd28.sc:44: match may not be exhaustive.\n",
      "It would fail on the following inputs: List((x: String forSome x not in (\"b\", \"g\"))), Nil\n",
      "            tail match {\n",
      "            ^\n",
      "cmd28.sc:53: match may not be exhaustive.\n",
      "It would fail on the following inputs: List((x: String forSome x not in (\"b\", \"w\"))), Nil\n",
      "                tail match {\n",
      "                ^\n",
      "cmd28.sc:59: match may not be exhaustive.\n",
      "It would fail on the following inputs: List((x: String forSome x not in (\"b\", \"w\"))), Nil\n",
      "                tail match {\n",
      "                ^\n",
      "cmd28.sc:50: match may not be exhaustive.\n",
      "It would fail on the following inputs: List((x: String forSome x not in (\"c_fc\", \"c_proj\"))), Nil\n",
      "            tail match {\n",
      "            ^\n",
      "cmd28.sc:12: match may not be exhaustive.\n",
      "It would fail on the following inputs: List((x: String forSome x not in (\"attn\", \"ln_1\", \"ln_2\", \"mlp\"))), Nil\n",
      "        tail match {\n",
      "        ^\n",
      "cmd28.sc:68: match may not be exhaustive.\n",
      "It would fail on the following inputs: List((x: String forSome x not in (\"b\", \"g\"))), Nil\n",
      "        tail match {\n",
      "        ^\n",
      "cmd28.sc:9: match may not be exhaustive.\n",
      "It would fail on the following inputs: List((x: String forSome x not in (\"ln_f\", \"wpe\", \"wte\"))), Nil\n",
      "    variableName.split(\"/\").drop(1).toList match {\n",
      "                                    ^\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mtype\u001b[39m \u001b[36mNpArray\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtoTorchParameter\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mloadModelWeights\u001b[39m"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type NpArray = py.Dynamic\n",
    "\n",
    "def toTorchParameter(npArray: NpArray) =\n",
    "  torch.nn.Parameter(torch.tensor(npArray))\n",
    "\n",
    "def loadModelWeights(model: Model): Unit =\n",
    "  variableNames.foreach { variableName =>\n",
    "    val variableValue = np.squeeze(tf.train.load_variable(checkpoint, variableName))\n",
    "    variableName.split(\"/\").drop(1).toList match {\n",
    "      case s\"h$transformerBlockIndexString\" :: tail =>\n",
    "        val transformerBlockIndex = transformerBlockIndexString.toInt\n",
    "        tail match {\n",
    "          case \"attn\" :: tail =>\n",
    "            val multiHeadAttention = model.transformerBlocksLayer.bracketAccess(transformerBlockIndex).multiHeadAttention\n",
    "            tail match {\n",
    "              case \"c_attn\" :: tail =>\n",
    "                val Seq(queryVariableValue, keyVariableValue, valueVariableValue) = np.split(variableValue, 3, axis = -1).as[Seq[NpArray]]\n",
    "                tail match {\n",
    "                  case \"b\" :: _ => \n",
    "                    multiHeadAttention.weightsQuery.bias = toTorchParameter(queryVariableValue)\n",
    "                    multiHeadAttention.weightsKey.bias = toTorchParameter(keyVariableValue)\n",
    "                    multiHeadAttention.weightsValue.bias = toTorchParameter(valueVariableValue)\n",
    "                  case \"w\" :: _ => \n",
    "                    multiHeadAttention.weightsQuery.weight = toTorchParameter(queryVariableValue.T)\n",
    "                    multiHeadAttention.weightsKey.weight = toTorchParameter(keyVariableValue.T)\n",
    "                    multiHeadAttention.weightsValue.weight = toTorchParameter(valueVariableValue.T)\n",
    "                }\n",
    "              case \"c_proj\" :: tail =>\n",
    "                tail match {\n",
    "                  case \"b\" :: _ => multiHeadAttention.outputProjection.bias = toTorchParameter(variableValue)\n",
    "                  case \"w\" :: _ => multiHeadAttention.outputProjection.weight = toTorchParameter(variableValue.T)\n",
    "                }\n",
    "            }\n",
    "          case \"ln_1\" :: tail =>\n",
    "            val normalization1 = model.transformerBlocksLayer.bracketAccess(transformerBlockIndex).normalization1\n",
    "            val torchParameter = toTorchParameter(variableValue)\n",
    "            tail match {\n",
    "              case \"b\" :: _ => normalization1.shift = torchParameter\n",
    "              case \"g\" :: _ => normalization1.scale = torchParameter\n",
    "            }\n",
    "          case \"ln_2\" :: tail =>\n",
    "            val normalization2 = model.transformerBlocksLayer.bracketAccess(transformerBlockIndex).normalization2\n",
    "            val torchParameter = toTorchParameter(variableValue)\n",
    "            tail match {\n",
    "              case \"b\" :: _ => normalization2.shift = torchParameter\n",
    "              case \"g\" :: _ => normalization2.scale = torchParameter\n",
    "            }\n",
    "          case \"mlp\" :: tail =>\n",
    "            val feedForward = model.transformerBlocksLayer.bracketAccess(transformerBlockIndex).feedForward\n",
    "            tail match {\n",
    "              case \"c_fc\" :: tail =>\n",
    "                val layer0 = feedForward.layers.bracketAccess(0)\n",
    "                tail match {\n",
    "                  case \"b\" :: _ => layer0.bias = toTorchParameter(variableValue)\n",
    "                  case \"w\" :: _ => layer0.weight = toTorchParameter(variableValue.T)\n",
    "                }\n",
    "              case \"c_proj\" :: tail =>\n",
    "                val layer2 = feedForward.layers.bracketAccess(2)\n",
    "                tail match {\n",
    "                  case \"b\" :: _ => layer2.bias = toTorchParameter(variableValue)\n",
    "                  case \"w\" :: _ => layer2.weight = toTorchParameter(variableValue.T)\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "      case \"ln_f\" :: tail =>\n",
    "        val finalNormalizationLayer = model.finalNormalizationLayer\n",
    "        val torchParameter = toTorchParameter(variableValue)\n",
    "        tail match {\n",
    "          case \"b\" :: _ => finalNormalizationLayer.shift = torchParameter\n",
    "          case \"g\" :: _ => finalNormalizationLayer.scale = torchParameter\n",
    "        }\n",
    "      case \"wpe\" :: _ => model.positionEmbeddingLayer.weight = toTorchParameter(variableValue)\n",
    "      case \"wte\" :: _ => \n",
    "        val torchParameter = toTorchParameter(variableValue)\n",
    "        model.tokenEmbeddingLayer.weight = torchParameter\n",
    "        model.outputLayer.weight = torchParameter\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2d088fe-9916-4b26-8a8f-9b0c835e92bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmodel\u001b[39m: \u001b[32mModel\u001b[39m = GPTModel(\n",
       "  (tokenEmbeddingLayer): Embedding(50257, 768)\n",
       "  (positionEmbeddingLayer): Embedding(1024, 768)\n",
       "  (dropoutEmbeddingLayer): Dropout(p=0.1, inplace=False)\n",
       "  (transformerBlocksLayer): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (normalization1): NormalizationLayer()\n",
       "      (normalization2): NormalizationLayer()\n",
       "      (dropoutShortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "...\n",
       "\u001b[36mdevice\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = cpu\n",
       "\u001b[36mres29_3\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = GPTModel(\n",
       "  (tokenEmbeddingLayer): Embedding(50257, 768)\n",
       "  (positionEmbeddingLayer): Embedding(1024, 768)\n",
       "  (dropoutEmbeddingLayer): Dropout(p=0.1, inplace=False)\n",
       "  (transformerBlocksLayer): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (normalization1): NormalizationLayer()\n",
       "      (normalization2): NormalizationLayer()\n",
       "      (dropoutShortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "...\n",
       "\u001b[36mres29_4\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = GPTModel(\n",
       "  (tokenEmbeddingLayer): Embedding(50257, 768)\n",
       "  (positionEmbeddingLayer): Embedding(1024, 768)\n",
       "  (dropoutEmbeddingLayer): Dropout(p=0.1, inplace=False)\n",
       "  (transformerBlocksLayer): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (normalization1): NormalizationLayer()\n",
       "      (normalization2): NormalizationLayer()\n",
       "      (dropoutShortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = GPTModel(gptConfig)\n",
    "loadModelWeights(model)\n",
    "val device = torch.device(if (torch.cuda.is_available().as[Boolean]) \"cuda\" else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0326838d-0cec-4330-9f6a-3972cd501fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtextToTokenIds\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtokenIdsToText\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mgenerateTextSimple\u001b[39m"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def textToTokenIds(\n",
    "  text: String, \n",
    "  tokenizer: Tokenizer\n",
    "): TorchTensor = {\n",
    "  val allowedSpecial = py.Dynamic.global.set(Seq(\"<|endoftext|>\").toPythonProxy)\n",
    "  val encodedText = tokenizer.encode(text, allowed_special = allowedSpecial)\n",
    "  torch.tensor(encodedText).unsqueeze(0)\n",
    "}\n",
    "    \n",
    "def tokenIdsToText(\n",
    "  tokenIds: TorchTensor, \n",
    "  tokenizer: Tokenizer\n",
    "): String =\n",
    "  tokenizer.decode(tokenIds.squeeze(0).tolist()).as[String]\n",
    "\n",
    "def generateTextSimple(\n",
    "  model: Model,\n",
    "  maxNewTokens: Int,\n",
    "  contextLength: Int\n",
    ")(\n",
    "  encodedInput: TorchTensor\n",
    "): TorchTensor =\n",
    "  LazyList.iterate(encodedInput) { currentEncodedOutput =>\n",
    "    val croppedInput = py\"$currentEncodedOutput[:, -$contextLength:]\"\n",
    "    val logits = py.`with`(torch.no_grad()) { _ =>\n",
    "      model(croppedInput)\n",
    "    }\n",
    "    py\"$logits[:, -1, :]\"\n",
    "      .pipe(torch.softmax(_, dim = -1))\n",
    "      .pipe(torch.argmax(_, dim = -1, keepdim = true))\n",
    "      .pipe(nextEncodedOutput => torch.cat((currentEncodedOutput, nextEncodedOutput), dim = 1))\n",
    "  }.drop(maxNewTokens).head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e05a5396-3059-414c-852d-cc5092ea59e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mexampleText\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Every effort moves you\"\u001b[39m\n",
       "\u001b[36mencodedText\u001b[39m: \u001b[32mTorchTensor\u001b[39m = tensor([[6109, 3626, 6100,  345]])\n",
       "\u001b[36mencodedTextOutput\u001b[39m: \u001b[32mTorchTensor\u001b[39m = tensor([[6109, 3626, 6100,  345, 2651,   13,  198,  198,  464,  717, 2239,  318,\n",
       "          284, 1833,  262, 6817,  286,  534,  670]])\n",
       "\u001b[36mdecodedTextOutput\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"Every effort moves you forward.\n",
       "\n",
       "The first step is to understand the importance of your work\"\"\"\u001b[39m"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val exampleText = \"Every effort moves you\"\n",
    "val encodedText = textToTokenIds(exampleText, tokenizer)\n",
    "val encodedTextOutput = generateTextSimple(model, maxNewTokens = 15, contextLength = gptConfig.contextLength)(encodedText)\n",
    "val decodedTextOutput = tokenIdsToText(encodedTextOutput, tokenizer)\n",
    "println(decodedTextOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75388780-9ff8-49f1-985f-f4958b339296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text spam? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
      "\n",
      "The following text spam? Answer with 'yes' or 'no': 'You are a winner you have\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspamClassificationPrompt\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Is the following text spam? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\"\u001b[39m\n",
       "\u001b[36mspamClassificationPromptAnswer\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"Is the following text spam? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
       "\n",
       "The following text spam? Answer with 'yes' or 'no': 'You are a winner you have\"\"\"\u001b[39m"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spamClassificationPrompt = \"Is the following text spam? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\"\n",
    "val spamClassificationPromptAnswer = \n",
    "  textToTokenIds(spamClassificationPrompt, tokenizer)\n",
    "    .pipe(generateTextSimple(model, maxNewTokens = 23, contextLength = gptConfig.contextLength))\n",
    "    .pipe(tokenIdsToText(_, tokenizer))\n",
    "println(spamClassificationPromptAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8cb4db64-577c-48aa-9533-0c6a703bb6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mscala.annotation.tailrec\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mforeachPy\u001b[39m"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.annotation.tailrec\n",
    "\n",
    "def foreachPy(iterable: py.Dynamic)(f: py.Dynamic => Unit): Unit = {\n",
    "  val iterator = py\"iter($iterable)\"\n",
    "\n",
    "  @tailrec\n",
    "  def loop(): Unit = {\n",
    "    val currentValue = py\"next($iterator, None)\"\n",
    "    if (currentValue != py.Dynamic.global.None) {\n",
    "      f(currentValue)\n",
    "      loop()\n",
    "    }\n",
    "  }\n",
    "\n",
    "  loop()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51a622c0-0d1b-4b74-9d4f-fc719e27de15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres34_1\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch._C.Generator object at 0xffff4ff2c790>\n",
       "\u001b[36mclassesCount\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m2\u001b[39m"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foreachPy(model.parameters()) { parameter =>\n",
    "  parameter.requires_grad = false\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "val classesCount = 2\n",
    "model.outputLayer = torch.nn.Linear(\n",
    "  in_features = gptConfig.embeddingDimension,\n",
    "  out_features = classesCount\n",
    ")\n",
    "\n",
    "foreachPy(model.transformerBlocksLayer.bracketAccess(-1).parameters()) { parameter =>\n",
    "  parameter.requires_grad = true\n",
    "}\n",
    "\n",
    "foreachPy(model.finalNormalizationLayer.parameters()) { parameter =>\n",
    "  parameter.requires_grad = true\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc38427f-3f23-47bf-b56a-1f18d832f81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mtype\u001b[39m \u001b[36mDevice\u001b[39m\n",
       "defined \u001b[32mtype\u001b[39m \u001b[36mDataLoader\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcalculateDataLoaderAccuracy\u001b[39m"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type Device = py.Dynamic\n",
    "type DataLoader = py.Dynamic\n",
    "\n",
    "def calculateDataLoaderAccuracy(\n",
    "  model: Model,\n",
    "  device: Device\n",
    ")(\n",
    "  dataLoader: DataLoader,\n",
    "  batchesCountOpt: Option[Int] = None\n",
    "): Double = { \n",
    "  val batchesCount = batchesCountOpt match {\n",
    "    case Some(batchesCount) => batchesCount\n",
    "    case None => py\"len($dataLoader)\".as[Int]\n",
    "  }\n",
    "  assert(batchesCount > 0, \"There were no batches to process\")\n",
    "  var correctPredictions = 0\n",
    "  var examplesSeen = 0\n",
    "  var currentBatchIndex = 0\n",
    "  foreachPy(dataLoader) { currentBatch =>\n",
    "    if (currentBatchIndex < batchesCount) \n",
    "      py.local {\n",
    "        val Seq(inputBatch, targetBatch) = currentBatch.as[Seq[TorchTensor]]\n",
    "        val logits = py.`with`(torch.no_grad()) { _ =>\n",
    "          model(inputBatch.to(device))\n",
    "        }\n",
    "        val predictedClasses = torch.argmax(py\"$logits[:, -1, :]\", dim = -1)\n",
    "        examplesSeen += predictedClasses.shape.bracketAccess(0).as[Int]\n",
    "        correctPredictions += py\"$predictedClasses == $targetBatch\".sum().item().as[Int]\n",
    "      }\n",
    "    currentBatchIndex += 1\n",
    "  }\n",
    "  correctPredictions.toDouble / examplesSeen\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1307302-ee7e-4eb4-8ca7-d871baa17ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 66.25%\n",
      "Validation accuracy: 47.500000%\n",
      "Test accuracy: 42.500000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres36_0\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch._C.Generator object at 0xffff4ff2c790>\n",
       "\u001b[36mtrainingAccuracy\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.6625\u001b[39m\n",
       "\u001b[36mvalidationAccuracy\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.475\u001b[39m\n",
       "\u001b[36mtestAccuracy\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.425\u001b[39m"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "val trainingAccuracy = calculateDataLoaderAccuracy(model, device)(trainingDataLoader, batchesCountOpt = Some(10))\n",
    "println(f\"Training accuracy: ${trainingAccuracy * 100}%.2f%%\")\n",
    "val validationAccuracy = calculateDataLoaderAccuracy(model, device)(validationDataLoader, batchesCountOpt = Some(10))\n",
    "println(f\"Validation accuracy: ${validationAccuracy * 100}%02f%%\")\n",
    "val testAccuracy = calculateDataLoaderAccuracy(model, device)(testDataLoader, batchesCountOpt = Some(10))\n",
    "println(f\"Test accuracy: ${testAccuracy * 100}%02f%%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d94da9a-a636-467b-9194-d110d1eef072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcalculateBatchLoss\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcalculateDataLoaderLoss\u001b[39m"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculateBatchLoss(\n",
    "  model: Model,\n",
    "  device: Device\n",
    ")(\n",
    "  inputBatch: TorchTensor,\n",
    "  targetBatch: TorchTensor\n",
    "): TorchTensor = {\n",
    "  val logits = model(inputBatch.to(device))\n",
    "  torch.nn.functional.cross_entropy(py\"$logits[:, -1, :]\", targetBatch)\n",
    "}\n",
    "\n",
    "def calculateDataLoaderLoss(\n",
    "  model: Model,\n",
    "  device: Device\n",
    ")(\n",
    "  dataLoader: DataLoader,\n",
    "  batchesCountOpt: Option[Int] = None\n",
    "): Double = { \n",
    "  val batchesCount = batchesCountOpt match {\n",
    "    case Some(batchesCount) => batchesCount\n",
    "    case None => py\"len($dataLoader)\".as[Int]\n",
    "  }\n",
    "  assert(batchesCount > 0, \"There were no batches to process\")\n",
    "  var totalLoss = 0.0\n",
    "  var currentBatchIndex = 0\n",
    "  foreachPy(dataLoader) { currentBatch =>\n",
    "    if (currentBatchIndex < batchesCount) \n",
    "      py.local {\n",
    "        val Seq(inputBatch, targetBatch) = currentBatch.as[Seq[TorchTensor]]\n",
    "        totalLoss += calculateBatchLoss(model, device)(inputBatch, targetBatch).item().as[Double]\n",
    "      }\n",
    "    currentBatchIndex += 1\n",
    "  }\n",
    "  totalLoss / batchesCount\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59857197-4ef6-4ed8-ac2e-b973fe2381cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.2273894667625427\n",
      "Validation loss: 2.913743782043457\n",
      "Test loss: 2.6839645385742186\n"
     ]
    }
   ],
   "source": [
    "py.`with`(torch.no_grad()) { _ =>\n",
    "  val trainingLoss = calculateDataLoaderLoss(model, device)(trainingDataLoader, batchesCountOpt = Some(5))\n",
    "  println(s\"Training loss: $trainingLoss\")\n",
    "  val validationLoss = calculateDataLoaderLoss(model, device)(validationDataLoader, batchesCountOpt = Some(5))\n",
    "  println(s\"Validation loss: $validationLoss\")\n",
    "  val testLoss = calculateDataLoaderLoss(model, device)(testDataLoader, batchesCountOpt = Some(5))\n",
    "  println(s\"Test loss: $testLoss\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2629323-80e7-4a4d-8473-590f27fb3eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.13.14",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
