{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0f6387d-40fc-4c57-9759-bd3e9ee1a1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $file.^.Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1832f5d-7bd6-4d42-bbef-9959b6f3d636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mGPTConfig\u001b[39m\n",
       "\u001b[36mgptConfig\u001b[39m: \u001b[32mGPTConfig\u001b[39m = \u001b[33mGPTConfig\u001b[39m(\n",
       "  vocabularySize = \u001b[32m50257\u001b[39m,\n",
       "  contextLength = \u001b[32m1024\u001b[39m,\n",
       "  embeddingDimension = \u001b[32m768\u001b[39m,\n",
       "  attentionHeadsCount = \u001b[32m12\u001b[39m,\n",
       "  layersCount = \u001b[32m12\u001b[39m,\n",
       "  dropoutRate = \u001b[32m0.1\u001b[39m,\n",
       "  queryKeyValueBias = \u001b[32mtrue\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class GPTConfig(\n",
    "  vocabularySize: Int,\n",
    "  contextLength: Int,\n",
    "  embeddingDimension: Int,\n",
    "  attentionHeadsCount: Int,\n",
    "  layersCount: Int,\n",
    "  dropoutRate: Double,\n",
    "  queryKeyValueBias: Boolean\n",
    ")\n",
    "\n",
    "val gptConfig = GPTConfig(\n",
    "  vocabularySize = 50257,\n",
    "  contextLength = 1024,\n",
    "  embeddingDimension = 768,\n",
    "  attentionHeadsCount = 12,\n",
    "  layersCount = 12,\n",
    "  dropoutRate = 0.1,\n",
    "  queryKeyValueBias = true\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd31dddb-772f-45bf-b9a1-9e1fcc33c3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.4.* in /usr/local/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (75.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->torch==2.4.*) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/site-packages (from sympy->torch==2.4.*) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "Magic.!(\"pip\", \"install\", \"torch==2.4.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea2ea2fd-47db-40b4-a634-4f3628e9e56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mme.shadaj.scalapy.py\u001b[39m\n",
       "\u001b[36mtorch\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mModule\u001b[39m = <module 'torch' from '/usr/local/lib/python3.12/site-packages/torch/__init__.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`dev.scalapy::scalapy-core:0.5.3`\n",
    "\n",
    "import me.shadaj.scalapy.py\n",
    "\n",
    "val torch = py.module(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a6b593c-46ea-47cb-b7a4-3bbbf9718b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mpy.PyQuote\u001b[39m\n",
       "defined \u001b[32mtype\u001b[39m \u001b[36mTorchTensor\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mMultiHeadAttention\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import py.PyQuote\n",
    "\n",
    "type TorchTensor = py.Dynamic\n",
    "\n",
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class MultiHeadAttention(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def MultiHeadAttention(\n",
    "  inputDimension: Int,\n",
    "  outputDimension: Int,\n",
    "  dropoutProbability: Double,\n",
    "  contextLength: Int,\n",
    "  headsCount: Int,\n",
    "  queryKeyValueBias: Boolean\n",
    "): py.Dynamic = {\n",
    "  assert(outputDimension % headsCount == 0, \"Output dimension must be a multiple of heads count\")\n",
    "  val headDimension = outputDimension / headsCount\n",
    "    \n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.weightsQuery = torch.nn.Linear(inputDimension, outputDimension, bias = queryKeyValueBias)\n",
    "    self.weightsKey = torch.nn.Linear(inputDimension, outputDimension, bias = queryKeyValueBias)\n",
    "    self.weightsValue = torch.nn.Linear(inputDimension, outputDimension, bias = queryKeyValueBias)\n",
    "    self.outputProjection = torch.nn.Linear(outputDimension, outputDimension)\n",
    "    self.dropout = torch.nn.Dropout(dropoutProbability)\n",
    "    self.register_buffer(\"mask\", torch.triu(torch.ones(contextLength, contextLength), diagonal = 1))\n",
    "      \n",
    "    val forward = (batchedInputs: TorchTensor) => {\n",
    "      val (batchesCount, tokensCount, tokenDimension) = batchedInputs.shape.as[(Int, Int, Int)]\n",
    "      val queries = self.weightsQuery(batchedInputs)\n",
    "        .view(batchesCount, tokensCount, headsCount, headDimension)\n",
    "        .transpose(1, 2)\n",
    "      val keys = self.weightsKey(batchedInputs)\n",
    "        .view(batchesCount, tokensCount, headsCount, headDimension)\n",
    "        .transpose(1, 2)\n",
    "      val values = self.weightsValue(batchedInputs)\n",
    "        .view(batchesCount, tokensCount, headsCount, headDimension)\n",
    "        .transpose(1, 2)\n",
    "      val attentionScores = py\"$queries @ $keys.transpose(2, 3)\"\n",
    "      attentionScores.masked_fill_(py\"${self.mask}.bool()[:$tokensCount, :$tokensCount]\", -torch.inf)\n",
    "      val attentionWeights = self.dropout(torch.softmax(py\"$attentionScores / $headDimension**0.5\", dim = -1))\n",
    "      self.outputProjection(\n",
    "        py\"$attentionWeights @ $values\"\n",
    "          .transpose(1, 2)\n",
    "          .reshape(batchesCount, tokensCount, outputDimension)\n",
    "      )\n",
    "    }\n",
    "    self.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.MultiHeadAttention(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d570669a-847e-405d-9425-315e2730b6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mGELU\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "// Because it mostly uses Python operators, it's implemented fully in Python\n",
    "py.exec {\n",
    "  s\"\"\"import torch\n",
    "     |import torch.nn as nn\n",
    "     |\n",
    "     |class GELU(nn.Module):\n",
    "     |  def __init__(self):\n",
    "     |    super().__init__()\n",
    "     |\n",
    "     |  def forward(self, inputs):\n",
    "     |    return 0.5 * inputs * (\n",
    "     |      1 + torch.tanh(\n",
    "     |        torch.sqrt(torch.tensor(2.0 / torch.pi)) * (inputs + 0.044715 * torch.pow(inputs, 3))\n",
    "     |      )\n",
    "     |    )\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def GELU() = py.Dynamic.global.GELU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a23495f2-c203-4612-a1a4-85d99e28a9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mFeedForward\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class FeedForward(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def FeedForward(\n",
    "  embeddingDimension: Int\n",
    "): py.Dynamic = {\n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.layers = torch.nn.Sequential(\n",
    "      torch.nn.Linear(embeddingDimension, 4 * embeddingDimension),\n",
    "      GELU(),\n",
    "      torch.nn.Linear(4 * embeddingDimension, embeddingDimension)\n",
    "    )\n",
    "      \n",
    "    val forward = (inputs: TorchTensor) => self.layers(inputs)\n",
    "    self.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.FeedForward(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b03b363-da08-4ba4-b265-450334ac88a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mNormalizationLayer\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class NormalizationLayer(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def NormalizationLayer(\n",
    "  embeddingDimension: Int\n",
    "): py.Dynamic = {\n",
    "  val epsilon = 1e-5\n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.scale = torch.nn.Parameter(torch.ones(embeddingDimension))\n",
    "    self.shift = torch.nn.Parameter(torch.zeros(embeddingDimension))\n",
    "      \n",
    "    val forward = (inputs: TorchTensor) => {\n",
    "      val mean = inputs.mean(dim = -1, keepdim = true)\n",
    "      val variance = inputs.`var`(dim = -1, keepdim = true, unbiased = false)\n",
    "      val normalizedInputs = py\"($inputs - $mean) / torch.sqrt($variance + $epsilon)\"\n",
    "      py\"${self.scale} * $normalizedInputs + ${self.shift}\"\n",
    "    }\n",
    "    self.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.NormalizationLayer(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef4eb2a4-ec74-4954-9134-554a50935510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.chaining._\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mTransformerBlock\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.chaining._\n",
    "\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class TransformerBlock(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def TransformerBlock(\n",
    "  config: GPTConfig\n",
    "): py.Dynamic = {\n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.multiHeadAttention = MultiHeadAttention(\n",
    "      inputDimension = config.embeddingDimension,\n",
    "      outputDimension = config.embeddingDimension,\n",
    "      dropoutProbability = config.dropoutRate,\n",
    "      contextLength = config.contextLength,\n",
    "      headsCount = config.attentionHeadsCount,\n",
    "      queryKeyValueBias = config.queryKeyValueBias\n",
    "    )\n",
    "    self.feedForward = FeedForward(config.embeddingDimension)\n",
    "    self.normalization1 = NormalizationLayer(config.embeddingDimension)\n",
    "    self.normalization2 = NormalizationLayer(config.embeddingDimension)\n",
    "    self.dropoutShortcut = torch.nn.Dropout(config.dropoutRate)\n",
    "    \n",
    "    val forward = (inputs: TorchTensor) => {\n",
    "      val shortcut = inputs\n",
    "      val newShortcut = inputs\n",
    "        .pipe(self.normalization1(_))\n",
    "        .pipe(self.multiHeadAttention(_))\n",
    "        .pipe(self.dropoutShortcut(_))\n",
    "        .pipe(o => py\"$o + $shortcut\")\n",
    "      newShortcut\n",
    "        .pipe(self.normalization2(_))\n",
    "        .pipe(self.feedForward(_))\n",
    "        .pipe(self.dropoutShortcut(_))\n",
    "        .pipe(o => py\"$o + $newShortcut\")\n",
    "    }\n",
    "    self.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.TransformerBlock(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0440b509-feb1-4fde-8c70-205d565502f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mtype\u001b[39m \u001b[36mModel\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mGPTModel\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class GPTModel(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "type Model = py.Dynamic\n",
    "def GPTModel(\n",
    "  config: GPTConfig\n",
    "): Model = {\n",
    "  val transformerBlocks = Seq.fill(config.layersCount)(TransformerBlock(config))\n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.tokenEmbeddingLayer = torch.nn.Embedding(config.vocabularySize, config.embeddingDimension)\n",
    "    self.positionEmbeddingLayer = torch.nn.Embedding(config.contextLength, config.embeddingDimension)\n",
    "    self.dropoutEmbeddingLayer = torch.nn.Dropout(config.dropoutRate)\n",
    "    self.transformerBlocksLayer = py\"nn.Sequential(*${transformerBlocks.toPythonProxy})\"\n",
    "    self.finalNormalizationLayer = NormalizationLayer(config.embeddingDimension)\n",
    "    self.outputLayer = torch.nn.Linear(config.embeddingDimension, config.vocabularySize, bias = false)\n",
    "      \n",
    "    val forward = (batchedInputs: TorchTensor) => {\n",
    "      val (_, sequenceLength) = batchedInputs.shape.as[(Int, Int)]\n",
    "      val tokenEmbeddings = self.tokenEmbeddingLayer(batchedInputs)\n",
    "      val positionEmbeddings = self.positionEmbeddingLayer(torch.arange(sequenceLength, device = batchedInputs.device))\n",
    "      py\"$tokenEmbeddings + $positionEmbeddings\"\n",
    "        .pipe(self.dropoutEmbeddingLayer(_))\n",
    "        .pipe(self.transformerBlocksLayer(_))\n",
    "        .pipe(self.finalNormalizationLayer(_))\n",
    "        .pipe(self.outputLayer(_))\n",
    "    }\n",
    "    self.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.GPTModel(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f121b8b-6ac7-4e8b-83aa-f537253dca14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmodel\u001b[39m: \u001b[32mModel\u001b[39m = GPTModel(\n",
       "  (tokenEmbeddingLayer): Embedding(50257, 768)\n",
       "  (positionEmbeddingLayer): Embedding(1024, 768)\n",
       "  (dropoutEmbeddingLayer): Dropout(p=0.1, inplace=False)\n",
       "  (transformerBlocksLayer): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (normalization1): NormalizationLayer()\n",
       "      (normalization2): NormalizationLayer()\n",
       "      (dropoutShortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "...\n",
       "\u001b[36mdevice\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = cpu\n",
       "\u001b[36mres11_2\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = GPTModel(\n",
       "  (tokenEmbeddingLayer): Embedding(50257, 768)\n",
       "  (positionEmbeddingLayer): Embedding(1024, 768)\n",
       "  (dropoutEmbeddingLayer): Dropout(p=0.1, inplace=False)\n",
       "  (transformerBlocksLayer): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (normalization1): NormalizationLayer()\n",
       "      (normalization2): NormalizationLayer()\n",
       "      (dropoutShortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "...\n",
       "\u001b[36mcheckpoint\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = {'model': OrderedDict({'tokenEmbeddingLayer.weight': tensor([[ 1.7467e+00,  1.6750e+00,  2.4550e-01,  ...,  8.2859e-01,\n",
       "         -2.2970e-01,  1.4564e+00],\n",
       "        [-4.4679e-01,  1.1839e+00,  1.0902e+00,  ..., -7.5460e-01,\n",
       "          4.2087e-01,  5.8719e-01],\n",
       "        [ 1.5945e+00, -1.2873e+00, -3.3599e-03,  ...,  1.5097e+00,\n",
       "         -1.7991e-01,  1.5387e-01],\n",
       "        ...,\n",
       "        [-1.1012e+00,  3.5025e-01, -1.9819e-01,  ...,  1.4751e+00,\n",
       "         -6.8413e-01, -9.2930e-01],\n",
       "        [-1.2830e+00,  1.1319e+00, -1.5019e+00,  ..., -1.1252e+00,\n",
       "         -6.7377e-01, -8.8464e-01],\n",
       "        [-1.0690e-01, -7.7091e-01,  1.5382e-01,  ...,  1.2587e-01,\n",
       "          8.4397e-04,  9.9906e-01]]), 'positionEmbeddingLayer.weight': tensor([[ 2.8500e+00,  9.4432e-02, -6.4361e-01,  ..., -6.8505e-02,\n",
       "          1.3166e+00, -6.2983e-01],\n",
       "        [ 1.4499e+00,  5.0159e-01,  1.0971e+00,  ..., -2.2200e-01,\n",
       "          2.4517e-01, -2.3939e-01],\n",
       "        [-7.8571e-01, -5.4469e-01,  1.5396e+00,  ...,  3.0527e-01,\n",
       "          4.8201e-01,  4.0175e-01],\n",
       "        ...,\n",
       "        [-5.5355e-01, -3.0836e-01, -9.3897e-01,  ..., -4.9950e-01,\n",
       "         -3.3963e-01,  5.2340e-01],\n",
       "        [-1.7820e-01,  1.5635e-01,  2.0383e+00,  ...,  1.6944e+00,\n",
       "          3.0975e-01, -4.6069e-01],\n",
       "        [-4.3511e-01, -2.4559e-01,  7.3841e-01,  ...,  7.2764e-02,\n",
       "         -1.2494e+00,  1.4188e-03]]), 'transformerBlocksLayer.0.multiHeadAttention.mask': tensor([[0., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'transformerBlocksLayer.0.multiHeadAttention.weightsQuery.weight': tensor([[-0.0145,  0.0009, -0.0159,  ...,  0.0345,  0.0248, -0.0005],\n",
       "        [-0.0192, -0.0138, -0.0155,  ...,  0.0027, -0.0016,  0.0211],\n",
       "        [ 0.0154, -0.0178, -0.0071,  ...,  0.0292,  0.0002,  0.0068],\n",
       "        ...,\n",
       "...\n",
       "\u001b[36mmodelStateKey\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"model\"\u001b[39m\n",
       "\u001b[36mres11_5\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = GPTModel(gptConfig)\n",
    "val device = torch.device(if (torch.cuda.is_available().as[Boolean]) \"cuda\" else \"cpu\")\n",
    "model.to(device)\n",
    "val checkpoint = torch.load(\"model_and_optimizer.pth\", map_location = device)\n",
    "val modelStateKey = \"model\"\n",
    "model.load_state_dict(checkpoint.bracketAccess(modelStateKey))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd78912f-7901-4790-ac0c-50b529cea58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken==0.7.* in /usr/local/lib/python3.12/site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/site-packages (from tiktoken==0.7.*) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/site-packages (from tiktoken==0.7.*) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.7.*) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.7.*) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.7.*) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.7.*) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "Magic.!(\"pip\", \"install\", \"tiktoken==0.7.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec329eb4-3e5c-4ab4-ae47-4e67baf00381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtiktoken\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mModule\u001b[39m = <module 'tiktoken' from '/usr/local/lib/python3.12/site-packages/tiktoken/__init__.py'>\n",
       "\u001b[36mtokenizer\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <Encoding 'gpt2'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tiktoken = py.module(\"tiktoken\")\n",
    "val tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3a9e442-0691-4180-bfdc-764f809d1880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mpy.SeqConverters\u001b[39m\n",
       "defined \u001b[32mtype\u001b[39m \u001b[36mTokenizer\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mgenerateTextSimple\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtextToTokenIds\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtokenIdsToText\u001b[39m"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import py.SeqConverters\n",
    "\n",
    "type Tokenizer = py.Dynamic\n",
    "\n",
    "def generateTextSimple(\n",
    "  model: Model,\n",
    "  maxNewTokens: Int,\n",
    "  contextLength: Int\n",
    ")(\n",
    "  encodedInput: Vector[Int]\n",
    "): Vector[Int] =\n",
    "  LazyList.iterate(encodedInput) { currentEncodedOutput =>\n",
    "    py.local {\n",
    "      val croppedInput = currentEncodedOutput.takeRight(contextLength)\n",
    "      val logits = py.`with`(torch.no_grad()) { _ =>\n",
    "        val inputTensor = torch.tensor(croppedInput.toPythonProxy).unsqueeze(0)\n",
    "        model(inputTensor)\n",
    "      }\n",
    "      py\"$logits[:, -1, :]\"\n",
    "        .pipe(torch.softmax(_, dim = -1))\n",
    "        .pipe(torch.argmax(_, dim = -1, keepdim = true))\n",
    "        .pipe(_.squeeze(0).tolist().as[Seq[Int]].head)\n",
    "        .pipe(nextEncodedOutput => currentEncodedOutput :+ nextEncodedOutput)\n",
    "    }\n",
    "  }.drop(maxNewTokens).head\n",
    "\n",
    "def textToTokenIds(\n",
    "  text: String, \n",
    "  tokenizer: Tokenizer\n",
    "): Vector[Int] = {\n",
    "  val allowedSpecial = py.Dynamic.global.set(Seq(\"<|endoftext|>\").toPythonProxy)\n",
    "  val encodedText = tokenizer.encode(text, allowed_special = allowedSpecial)\n",
    "  torch.tensor(encodedText).tolist().as[Vector[Int]]\n",
    "}\n",
    "    \n",
    "def tokenIdsToText(\n",
    "  tokenIds: Vector[Int], \n",
    "  tokenizer: Tokenizer\n",
    "): String =\n",
    "  tokenizer.decode(tokenIds.toPythonProxy).as[String]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "012a1151-8cc2-4438-8193-6ed503f3ad01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mInstructionDataRecord\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class InstructionDataRecord(\n",
    "  instruction: String,\n",
    "  input: String,\n",
    "  output: String\n",
    ") {\n",
    "  lazy val alpacaFormat: String = {\n",
    "    val formattedInput = if (input.nonEmpty) s\"\\n### Input:\\n$input\\n\" else \"\"\n",
    "    s\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "       |\n",
    "       |### Instruction:\n",
    "       |$instruction\n",
    "       |$formattedInput\n",
    "       |### Response:\n",
    "       |$output\n",
    "       |\"\"\".stripMargin\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bb41593-f9c9-4a42-97f9-b3904f8690e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtestInputRecords\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mInstructionDataRecord\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Evaluate the following phrase by transforming it into the spelling given.\"\u001b[39m,\n",
       "    input = \u001b[32m\"freind --> friend\"\u001b[39m,\n",
       "    output = \u001b[32m\"The spelling of the given phrase \\\"freind\\\" is incorrect, the correct spelling is \\\"friend\\\".\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Edit the following sentence for grammar.\"\u001b[39m,\n",
       "    input = \u001b[32m\"He go to the park every day.\"\u001b[39m,\n",
       "    output = \u001b[32m\"He goes to the park every day.\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Convert 45 kilometers to meters.\"\u001b[39m,\n",
       "    input = \u001b[32m\"\"\u001b[39m,\n",
       "    output = \u001b[32m\"45 kilometers is 45000 meters.\"\u001b[39m\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testInputRecords = List(\n",
    "  InstructionDataRecord(\n",
    "    instruction = \"Evaluate the following phrase by transforming it into the spelling given.\",\n",
    "    input = \"freind --> friend\",\n",
    "    output = \"The spelling of the given phrase \\\"freind\\\" is incorrect, the correct spelling is \\\"friend\\\".\"\n",
    "  ),\n",
    "  InstructionDataRecord(\n",
    "    instruction = \"Edit the following sentence for grammar.\",\n",
    "    input = \"He go to the park every day.\",\n",
    "    output = \"He goes to the park every day.\"\n",
    "  ),\n",
    "  InstructionDataRecord(\n",
    "    instruction = \"Convert 45 kilometers to meters.\",\n",
    "    input = \"\",\n",
    "    output = \"45 kilometers is 45000 meters.\"\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d07d11bd-5fd6-4b0a-b54f-e249fd7febe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct response: The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".\n",
      "Model response: <|endoftext|>, and the following sentence.\n",
      "<|endoftext|>, and the following sentence.\n",
      "<|endoftext|>, the word 'The the following sentence.\n",
      "<|endoftext|>, and the word ', and the following sentence.\n",
      "<|endoftext|>, and the following.\n",
      "\n",
      "<|endoftext|>, the sentence.\n",
      "<|endoftext|>.\n",
      "<|endoftext|>, and the following sentence.\n",
      "\n",
      "<|endoftext|>, and a, the word ', and the following sentence.\n",
      "<|endoftext|>.\n",
      "<|endoftext|>.\n",
      "\n",
      "<|endoftext|> the following sentence.\n",
      "<|endoftext|>\n",
      "<|endoftext|> is a sentence.\n",
      "<|endoftext|>, the following the following, the word ', the sentence.\n",
      "\n",
      "<|endoftext|>, and the following the following sentence.\n",
      "<|endoftext|>, and, and, the sentence.\n",
      "<|endoftext|>\n",
      "<|endoftext|>\n",
      "<|endoftext|>, and, and the following\n",
      "<|endoftext|>, the following sentence.\n",
      "<|endoftext|>, the sentence.\n",
      "<|endoftext|>, the following sentence.\n",
      "<|endoftext|>, and the following sentence.\n",
      "<|endoftext|>, the following\n",
      "<|endoftext|>, the following sentence.\n",
      "### Response:\n",
      "<|endoftext|>, the sentence.\n",
      "ollowing sentence.f\n",
      "<|endoftext|>, and the sentence.\n",
      "<|endoftext|>, and the following the sentence.\n",
      "<|endoftext|>, the following sentence.\n",
      "<|endoftext|>, and the sentence.\n",
      "<|endoftext|>, the following sentence.\n",
      "<|endoftext|>, the word ' the following sentence.\n",
      "<|endoftext|>, and\n",
      "---------------------------------------\n",
      "\n",
      "Correct response: He goes to the park every day.\n",
      "Model response: <|endoftext|>, and the following sentence.\n",
      "<|endoftext|>, and a sentence.\n",
      "<|endoftext|>, and the following sentence.\n",
      "<|endoftext|>, and the following sentence.\n",
      "<|endoftext|>, the word 'I the following sentence.\n",
      "<|endoftext|>, the following sentence.\n",
      "<|endoftext|>, and a, and the word 'I, and the following sentence.\n",
      "<|endoftext|>, the following sentence.\n",
      "<|endoftext|>, the sentence.\n",
      "<|endoftext|>\n",
      "<|endoftext|>, and, and the following sentence.\n",
      "<|endoftext|>, and a, the following.\n",
      "<|endoftext|>\n",
      "\n",
      "<|endoftext|> the word.\n",
      "<|endoftext|>.\n",
      "<|endoftext|>, and, the following the following sentence.\n",
      "<|endoftext|>, the word ', the following sentence, and, and the sentence.\n",
      "<|endoftext|>, and the following sentence.\n",
      "<|endoftext|>, the sentence.\n",
      "<|endoftext|>\n",
      "<|endoftext|>\n",
      "<|endoftext|>, and the sentence using the following sentence.\n",
      "<|endoftext|>, and the sentence.\n",
      "<|endoftext|> the following:\n",
      "<|endoftext|>, and the following sentence.\n",
      "\n",
      "<|endoftext|>\n",
      "<|endoftext|>.\n",
      "<|endoftext|>, and\n",
      " following sentence.he\n",
      "<|endoftext|>.\n",
      "<|endoftext|>, the following sentence.\n",
      "<|endoftext|>, and the following sentence.\n",
      "<|endoftext|>, and the following sentence.\n",
      "<|endoftext|>, and the sentence.\n",
      "<|endoftext|> the sentence.\n",
      "<|endoftext|>, and the sentence.\n",
      "<|endoftext|>, the following sentence\n",
      "---------------------------------------\n",
      "\n",
      "Correct response: 45 kilometers is 45000 meters.\n",
      "Model response: <|endoftext|> is 'The capital of 'The opposite of the sentence.\n",
      "<|endoftext|> the following sentence.\n",
      "<|endoftext|>.\n",
      "<|endoftext|>, and the following sentence.\n",
      "<|endoftext|>.\n",
      "<|endoftext|>, and the following sentence.\n",
      "<|endoftext|>, the sentence to the word ' the word ', and the following sentence.\n",
      "<|endoftext|>.\n",
      "<|endoftext|>, and the word 'I, and the sentence.\n",
      "<|endoftext|>\n",
      "<|endoftext|>:\n",
      "<|endoftext|>.\n",
      "<|endoftext|>.\n",
      "<|endoftext|> is a.\n",
      "<|endoftext|>, and the sentence.\n",
      "<|endoftext|>, and the following:\n",
      "<|endoftext|> the sentence.\n",
      "<|endoftext|>.\n",
      "<|endoftext|>.\n",
      "<|endoftext|>, the sentence.\n",
      "<|endoftext|>:\n",
      "<|endoftext|>, and, the following the following sentence.\n",
      "<|endoftext|>, the following sentence.\n",
      "<|endoftext|>, the following:\n",
      "<|endoftext|>, and the following sentence.\n",
      "<|endoftext|>, and, and the, and the following, and, and the sentence.\n",
      "<|endoftext|>, and the sentence, the sentence to the following sentence.\n",
      "<|endoftext|> the following.\n",
      "<|endoftext|>, and the following sentence.\n",
      "xt|>.ofte\n",
      "<|endoftext|>, and the following\n",
      "<|endoftext|>, and the following sentence.\n",
      "<|endoftext|>, and the\n",
      "<|endoftext|>, the following the following sentence.\n",
      "### Response:\n",
      "<|endoftext|>, and the following.\n",
      "<|endoftext|>, and the sentence.\n",
      "<|endoftext|>.\n",
      "<|endoftext|>\n",
      "---------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testInputRecords.foreach { record =>\n",
    "  val inputText = record.alpacaFormat\n",
    "  val decodedOutputText = py.local {\n",
    "    val outputTextIds = generateTextSimple(\n",
    "      model = model, \n",
    "      maxNewTokens = 256,\n",
    "      contextLength = gptConfig.contextLength\n",
    "    )(\n",
    "      encodedInput = textToTokenIds(inputText, tokenizer)\n",
    "    )\n",
    "    tokenIdsToText(outputTextIds, tokenizer)\n",
    "  }\n",
    "  val responseText = decodedOutputText.drop(inputText.length)\n",
    "  println(s\"Correct response: ${record.output}\")\n",
    "  println(s\"Model response: $responseText\")\n",
    "  println(\"---------------------------------------\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15873006-7690-4c33-9af5-64068eb76998",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.13.14",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
