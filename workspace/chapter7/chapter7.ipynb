{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0f6387d-40fc-4c57-9759-bd3e9ee1a1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $file.^.Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a38067cb-2f01-4202-a38c-58fb530939b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdatasetUrl\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json\"\u001b[39m\n",
       "\u001b[36moutputDir\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"data/instruction-data-raw\"\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datasetUrl = s\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    "val outputDir = \"data/instruction-data-raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "694d7bb4-f008-4cd3-ac3f-cf09faa520bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  198k  100  198k    0     0   378k      0 --:--:-- --:--:-- --:--:--  377k\n"
     ]
    }
   ],
   "source": [
    "Magic.!(\"curl\", \"--create-dirs\", \"-O\", \"--output-dir\", outputDir, datasetUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "012a1151-8cc2-4438-8193-6ed503f3ad01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mInstructionDataRecord\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class InstructionDataRecord(\n",
    "  instruction: String,\n",
    "  input: String,\n",
    "  output: String\n",
    ") {\n",
    "  lazy val alpacaFormat: String = {\n",
    "    val formattedInput = if (input.nonEmpty) s\"\\n### Input:\\n$input\\n\" else \"\"\n",
    "    s\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "       |\n",
    "       |### Instruction:\n",
    "       |$instruction\n",
    "       |$formattedInput\n",
    "       |### Response:\n",
    "       |$output\n",
    "       |\"\"\".stripMargin\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04399692-959b-4da6-8496-b28ca061eaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction data records count: 1100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.io.Source\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mupickle.default.{read, Reader, macroR}\u001b[39m\n",
       "\u001b[36minstructionDataRecordReader\u001b[39m: \u001b[32mReader\u001b[39m[\u001b[32mInstructionDataRecord\u001b[39m] = ammonite.$sess.cmd5$Helper$$anon$1@15673daa\n",
       "\u001b[36mdatasetRaw\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"[\n",
       "    {\n",
       "        \"instruction\": \"Evaluate the following phrase by transforming it into the spelling given.\",\n",
       "        \"input\": \"freind --> friend\",\n",
       "        \"output\": \"The spelling of the given phrase \\\"freind\\\" is incorrect, the correct spelling is \\\"friend\\\".\"\n",
       "    },\n",
       "    {\n",
       "        \"instruction\": \"Edit the following sentence for grammar.\",\n",
       "        \"input\": \"He go to the park every day.\",\n",
       "        \"output\": \"He goes to the park every day.\"\n",
       "    },\n",
       "    {\n",
       "        \"instruction\": \"Convert 45 kilometers to meters.\",\n",
       "        \"input\": \"\",\n",
       "        \"output\": \"45 kilometers is 45000 meters.\"\n",
       "    },\n",
       "    {\n",
       "        \"instruction\": \"Rewrite this sentence to start with 'Although': Despite the rain, they went for a walk.\",\n",
       "        \"input\": \"\",\n",
       "        \"output\": \"Although it was raining, they went for a walk.\"\n",
       "    },\n",
       "    {\n",
       "        \"instruction\": \"What are the first 10 square numbers?\",\n",
       "        \"input\": \"\",\n",
       "        \"output\": \"1, 4, 9, 16, 25, 36, 49, 64, 81, 100.\"\n",
       "    },\n",
       "    {\n",
       "        \"instruction\": \"Suggest a more formal synonym for \\\"happy.\\\"\",\n",
       "        \"input\": \"\",\n",
       "        \"output\": \"A more formal synonym for \\\"happy\\\" is \\\"content.\\\"\"\n",
       "    },\n",
       "    {\n",
       "        \"instruction\": \"Translate the following sentence into French.\",\n",
       "        \"input\": \"Where is the nearest restaurant?\",\n",
       "        \"output\": \"O\\u00f9 est le restaurant le plus proche?\"\n",
       "    },\n",
       "\u001b[39m...\n",
       "defined \u001b[32mtype\u001b[39m \u001b[36mDataset\u001b[39m\n",
       "\u001b[36minstructionDataRecords\u001b[39m: \u001b[32mDataset\u001b[39m = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Evaluate the following phrase by transforming it into the spelling given.\"\u001b[39m,\n",
       "    input = \u001b[32m\"freind --> friend\"\u001b[39m,\n",
       "    output = \u001b[32m\"The spelling of the given phrase \\\"freind\\\" is incorrect, the correct spelling is \\\"friend\\\".\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Edit the following sentence for grammar.\"\u001b[39m,\n",
       "    input = \u001b[32m\"He go to the park every day.\"\u001b[39m,\n",
       "    output = \u001b[32m\"He goes to the park every day.\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Convert 45 kilometers to meters.\"\u001b[39m,\n",
       "    input = \u001b[32m\"\"\u001b[39m,\n",
       "    output = \u001b[32m\"45 kilometers is 45000 meters.\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Rewrite this sentence to start with 'Although': Despite the rain, they went for a walk.\"\u001b[39m,\n",
       "    input = \u001b[32m\"\"\u001b[39m,\n",
       "    output = \u001b[32m\"Although it was raining, they went for a walk.\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"What are the first 10 square numbers?\"\u001b[39m,\n",
       "    input = \u001b[32m\"\"\u001b[39m,\n",
       "    output = \u001b[32m\"1, 4, 9, 16, 25, 36, 49, 64, 81, 100.\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Suggest a more formal synonym for \\\"happy.\\\"\"\u001b[39m,\n",
       "    input = \u001b[32m\"\"\u001b[39m,\n",
       "    output = \u001b[32m\"A more formal synonym for \\\"happy\\\" is \\\"content.\\\"\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Translate the following sentence into French.\"\u001b[39m,\n",
       "    input = \u001b[32m\"Where is the nearest restaurant?\"\u001b[39m,\n",
       "    output = \u001b[32m\"OÃ¹ est le restaurant le plus proche?\"\u001b[39m\n",
       "  ),\n",
       "..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`com.lihaoyi::upickle:4.1.0`\n",
    "\n",
    "import scala.io.Source\n",
    "import upickle.default.{read, Reader, macroR}\n",
    "\n",
    "implicit val instructionDataRecordReader: Reader[InstructionDataRecord] = macroR\n",
    "\n",
    "val datasetRaw = Source.fromFile(s\"$outputDir/instruction-data.json\").mkString\n",
    "type Dataset = Vector[InstructionDataRecord]\n",
    "val instructionDataRecords = read[Dataset](datasetRaw)\n",
    "\n",
    "println(s\"Instruction data records count: ${instructionDataRecords.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2f2f5ca-19c8-4d5a-9430-280f85524013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Evaluate the following phrase by transforming it into the spelling given.\n",
      "\n",
      "### Input:\n",
      "freind --> friend\n",
      "\n",
      "### Response:\n",
      "The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".\n",
      "\n",
      "-----\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert 45 kilometers to meters.\n",
      "\n",
      "### Response:\n",
      "45 kilometers is 45000 meters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(instructionDataRecords.find(_.input.nonEmpty).get.alpacaFormat)\n",
    "println(\"-----\\n\")\n",
    "println(instructionDataRecords.find(_.input.isEmpty).get.alpacaFormat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef595386-7c08-43f2-ab52-984556944781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 935\n",
      "Validation set size: 55\n",
      "Test set size: 110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mtype\u001b[39m \u001b[36mTraining\u001b[39m\n",
       "defined \u001b[32mtype\u001b[39m \u001b[36mValidation\u001b[39m\n",
       "defined \u001b[32mtype\u001b[39m \u001b[36mTest\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36msplit\u001b[39m\n",
       "\u001b[36mtraining\u001b[39m: \u001b[32mTraining\u001b[39m = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Evaluate the following phrase by transforming it into the spelling given.\"\u001b[39m,\n",
       "    input = \u001b[32m\"freind --> friend\"\u001b[39m,\n",
       "    output = \u001b[32m\"The spelling of the given phrase \\\"freind\\\" is incorrect, the correct spelling is \\\"friend\\\".\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Edit the following sentence for grammar.\"\u001b[39m,\n",
       "    input = \u001b[32m\"He go to the park every day.\"\u001b[39m,\n",
       "    output = \u001b[32m\"He goes to the park every day.\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Convert 45 kilometers to meters.\"\u001b[39m,\n",
       "    input = \u001b[32m\"\"\u001b[39m,\n",
       "    output = \u001b[32m\"45 kilometers is 45000 meters.\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Rewrite this sentence to start with 'Although': Despite the rain, they went for a walk.\"\u001b[39m,\n",
       "    input = \u001b[32m\"\"\u001b[39m,\n",
       "    output = \u001b[32m\"Although it was raining, they went for a walk.\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"What are the first 10 square numbers?\"\u001b[39m,\n",
       "    input = \u001b[32m\"\"\u001b[39m,\n",
       "    output = \u001b[32m\"1, 4, 9, 16, 25, 36, 49, 64, 81, 100.\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Suggest a more formal synonym for \\\"happy.\\\"\"\u001b[39m,\n",
       "    input = \u001b[32m\"\"\u001b[39m,\n",
       "    output = \u001b[32m\"A more formal synonym for \\\"happy\\\" is \\\"content.\\\"\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Translate the following sentence into French.\"\u001b[39m,\n",
       "    input = \u001b[32m\"Where is the nearest restaurant?\"\u001b[39m,\n",
       "    output = \u001b[32m\"OÃ¹ est le restaurant le plus proche?\"\u001b[39m\n",
       "  ),\n",
       "...\n",
       "\u001b[36mvalidation\u001b[39m: \u001b[32mValidation\u001b[39m = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Rewrite the sentence using a simile.\"\u001b[39m,\n",
       "    input = \u001b[32m\"The car is very fast.\"\u001b[39m,\n",
       "    output = \u001b[32m\"The car is as fast as lightning.\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"What type of cloud is typically associated with thunderstorms?\"\u001b[39m,\n",
       "    input = \u001b[32m\"\"\u001b[39m,\n",
       "    output = \u001b[32m\"The type of cloud typically associated with thunderstorms is cumulonimbus.\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Name the author of 'Pride and Prejudice'.\"\u001b[39m,\n",
       "    input = \u001b[32m\"\"\u001b[39m,\n",
       "    output = \u001b[32m\"Jane Austen.\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"What is the periodic symbol for chlorine?\"\u001b[39m,\n",
       "    input = \u001b[32m\"\"\u001b[39m,\n",
       "    output = \u001b[32m\"The periodic symbol for chlorine is Cl.\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Correct the punctuation in the sentence.\"\u001b[39m,\n",
       "    input = \u001b[32m\"Its time to go home.\"\u001b[39m,\n",
       "    output = \u001b[32m\"The corrected sentence should be: 'It's time to go home.'\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Rewrite the sentence.\"\u001b[39m,\n",
       "    input = \u001b[32m\"The lecture was delivered in a clear manner.\"\u001b[39m,\n",
       "    output = \u001b[32m\"The lecture was delivered clearly.\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Generate a humorous anecdote.\"\u001b[39m,\n",
       "    input = \u001b[32m\"\"\u001b[39m,\n",
       "    output = \u001b[32m\"Why was the math book sad? Because it had too many problems!\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "...\n",
       "\u001b[36mtest\u001b[39m: \u001b[32mTest\u001b[39m = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Explain the primary function of the human heart.\"\u001b[39m,\n",
       "    input = \u001b[32m\"\"\u001b[39m,\n",
       "    output = \u001b[32m\"The primary function of the human heart is to pump blood throughout the body, delivering oxygen and nutrients to tissues and removing carbon dioxide and other wastes.\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Reword the following sentence to the future tense.\"\u001b[39m,\n",
       "    input = \u001b[32m\"He is reading a novel inspired by his grandmother.\"\u001b[39m,\n",
       "    output = \u001b[32m\"He will be reading a novel inspired by his grandmother.\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Convert the given sentence into active voice.\"\u001b[39m,\n",
       "    input = \u001b[32m\"The law was passed by the government.\"\u001b[39m,\n",
       "    output = \u001b[32m\"The government passed the law.\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Create a sentence using the word 'inevitable'.\"\u001b[39m,\n",
       "    input = \u001b[32m\"\"\u001b[39m,\n",
       "    output = \u001b[32m\"The confrontation was inevitable given the circumstances.\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Categorize the following sentence as either factual or opinion-based.\"\u001b[39m,\n",
       "    input = \u001b[32m\"Chocolate is the best dessert.\"\u001b[39m,\n",
       "    output = \u001b[32m\"Opinion-based.\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"What is an antonym of 'old'?\"\u001b[39m,\n",
       "    input = \u001b[32m\"\"\u001b[39m,\n",
       "    output = \u001b[32m\"young.\"\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mInstructionDataRecord\u001b[39m(\n",
       "    instruction = \u001b[32m\"Provide a synonym for 'hardworking'.\"\u001b[39m,\n",
       "    input = \u001b[32m\"\"\u001b[39m,\n",
       "    output = \u001b[32m\"A synonym for 'hardworking' is 'diligent'.\"\u001b[39m\n",
       "  ),\n",
       "..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type Training = Dataset\n",
    "type Validation = Dataset\n",
    "type Test = Dataset\n",
    "\n",
    "def split(dataset: Dataset, trainingFraction: Double, validationFraction: Double): (Training, Validation, Test) = {\n",
    "  val trainingSize = (dataset.size * trainingFraction).floor.toInt\n",
    "  val validationSize = (dataset.size * validationFraction).floor.toInt\n",
    "\n",
    "  val (training, remainingRecords) = dataset.splitAt(trainingSize)\n",
    "  val (validation, test) = remainingRecords.splitAt(validationSize)\n",
    "  (training, validation, test)\n",
    "}\n",
    "\n",
    "val (training, validation, test) = split(instructionDataRecords, trainingFraction = 0.85, validationFraction = 0.05) \n",
    "\n",
    "println(s\"Training set size: ${training.size}\")\n",
    "println(s\"Validation set size: ${validation.size}\")\n",
    "println(s\"Test set size: ${test.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0ae12d3-46af-46a9-a959-70da91c1fabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mme.shadaj.scalapy.py\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mpy.SeqConverters\u001b[39m\n",
       "defined \u001b[32mtype\u001b[39m \u001b[36mTokenizer\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mInstructionDataset\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`dev.scalapy::scalapy-core:0.5.3`\n",
    "\n",
    "import me.shadaj.scalapy.py\n",
    "import py.SeqConverters\n",
    "\n",
    "type Tokenizer = py.Dynamic\n",
    "\n",
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"from torch.utils.data import Dataset\n",
    "     |\n",
    "     |class InstructionDataset(Dataset):\n",
    "     |  def __init__(self, init):\n",
    "     |    init(self)\n",
    "     |\n",
    "     |  def __getitem__(self, index):\n",
    "     |    return self.getItem(index)\n",
    "     |  \n",
    "     |  def __len__(self):\n",
    "     |    return self.len()\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def InstructionDataset(\n",
    "  dataset: Dataset,\n",
    "  tokenizer: Tokenizer\n",
    "): py.Dynamic = {\n",
    "  val encodedTexts = dataset.map(_.alpacaFormat).map(tokenizer.encode(_).as[Seq[Int]])\n",
    "    \n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.maxLength = encodedTexts.head.length\n",
    "    \n",
    "    val getItem = (index: Int) => encodedTexts(index).toPythonProxy\n",
    "    self.getItem = getItem\n",
    "\n",
    "    val len = () => dataset.size\n",
    "    self.len = len\n",
    "  }\n",
    "  py.Dynamic.global.InstructionDataset(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "313d96b0-a719-49dc-ac3f-cae678c1a055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.chaining._\u001b[39m\n",
       "defined \u001b[32mtype\u001b[39m \u001b[36mDevice\u001b[39m\n",
       "defined \u001b[32mtype\u001b[39m \u001b[36mTorchTensor\u001b[39m\n",
       "\u001b[36mtorch\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mModule\u001b[39m = <module 'torch' from '/usr/local/lib/python3.12/site-packages/torch/__init__.py'>\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcollate\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.chaining._\n",
    "\n",
    "type Device = py.Dynamic\n",
    "type TorchTensor = py.Dynamic\n",
    "\n",
    "val torch = py.module(\"torch\")\n",
    "\n",
    "def collate(device: Device)(\n",
    "  batch: Vector[Vector[Int]],\n",
    "  paddingTokenId: Int = 50_256,\n",
    "  ignoreIndex: Int = -100,\n",
    "  allowedMaxLength: Option[Int] = None\n",
    "): (TorchTensor, TorchTensor) = {\n",
    "  val batchMaxLength = batch.map(_.length).max + 1\n",
    "  val (inputs, targets) = batch\n",
    "    .map(_ :+ paddingTokenId)\n",
    "    .map(_.padTo(batchMaxLength, paddingTokenId))\n",
    "    .map { paddedItem =>\n",
    "      val maxLength = allowedMaxLength.getOrElse(Int.MaxValue)\n",
    "      val inputs = paddedItem.init.take(maxLength)\n",
    "      val targets = paddedItem.sliding(2).foldLeft(Vector.empty[Int]) { // skip head and apply mask to final padding tokens\n",
    "        case (acc, Vector(a, b)) if a == paddingTokenId => acc :+ ignoreIndex\n",
    "        case (acc, Vector(_, b))                        => acc :+ b\n",
    "        case (acc, _)                                   => acc\n",
    "      }.take(maxLength)\n",
    "      (inputs, targets)\n",
    "    }\n",
    "    .unzip\n",
    "\n",
    "  def stackTensor(batch: Vector[Vector[Int]]): TorchTensor =\n",
    "    batch\n",
    "      .map(_.toPythonProxy)\n",
    "      .map(torch.tensor(_))\n",
    "      .toPythonProxy\n",
    "      .pipe(py.Dynamic.global.tuple(_))\n",
    "      .pipe(torch.stack(_).to(device))\n",
    "\n",
    "  (stackTensor(inputs), stackTensor(targets))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94255760-1979-4470-94b9-6e9589934e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3],\n",
      "        [    5,     6, 50256, 50256],\n",
      "        [    7,     8,     9, 50256]])\n",
      "tensor([[    1,     2,     3,     4],\n",
      "        [    6, 50256,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdevice\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = cpu\n",
       "\u001b[36mexampleBatch\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mVector\u001b[39m[\u001b[32mInt\u001b[39m]] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[32m5\u001b[39m, \u001b[32m6\u001b[39m),\n",
       "  \u001b[33mVector\u001b[39m(\u001b[32m7\u001b[39m, \u001b[32m8\u001b[39m, \u001b[32m9\u001b[39m)\n",
       ")\n",
       "\u001b[36mexampleInputs\u001b[39m: \u001b[32mTorchTensor\u001b[39m = tensor([[    0,     1,     2,     3],\n",
       "        [    5,     6, 50256, 50256],\n",
       "        [    7,     8,     9, 50256]])\n",
       "\u001b[36mexampleTargets\u001b[39m: \u001b[32mTorchTensor\u001b[39m = tensor([[    1,     2,     3,     4],\n",
       "        [    6, 50256,  -100,  -100],\n",
       "        [    8,     9, 50256,  -100]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val device = torch.device(if (torch.cuda.is_available().as[Boolean]) \"cuda\" else \"cpu\")\n",
    "val exampleBatch = Vector(\n",
    "  Vector(0, 1, 2, 3, 4),\n",
    "  Vector(5, 6),\n",
    "  Vector(7, 8, 9)\n",
    ")\n",
    "val (exampleInputs, exampleTargets) = collate(device)(exampleBatch, allowedMaxLength = Some(4))\n",
    "println(exampleInputs)\n",
    "println(exampleTargets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "289a1b23-a709-46e9-a549-40fddebb4b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken==0.7.* in /usr/local/lib/python3.12/site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/site-packages (from tiktoken==0.7.*) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/site-packages (from tiktoken==0.7.*) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.7.*) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.7.*) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.7.*) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.7.*) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "Magic.!(\"pip\", \"install\", \"tiktoken==0.7.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1f9f976-69fa-4bad-9b89-d5dec3e5e5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtiktoken\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mModule\u001b[39m = <module 'tiktoken' from '/usr/local/lib/python3.12/site-packages/tiktoken/__init__.py'>\n",
       "\u001b[36mtokenizer\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <Encoding 'gpt2'>\n",
       "\u001b[36mtrainingDataset\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <InstructionDataset object at 0xffff6ab6a3f0>\n",
       "\u001b[36mvalidationDataset\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <InstructionDataset object at 0xffff6ab6a090>\n",
       "\u001b[36mtestDataset\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <InstructionDataset object at 0xffff6ab68bf0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tiktoken = py.module(\"tiktoken\")\n",
    "\n",
    "val tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "val trainingDataset = InstructionDataset(training, tokenizer)\n",
    "val validationDataset = InstructionDataset(validation, tokenizer)\n",
    "val testDataset = InstructionDataset(test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1832f5d-7bd6-4d42-bbef-9959b6f3d636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mGPTConfig\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class GPTConfig(\n",
    "  vocabularySize: Int,\n",
    "  contextLength: Int,\n",
    "  embeddingDimension: Int,\n",
    "  attentionHeadsCount: Int,\n",
    "  layersCount: Int,\n",
    "  dropoutRate: Double,\n",
    "  queryKeyValueBias: Boolean\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a6b593c-46ea-47cb-b7a4-3bbbf9718b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mpy.PyQuote\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mMultiHeadAttention\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import py.PyQuote\n",
    "\n",
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class MultiHeadAttention(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def MultiHeadAttention(\n",
    "  inputDimension: Int,\n",
    "  outputDimension: Int,\n",
    "  dropoutProbability: Double,\n",
    "  contextLength: Int,\n",
    "  headsCount: Int,\n",
    "  queryKeyValueBias: Boolean\n",
    "): py.Dynamic = {\n",
    "  assert(outputDimension % headsCount == 0, \"Output dimension must be a multiple of heads count\")\n",
    "  val headDimension = outputDimension / headsCount\n",
    "    \n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.weightsQuery = torch.nn.Linear(inputDimension, outputDimension, bias = queryKeyValueBias)\n",
    "    self.weightsKey = torch.nn.Linear(inputDimension, outputDimension, bias = queryKeyValueBias)\n",
    "    self.weightsValue = torch.nn.Linear(inputDimension, outputDimension, bias = queryKeyValueBias)\n",
    "    self.outputProjection = torch.nn.Linear(outputDimension, outputDimension)\n",
    "    self.dropout = torch.nn.Dropout(dropoutProbability)\n",
    "    self.register_buffer(\"mask\", torch.triu(torch.ones(contextLength, contextLength), diagonal = 1))\n",
    "      \n",
    "    val forward = (batchedInputs: TorchTensor) => {\n",
    "      val (batchesCount, tokensCount, tokenDimension) = batchedInputs.shape.as[(Int, Int, Int)]\n",
    "      val queries = self.weightsQuery(batchedInputs)\n",
    "        .view(batchesCount, tokensCount, headsCount, headDimension)\n",
    "        .transpose(1, 2)\n",
    "      val keys = self.weightsKey(batchedInputs)\n",
    "        .view(batchesCount, tokensCount, headsCount, headDimension)\n",
    "        .transpose(1, 2)\n",
    "      val values = self.weightsValue(batchedInputs)\n",
    "        .view(batchesCount, tokensCount, headsCount, headDimension)\n",
    "        .transpose(1, 2)\n",
    "      val attentionScores = py\"$queries @ $keys.transpose(2, 3)\"\n",
    "      attentionScores.masked_fill_(py\"${self.mask}.bool()[:$tokensCount, :$tokensCount]\", -torch.inf)\n",
    "      val attentionWeights = self.dropout(torch.softmax(py\"$attentionScores / $headDimension**0.5\", dim = -1))\n",
    "      self.outputProjection(\n",
    "        py\"$attentionWeights @ $values\"\n",
    "          .transpose(1, 2)\n",
    "          .reshape(batchesCount, tokensCount, outputDimension)\n",
    "      )\n",
    "    }\n",
    "    self.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.MultiHeadAttention(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d570669a-847e-405d-9425-315e2730b6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mGELU\u001b[39m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "// Because it mostly uses Python operators, it's implemented fully in Python\n",
    "py.exec {\n",
    "  s\"\"\"import torch\n",
    "     |import torch.nn as nn\n",
    "     |\n",
    "     |class GELU(nn.Module):\n",
    "     |  def __init__(self):\n",
    "     |    super().__init__()\n",
    "     |\n",
    "     |  def forward(self, inputs):\n",
    "     |    return 0.5 * inputs * (\n",
    "     |      1 + torch.tanh(\n",
    "     |        torch.sqrt(torch.tensor(2.0 / torch.pi)) * (inputs + 0.044715 * torch.pow(inputs, 3))\n",
    "     |      )\n",
    "     |    )\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def GELU() = py.Dynamic.global.GELU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a23495f2-c203-4612-a1a4-85d99e28a9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mFeedForward\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class FeedForward(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def FeedForward(\n",
    "  embeddingDimension: Int\n",
    "): py.Dynamic = {\n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.layers = torch.nn.Sequential(\n",
    "      torch.nn.Linear(embeddingDimension, 4 * embeddingDimension),\n",
    "      GELU(),\n",
    "      torch.nn.Linear(4 * embeddingDimension, embeddingDimension)\n",
    "    )\n",
    "      \n",
    "    val forward = (inputs: TorchTensor) => self.layers(inputs)\n",
    "    self.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.FeedForward(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b03b363-da08-4ba4-b265-450334ac88a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mNormalizationLayer\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class NormalizationLayer(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def NormalizationLayer(\n",
    "  embeddingDimension: Int\n",
    "): py.Dynamic = {\n",
    "  val epsilon = 1e-5\n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.scale = torch.nn.Parameter(torch.ones(embeddingDimension))\n",
    "    self.shift = torch.nn.Parameter(torch.zeros(embeddingDimension))\n",
    "      \n",
    "    val forward = (inputs: TorchTensor) => {\n",
    "      val mean = inputs.mean(dim = -1, keepdim = true)\n",
    "      val variance = inputs.`var`(dim = -1, keepdim = true, unbiased = false)\n",
    "      val normalizedInputs = py\"($inputs - $mean) / torch.sqrt($variance + $epsilon)\"\n",
    "      py\"${self.scale} * $normalizedInputs + ${self.shift}\"\n",
    "    }\n",
    "    self.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.NormalizationLayer(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef4eb2a4-ec74-4954-9134-554a50935510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mTransformerBlock\u001b[39m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class TransformerBlock(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def TransformerBlock(\n",
    "  config: GPTConfig\n",
    "): py.Dynamic = {\n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.multiHeadAttention = MultiHeadAttention(\n",
    "      inputDimension = config.embeddingDimension,\n",
    "      outputDimension = config.embeddingDimension,\n",
    "      dropoutProbability = config.dropoutRate,\n",
    "      contextLength = config.contextLength,\n",
    "      headsCount = config.attentionHeadsCount,\n",
    "      queryKeyValueBias = config.queryKeyValueBias\n",
    "    )\n",
    "    self.feedForward = FeedForward(config.embeddingDimension)\n",
    "    self.normalization1 = NormalizationLayer(config.embeddingDimension)\n",
    "    self.normalization2 = NormalizationLayer(config.embeddingDimension)\n",
    "    self.dropoutShortcut = torch.nn.Dropout(config.dropoutRate)\n",
    "    \n",
    "    val forward = (inputs: TorchTensor) => {\n",
    "      val shortcut = inputs\n",
    "      val newShortcut = inputs\n",
    "        .pipe(self.normalization1(_))\n",
    "        .pipe(self.multiHeadAttention(_))\n",
    "        .pipe(self.dropoutShortcut(_))\n",
    "        .pipe(o => py\"$o + $shortcut\")\n",
    "      newShortcut\n",
    "        .pipe(self.normalization2(_))\n",
    "        .pipe(self.feedForward(_))\n",
    "        .pipe(self.dropoutShortcut(_))\n",
    "        .pipe(o => py\"$o + $newShortcut\")\n",
    "    }\n",
    "    self.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.TransformerBlock(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0440b509-feb1-4fde-8c70-205d565502f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mtype\u001b[39m \u001b[36mModel\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mGPTModel\u001b[39m"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class GPTModel(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "type Model = py.Dynamic\n",
    "def GPTModel(\n",
    "  config: GPTConfig\n",
    "): Model = {\n",
    "  val transformerBlocks = Seq.fill(config.layersCount)(TransformerBlock(config))\n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.tokenEmbeddingLayer = torch.nn.Embedding(config.vocabularySize, config.embeddingDimension)\n",
    "    self.positionEmbeddingLayer = torch.nn.Embedding(config.contextLength, config.embeddingDimension)\n",
    "    self.dropoutEmbeddingLayer = torch.nn.Dropout(config.dropoutRate)\n",
    "    self.transformerBlocksLayer = py\"nn.Sequential(*${transformerBlocks.toPythonProxy})\"\n",
    "    self.finalNormalizationLayer = NormalizationLayer(config.embeddingDimension)\n",
    "    self.outputLayer = torch.nn.Linear(config.embeddingDimension, config.vocabularySize, bias = false)\n",
    "      \n",
    "    val forward = (batchedInputs: TorchTensor) => {\n",
    "      val (_, sequenceLength) = batchedInputs.shape.as[(Int, Int)]\n",
    "      val tokenEmbeddings = self.tokenEmbeddingLayer(batchedInputs)\n",
    "      val positionEmbeddings = self.positionEmbeddingLayer(torch.arange(sequenceLength, device = batchedInputs.device))\n",
    "      py\"$tokenEmbeddings + $positionEmbeddings\"\n",
    "        .pipe(self.dropoutEmbeddingLayer(_))\n",
    "        .pipe(self.transformerBlocksLayer(_))\n",
    "        .pipe(self.finalNormalizationLayer(_))\n",
    "        .pipe(self.outputLayer(_))\n",
    "    }\n",
    "    self.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.GPTModel(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3abc72ea-e0ba-4bda-afad-a3b985d841e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mbaseUrl\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2/124M\"\u001b[39m\n",
       "\u001b[36mhparamsFilename\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"hparams.json\"\u001b[39m\n",
       "\u001b[36mfilenames\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[32m\"checkpoint\"\u001b[39m,\n",
       "  \u001b[32m\"encoder.json\"\u001b[39m,\n",
       "  \u001b[32m\"hparams.json\"\u001b[39m,\n",
       "  \u001b[32m\"model.ckpt.data-00000-of-00001\"\u001b[39m,\n",
       "  \u001b[32m\"model.ckpt.index\"\u001b[39m,\n",
       "  \u001b[32m\"model.ckpt.meta\"\u001b[39m,\n",
       "  \u001b[32m\"vocab.bpe\"\u001b[39m\n",
       ")\n",
       "\u001b[36moutputDir\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"data/openai124M\"\u001b[39m"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val baseUrl = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2/124M\"\n",
    "// val baseUrl = \"https://openaipublic.blob.core.windows.net/gpt-2/models/124M\" // backup\n",
    "val hparamsFilename = \"hparams.json\"\n",
    "val filenames = List(\"checkpoint\", \"encoder.json\", hparamsFilename, \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\", \"model.ckpt.meta\", \"vocab.bpe\")\n",
    "\n",
    "val outputDir = \"data/openai124M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1ded2e4-408d-44dd-aa0c-ea428968065a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100    77  100    77    0     0     69      0  0:00:01  0:00:01 --:--:--    69\n",
      "100    77  100    77    0     0     69      0  0:00:01  0:00:01 --:--:--    69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading encoder.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  1 1017k    1 15946    0     0  20643      0  0:00:50 --:--:--  0:00:50 20628\n",
      "100 1017k  100 1017k    0     0   590k      0  0:00:01  0:00:01 --:--:--  591k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading hparams.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100    90  100    90    0     0    139      0 --:--:-- --:--:-- --:--:--   139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model.ckpt.data-00000-of-00001...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0  474M    0  191k    0     0   139k      0  0:58:09  0:00:01  0:58:08  139k\n",
      "  1  474M    1 6815k    0     0  2871k      0  0:02:49  0:00:02  0:02:47 2870k\n",
      "  4  474M    4 22.1M    0     0  6725k      0  0:01:12  0:00:03  0:01:09 6725k\n",
      "  8  474M    8 38.6M    0     0  8969k      0  0:00:54  0:00:04  0:00:50 8968k\n",
      " 11  474M   11 54.7M    0     0  10.1M      0  0:00:46  0:00:05  0:00:41 10.9M\n",
      " 14  474M   14 69.5M    0     0  10.8M      0  0:00:43  0:00:06  0:00:37 13.8M\n",
      " 18  474M   18 87.0M    0     0  11.7M      0  0:00:40  0:00:07  0:00:33 16.0M\n",
      " 21  474M   21  102M    0     0  12.0M      0  0:00:39  0:00:08  0:00:31 15.6M\n",
      " 24  474M   24  118M    0     0  12.4M      0  0:00:38  0:00:09  0:00:29 15.6M\n",
      " 28  474M   28  134M    0     0  12.9M      0  0:00:36  0:00:10  0:00:26 16.0M\n",
      " 31  474M   31  150M    0     0  13.2M      0  0:00:35  0:00:11  0:00:24 16.2M\n",
      " 34  474M   34  161M    0     0  13.0M      0  0:00:36  0:00:12  0:00:24 14.8M\n",
      " 37  474M   37  177M    0     0  13.1M      0  0:00:36  0:00:13  0:00:23 14.9M\n",
      " 40  474M   40  192M    0     0  13.3M      0  0:00:35  0:00:14  0:00:21 15.0M\n",
      " 43  474M   43  208M    0     0  13.4M      0  0:00:35  0:00:15  0:00:20 14.4M\n",
      " 46  474M   46  220M    0     0  13.4M      0  0:00:35  0:00:16  0:00:19 13.9M\n",
      " 50  474M   50  238M    0     0  13.6M      0  0:00:34  0:00:17  0:00:17 15.4M\n",
      " 53  474M   53  253M    0     0  13.7M      0  0:00:34  0:00:18  0:00:16 15.2M\n",
      " 56  474M   56  268M    0     0  13.8M      0  0:00:34  0:00:19  0:00:15 15.1M\n",
      " 59  474M   59  281M    0     0  13.8M      0  0:00:34  0:00:20  0:00:14 15.0M\n",
      " 62  474M   62  297M    0     0  13.9M      0  0:00:34  0:00:21  0:00:13 15.3M\n",
      " 65  474M   65  312M    0     0  13.9M      0  0:00:33  0:00:22  0:00:11 14.9M\n",
      " 69  474M   69  327M    0     0  14.0M      0  0:00:33  0:00:23  0:00:10 15.1M\n",
      " 72  474M   72  343M    0     0  14.0M      0  0:00:33  0:00:24  0:00:09 14.9M\n",
      " 75  474M   75  360M    0     0  14.1M      0  0:00:33  0:00:25  0:00:08 15.6M\n",
      " 77  474M   77  370M    0     0  14.0M      0  0:00:33  0:00:26  0:00:07 14.5M\n",
      " 81  474M   81  385M    0     0  13.9M      0  0:00:33  0:00:27  0:00:06 14.1M\n",
      " 84  474M   84  399M    0     0  14.0M      0  0:00:33  0:00:28  0:00:05 14.4M\n",
      " 87  474M   87  415M    0     0  14.1M      0  0:00:33  0:00:29  0:00:04 14.4M\n",
      " 91  474M   91  433M    0     0  14.2M      0  0:00:33  0:00:30  0:00:03 14.3M\n",
      " 94  474M   94  447M    0     0  14.2M      0  0:00:33  0:00:31  0:00:02 15.6M\n",
      " 97  474M   97  463M    0     0  14.3M      0  0:00:33  0:00:32  0:00:01 16.0M\n",
      "100  474M  100  474M    0     0  14.3M      0  0:00:33  0:00:33 --:--:-- 16.0M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model.ckpt.index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  5215  100  5215    0     0   7411      0 --:--:-- --:--:-- --:--:--  7407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model.ckpt.meta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      " 97  460k   97  447k    0     0   283k      0  0:00:01  0:00:01 --:--:--  283k\n",
      "100  460k  100  460k    0     0   290k      0  0:00:01  0:00:01 --:--:--  290k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading vocab.bpe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  8  445k    8 40518    0     0  41177      0  0:00:11 --:--:--  0:00:11 41176\n",
      "100  445k  100  445k    0     0   280k      0  0:00:01  0:00:01 --:--:--  280k\n"
     ]
    }
   ],
   "source": [
    "filenames.foreach { filename =>\n",
    "  println(s\"Downloading $filename...\")\n",
    "  Magic.!(\"curl\", \"--create-dirs\", \"-O\", \"--output-dir\", outputDir, s\"$baseUrl/$filename\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a963a0-16bd-4e84-a703-f6efc7c7fcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.16.* in /usr/local/lib/python3.12/site-packages (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (25.1.24)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (4.25.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /usr/local/lib/python3.12/site-packages (from tensorflow==2.16.*) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.16.*) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow==2.16.*) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow==2.16.*) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow==2.16.*) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.*) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.*) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.*) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.*) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.*) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.*) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.*) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.*) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.*) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.*) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.*) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "Magic.!(\"pip\", \"install\", \"tensorflow==2.16.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24ab51b2-4675-42ce-ae2d-3cde27857760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mscala.io.Source\u001b[39m\n",
       "\u001b[36mhparamsMap\u001b[39m: \u001b[32mujson\u001b[39m.\u001b[32mValue\u001b[39m.\u001b[32mValue\u001b[39m = \u001b[33mObj\u001b[39m(\n",
       "  value = \u001b[33mMap\u001b[39m(\n",
       "    \u001b[32m\"n_vocab\"\u001b[39m -> \u001b[33mNum\u001b[39m(value = \u001b[32m50257.0\u001b[39m),\n",
       "    \u001b[32m\"n_ctx\"\u001b[39m -> \u001b[33mNum\u001b[39m(value = \u001b[32m1024.0\u001b[39m),\n",
       "    \u001b[32m\"n_embd\"\u001b[39m -> \u001b[33mNum\u001b[39m(value = \u001b[32m768.0\u001b[39m),\n",
       "    \u001b[32m\"n_head\"\u001b[39m -> \u001b[33mNum\u001b[39m(value = \u001b[32m12.0\u001b[39m),\n",
       "    \u001b[32m\"n_layer\"\u001b[39m -> \u001b[33mNum\u001b[39m(value = \u001b[32m12.0\u001b[39m)\n",
       "  )\n",
       ")\n",
       "\u001b[36mgptConfig\u001b[39m: \u001b[32mGPTConfig\u001b[39m = \u001b[33mGPTConfig\u001b[39m(\n",
       "  vocabularySize = \u001b[32m50257\u001b[39m,\n",
       "  contextLength = \u001b[32m1024\u001b[39m,\n",
       "  embeddingDimension = \u001b[32m768\u001b[39m,\n",
       "  attentionHeadsCount = \u001b[32m12\u001b[39m,\n",
       "  layersCount = \u001b[32m12\u001b[39m,\n",
       "  dropoutRate = \u001b[32m0.1\u001b[39m,\n",
       "  queryKeyValueBias = \u001b[32mtrue\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.io.Source\n",
    "\n",
    "val hparamsMap = ujson.read(Source.fromFile(s\"$outputDir/$hparamsFilename\").mkString)\n",
    "\n",
    "val gptConfig = GPTConfig(\n",
    "  vocabularySize = hparamsMap(\"n_vocab\").num.toInt,\n",
    "  contextLength = hparamsMap(\"n_ctx\").num.toInt,\n",
    "  embeddingDimension = hparamsMap(\"n_embd\").num.toInt,\n",
    "  attentionHeadsCount = hparamsMap(\"n_head\").num.toInt,\n",
    "  layersCount = hparamsMap(\"n_layer\").num.toInt,\n",
    "  dropoutRate = 0.1,\n",
    "  queryKeyValueBias = true\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e278c7c-cfe6-456c-a34b-647a658f862e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtf\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mModule\u001b[39m = <module 'tensorflow' from '/usr/local/lib/python3.12/site-packages/tensorflow/__init__.py'>\n",
       "\u001b[36mnp\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mModule\u001b[39m = <module 'numpy' from '/usr/local/lib/python3.12/site-packages/numpy/__init__.py'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tf = py.module(\"tensorflow\")\n",
    "val np = py.module(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e87c5c5b-60fc-47f0-b4fc-90025eec3add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/h0/attn/c_attn/b\n",
      "model/h0/attn/c_attn/w\n",
      "model/h0/attn/c_proj/b\n",
      "model/h0/attn/c_proj/w\n",
      "model/h0/ln_1/b\n",
      "model/h0/ln_1/g\n",
      "model/h0/ln_2/b\n",
      "model/h0/ln_2/g\n",
      "model/h0/mlp/c_fc/b\n",
      "model/h0/mlp/c_fc/w\n",
      "model/h0/mlp/c_proj/b\n",
      "model/h0/mlp/c_proj/w\n",
      "model/h1/attn/c_attn/b\n",
      "model/h1/attn/c_attn/w\n",
      "model/h1/attn/c_proj/b\n",
      "model/h1/attn/c_proj/w\n",
      "model/h1/ln_1/b\n",
      "model/h1/ln_1/g\n",
      "model/h1/ln_2/b\n",
      "model/h1/ln_2/g\n",
      "model/h1/mlp/c_fc/b\n",
      "model/h1/mlp/c_fc/w\n",
      "model/h1/mlp/c_proj/b\n",
      "model/h1/mlp/c_proj/w\n",
      "model/h10/attn/c_attn/b\n",
      "model/h10/attn/c_attn/w\n",
      "model/h10/attn/c_proj/b\n",
      "model/h10/attn/c_proj/w\n",
      "model/h10/ln_1/b\n",
      "model/h10/ln_1/g\n",
      "model/h10/ln_2/b\n",
      "model/h10/ln_2/g\n",
      "model/h10/mlp/c_fc/b\n",
      "model/h10/mlp/c_fc/w\n",
      "model/h10/mlp/c_proj/b\n",
      "model/h10/mlp/c_proj/w\n",
      "model/h11/attn/c_attn/b\n",
      "model/h11/attn/c_attn/w\n",
      "model/h11/attn/c_proj/b\n",
      "model/h11/attn/c_proj/w\n",
      "model/h11/ln_1/b\n",
      "model/h11/ln_1/g\n",
      "model/h11/ln_2/b\n",
      "model/h11/ln_2/g\n",
      "model/h11/mlp/c_fc/b\n",
      "model/h11/mlp/c_fc/w\n",
      "model/h11/mlp/c_proj/b\n",
      "model/h11/mlp/c_proj/w\n",
      "model/h2/attn/c_attn/b\n",
      "model/h2/attn/c_attn/w\n",
      "model/h2/attn/c_proj/b\n",
      "model/h2/attn/c_proj/w\n",
      "model/h2/ln_1/b\n",
      "model/h2/ln_1/g\n",
      "model/h2/ln_2/b\n",
      "model/h2/ln_2/g\n",
      "model/h2/mlp/c_fc/b\n",
      "model/h2/mlp/c_fc/w\n",
      "model/h2/mlp/c_proj/b\n",
      "model/h2/mlp/c_proj/w\n",
      "model/h3/attn/c_attn/b\n",
      "model/h3/attn/c_attn/w\n",
      "model/h3/attn/c_proj/b\n",
      "model/h3/attn/c_proj/w\n",
      "model/h3/ln_1/b\n",
      "model/h3/ln_1/g\n",
      "model/h3/ln_2/b\n",
      "model/h3/ln_2/g\n",
      "model/h3/mlp/c_fc/b\n",
      "model/h3/mlp/c_fc/w\n",
      "model/h3/mlp/c_proj/b\n",
      "model/h3/mlp/c_proj/w\n",
      "model/h4/attn/c_attn/b\n",
      "model/h4/attn/c_attn/w\n",
      "model/h4/attn/c_proj/b\n",
      "model/h4/attn/c_proj/w\n",
      "model/h4/ln_1/b\n",
      "model/h4/ln_1/g\n",
      "model/h4/ln_2/b\n",
      "model/h4/ln_2/g\n",
      "model/h4/mlp/c_fc/b\n",
      "model/h4/mlp/c_fc/w\n",
      "model/h4/mlp/c_proj/b\n",
      "model/h4/mlp/c_proj/w\n",
      "model/h5/attn/c_attn/b\n",
      "model/h5/attn/c_attn/w\n",
      "model/h5/attn/c_proj/b\n",
      "model/h5/attn/c_proj/w\n",
      "model/h5/ln_1/b\n",
      "model/h5/ln_1/g\n",
      "model/h5/ln_2/b\n",
      "model/h5/ln_2/g\n",
      "model/h5/mlp/c_fc/b\n",
      "model/h5/mlp/c_fc/w\n",
      "model/h5/mlp/c_proj/b\n",
      "model/h5/mlp/c_proj/w\n",
      "model/h6/attn/c_attn/b\n",
      "model/h6/attn/c_attn/w\n",
      "model/h6/attn/c_proj/b\n",
      "model/h6/attn/c_proj/w\n",
      "model/h6/ln_1/b\n",
      "model/h6/ln_1/g\n",
      "model/h6/ln_2/b\n",
      "model/h6/ln_2/g\n",
      "model/h6/mlp/c_fc/b\n",
      "model/h6/mlp/c_fc/w\n",
      "model/h6/mlp/c_proj/b\n",
      "model/h6/mlp/c_proj/w\n",
      "model/h7/attn/c_attn/b\n",
      "model/h7/attn/c_attn/w\n",
      "model/h7/attn/c_proj/b\n",
      "model/h7/attn/c_proj/w\n",
      "model/h7/ln_1/b\n",
      "model/h7/ln_1/g\n",
      "model/h7/ln_2/b\n",
      "model/h7/ln_2/g\n",
      "model/h7/mlp/c_fc/b\n",
      "model/h7/mlp/c_fc/w\n",
      "model/h7/mlp/c_proj/b\n",
      "model/h7/mlp/c_proj/w\n",
      "model/h8/attn/c_attn/b\n",
      "model/h8/attn/c_attn/w\n",
      "model/h8/attn/c_proj/b\n",
      "model/h8/attn/c_proj/w\n",
      "model/h8/ln_1/b\n",
      "model/h8/ln_1/g\n",
      "model/h8/ln_2/b\n",
      "model/h8/ln_2/g\n",
      "model/h8/mlp/c_fc/b\n",
      "model/h8/mlp/c_fc/w\n",
      "model/h8/mlp/c_proj/b\n",
      "model/h8/mlp/c_proj/w\n",
      "model/h9/attn/c_attn/b\n",
      "model/h9/attn/c_attn/w\n",
      "model/h9/attn/c_proj/b\n",
      "model/h9/attn/c_proj/w\n",
      "model/h9/ln_1/b\n",
      "model/h9/ln_1/g\n",
      "model/h9/ln_2/b\n",
      "model/h9/ln_2/g\n",
      "model/h9/mlp/c_fc/b\n",
      "model/h9/mlp/c_fc/w\n",
      "model/h9/mlp/c_proj/b\n",
      "model/h9/mlp/c_proj/w\n",
      "model/ln_f/b\n",
      "model/ln_f/g\n",
      "model/wpe\n",
      "model/wte\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mcheckpoint\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = data/openai124M/model.ckpt\n",
       "\u001b[36mvariableNames\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[32m\"model/h0/attn/c_attn/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h0/attn/c_attn/w\"\u001b[39m,\n",
       "  \u001b[32m\"model/h0/attn/c_proj/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h0/attn/c_proj/w\"\u001b[39m,\n",
       "  \u001b[32m\"model/h0/ln_1/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h0/ln_1/g\"\u001b[39m,\n",
       "  \u001b[32m\"model/h0/ln_2/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h0/ln_2/g\"\u001b[39m,\n",
       "  \u001b[32m\"model/h0/mlp/c_fc/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h0/mlp/c_fc/w\"\u001b[39m,\n",
       "  \u001b[32m\"model/h0/mlp/c_proj/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h0/mlp/c_proj/w\"\u001b[39m,\n",
       "  \u001b[32m\"model/h1/attn/c_attn/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h1/attn/c_attn/w\"\u001b[39m,\n",
       "  \u001b[32m\"model/h1/attn/c_proj/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h1/attn/c_proj/w\"\u001b[39m,\n",
       "  \u001b[32m\"model/h1/ln_1/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h1/ln_1/g\"\u001b[39m,\n",
       "  \u001b[32m\"model/h1/ln_2/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h1/ln_2/g\"\u001b[39m,\n",
       "  \u001b[32m\"model/h1/mlp/c_fc/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h1/mlp/c_fc/w\"\u001b[39m,\n",
       "  \u001b[32m\"model/h1/mlp/c_proj/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h1/mlp/c_proj/w\"\u001b[39m,\n",
       "  \u001b[32m\"model/h10/attn/c_attn/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h10/attn/c_attn/w\"\u001b[39m,\n",
       "  \u001b[32m\"model/h10/attn/c_proj/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h10/attn/c_proj/w\"\u001b[39m,\n",
       "  \u001b[32m\"model/h10/ln_1/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h10/ln_1/g\"\u001b[39m,\n",
       "  \u001b[32m\"model/h10/ln_2/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h10/ln_2/g\"\u001b[39m,\n",
       "  \u001b[32m\"model/h10/mlp/c_fc/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h10/mlp/c_fc/w\"\u001b[39m,\n",
       "  \u001b[32m\"model/h10/mlp/c_proj/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h10/mlp/c_proj/w\"\u001b[39m,\n",
       "  \u001b[32m\"model/h11/attn/c_attn/b\"\u001b[39m,\n",
       "  \u001b[32m\"model/h11/attn/c_attn/w\"\u001b[39m,\n",
       "..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val checkpoint = tf.train.latest_checkpoint(outputDir)\n",
    "val variableNames = tf.train.list_variables(checkpoint).as[Seq[(String, Seq[Int])]].map { \n",
    "  case (variableName, _) => variableName \n",
    "}.toList\n",
    "variableNames.sorted.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dec512b8-60ba-4bdd-be61-1e5380889d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd26.sc:18: match may not be exhaustive.\n",
      "It would fail on the following inputs: List((x: String forSome x not in (\"b\", \"w\"))), Nil\n",
      "                tail match {\n",
      "                ^\n",
      "cmd26.sc:29: match may not be exhaustive.\n",
      "It would fail on the following inputs: List((x: String forSome x not in (\"b\", \"w\"))), Nil\n",
      "                tail match {\n",
      "                ^\n",
      "cmd26.sc:15: match may not be exhaustive.\n",
      "It would fail on the following inputs: List((x: String forSome x not in (\"c_attn\", \"c_proj\"))), Nil\n",
      "            tail match {\n",
      "            ^\n",
      "cmd26.sc:37: match may not be exhaustive.\n",
      "It would fail on the following inputs: List((x: String forSome x not in (\"b\", \"g\"))), Nil\n",
      "            tail match {\n",
      "            ^\n",
      "cmd26.sc:44: match may not be exhaustive.\n",
      "It would fail on the following inputs: List((x: String forSome x not in (\"b\", \"g\"))), Nil\n",
      "            tail match {\n",
      "            ^\n",
      "cmd26.sc:53: match may not be exhaustive.\n",
      "It would fail on the following inputs: List((x: String forSome x not in (\"b\", \"w\"))), Nil\n",
      "                tail match {\n",
      "                ^\n",
      "cmd26.sc:59: match may not be exhaustive.\n",
      "It would fail on the following inputs: List((x: String forSome x not in (\"b\", \"w\"))), Nil\n",
      "                tail match {\n",
      "                ^\n",
      "cmd26.sc:50: match may not be exhaustive.\n",
      "It would fail on the following inputs: List((x: String forSome x not in (\"c_fc\", \"c_proj\"))), Nil\n",
      "            tail match {\n",
      "            ^\n",
      "cmd26.sc:12: match may not be exhaustive.\n",
      "It would fail on the following inputs: List((x: String forSome x not in (\"attn\", \"ln_1\", \"ln_2\", \"mlp\"))), Nil\n",
      "        tail match {\n",
      "        ^\n",
      "cmd26.sc:68: match may not be exhaustive.\n",
      "It would fail on the following inputs: List((x: String forSome x not in (\"b\", \"g\"))), Nil\n",
      "        tail match {\n",
      "        ^\n",
      "cmd26.sc:9: match may not be exhaustive.\n",
      "It would fail on the following inputs: List((x: String forSome x not in (\"ln_f\", \"wpe\", \"wte\"))), Nil\n",
      "    variableName.split(\"/\").drop(1).toList match {\n",
      "                                    ^\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mtype\u001b[39m \u001b[36mNpArray\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtoTorchParameter\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mloadModelWeights\u001b[39m"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type NpArray = py.Dynamic\n",
    "\n",
    "def toTorchParameter(npArray: NpArray) =\n",
    "  torch.nn.Parameter(torch.tensor(npArray))\n",
    "\n",
    "def loadModelWeights(model: Model): Unit =\n",
    "  variableNames.foreach { variableName =>\n",
    "    val variableValue = np.squeeze(tf.train.load_variable(checkpoint, variableName))\n",
    "    variableName.split(\"/\").drop(1).toList match {\n",
    "      case s\"h$transformerBlockIndexString\" :: tail =>\n",
    "        val transformerBlockIndex = transformerBlockIndexString.toInt\n",
    "        tail match {\n",
    "          case \"attn\" :: tail =>\n",
    "            val multiHeadAttention = model.transformerBlocksLayer.bracketAccess(transformerBlockIndex).multiHeadAttention\n",
    "            tail match {\n",
    "              case \"c_attn\" :: tail =>\n",
    "                val Seq(queryVariableValue, keyVariableValue, valueVariableValue) = np.split(variableValue, 3, axis = -1).as[Seq[NpArray]]\n",
    "                tail match {\n",
    "                  case \"b\" :: _ => \n",
    "                    multiHeadAttention.weightsQuery.bias = toTorchParameter(queryVariableValue)\n",
    "                    multiHeadAttention.weightsKey.bias = toTorchParameter(keyVariableValue)\n",
    "                    multiHeadAttention.weightsValue.bias = toTorchParameter(valueVariableValue)\n",
    "                  case \"w\" :: _ => \n",
    "                    multiHeadAttention.weightsQuery.weight = toTorchParameter(queryVariableValue.T)\n",
    "                    multiHeadAttention.weightsKey.weight = toTorchParameter(keyVariableValue.T)\n",
    "                    multiHeadAttention.weightsValue.weight = toTorchParameter(valueVariableValue.T)\n",
    "                }\n",
    "              case \"c_proj\" :: tail =>\n",
    "                tail match {\n",
    "                  case \"b\" :: _ => multiHeadAttention.outputProjection.bias = toTorchParameter(variableValue)\n",
    "                  case \"w\" :: _ => multiHeadAttention.outputProjection.weight = toTorchParameter(variableValue.T)\n",
    "                }\n",
    "            }\n",
    "          case \"ln_1\" :: tail =>\n",
    "            val normalization1 = model.transformerBlocksLayer.bracketAccess(transformerBlockIndex).normalization1\n",
    "            val torchParameter = toTorchParameter(variableValue)\n",
    "            tail match {\n",
    "              case \"b\" :: _ => normalization1.shift = torchParameter\n",
    "              case \"g\" :: _ => normalization1.scale = torchParameter\n",
    "            }\n",
    "          case \"ln_2\" :: tail =>\n",
    "            val normalization2 = model.transformerBlocksLayer.bracketAccess(transformerBlockIndex).normalization2\n",
    "            val torchParameter = toTorchParameter(variableValue)\n",
    "            tail match {\n",
    "              case \"b\" :: _ => normalization2.shift = torchParameter\n",
    "              case \"g\" :: _ => normalization2.scale = torchParameter\n",
    "            }\n",
    "          case \"mlp\" :: tail =>\n",
    "            val feedForward = model.transformerBlocksLayer.bracketAccess(transformerBlockIndex).feedForward\n",
    "            tail match {\n",
    "              case \"c_fc\" :: tail =>\n",
    "                val layer0 = feedForward.layers.bracketAccess(0)\n",
    "                tail match {\n",
    "                  case \"b\" :: _ => layer0.bias = toTorchParameter(variableValue)\n",
    "                  case \"w\" :: _ => layer0.weight = toTorchParameter(variableValue.T)\n",
    "                }\n",
    "              case \"c_proj\" :: tail =>\n",
    "                val layer2 = feedForward.layers.bracketAccess(2)\n",
    "                tail match {\n",
    "                  case \"b\" :: _ => layer2.bias = toTorchParameter(variableValue)\n",
    "                  case \"w\" :: _ => layer2.weight = toTorchParameter(variableValue.T)\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "      case \"ln_f\" :: tail =>\n",
    "        val finalNormalizationLayer = model.finalNormalizationLayer\n",
    "        val torchParameter = toTorchParameter(variableValue)\n",
    "        tail match {\n",
    "          case \"b\" :: _ => finalNormalizationLayer.shift = torchParameter\n",
    "          case \"g\" :: _ => finalNormalizationLayer.scale = torchParameter\n",
    "        }\n",
    "      case \"wpe\" :: _ => model.positionEmbeddingLayer.weight = toTorchParameter(variableValue)\n",
    "      case \"wte\" :: _ => \n",
    "        val torchParameter = toTorchParameter(variableValue)\n",
    "        model.tokenEmbeddingLayer.weight = torchParameter\n",
    "        model.outputLayer.weight = torchParameter\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a67b190-61de-4d66-bec9-86696a29429f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmodel\u001b[39m: \u001b[32mModel\u001b[39m = GPTModel(\n",
       "  (tokenEmbeddingLayer): Embedding(50257, 768)\n",
       "  (positionEmbeddingLayer): Embedding(1024, 768)\n",
       "  (dropoutEmbeddingLayer): Dropout(p=0.1, inplace=False)\n",
       "  (transformerBlocksLayer): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (normalization1): NormalizationLayer()\n",
       "      (normalization2): NormalizationLayer()\n",
       "      (dropoutShortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = GPTModel(gptConfig)\n",
    "loadModelWeights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9354f44-75dd-484f-b3a5-049ed928fe96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mbatchSize\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m8\u001b[39m\n",
       "\u001b[36mres28_1\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch._C.Generator object at 0xffff902d0790>\n",
       "\u001b[36mcustomizedCollate\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m => (\u001b[32mTorchTensor\u001b[39m, \u001b[32mTorchTensor\u001b[39m) = ammonite.$sess.cmd28$Helper$$Lambda$3663/0x0000008001ab8420@404ad6d1\n",
       "\u001b[36mtrainingLoader\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch.utils.data.dataloader.DataLoader object at 0xffff0824dc70>\n",
       "\u001b[36mvalidationLoader\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch.utils.data.dataloader.DataLoader object at 0xffff0824f050>\n",
       "\u001b[36mtestLoader\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch.utils.data.dataloader.DataLoader object at 0xffff0824fe90>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val batchSize = 8\n",
    "torch.manual_seed(123)\n",
    "\n",
    "val customizedCollate = (batch: py.Dynamic) => collate(device)(batch = batch.as[Vector[Vector[Int]]], allowedMaxLength = Some(gptConfig.contextLength))\n",
    "val trainingLoader = torch.utils.data.DataLoader(\n",
    "  dataset = trainingDataset, \n",
    "  batch_size = batchSize,\n",
    "  collate_fn = customizedCollate,\n",
    "  shuffle = true,\n",
    "  num_workers = 0,\n",
    "  drop_last = true\n",
    ")\n",
    "val validationLoader = torch.utils.data.DataLoader(\n",
    "  dataset = validationDataset, \n",
    "  batch_size = batchSize,\n",
    "  collate_fn = customizedCollate,\n",
    "  num_workers = 0,\n",
    "  drop_last = false\n",
    ")\n",
    "val testLoader = torch.utils.data.DataLoader(\n",
    "  dataset = testDataset, \n",
    "  batch_size = batchSize,\n",
    "  collate_fn = customizedCollate,\n",
    "  num_workers = 0,\n",
    "  drop_last = false\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aee24098-87bf-481f-86b6-45f88b9b2c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mgenerateTextSimple\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtextToTokenIds\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtokenIdsToText\u001b[39m"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generateTextSimple(\n",
    "  model: Model,\n",
    "  maxNewTokens: Int,\n",
    "  contextLength: Int\n",
    ")(\n",
    "  encodedInput: TorchTensor\n",
    "): TorchTensor =\n",
    "  LazyList.iterate(encodedInput) { currentEncodedOutput =>\n",
    "    val croppedInput = py\"$currentEncodedOutput[:, -$contextLength:]\"\n",
    "    val logits = py.`with`(torch.no_grad()) { _ =>\n",
    "      model(croppedInput)\n",
    "    }\n",
    "    py\"$logits[:, -1, :]\"\n",
    "      .pipe(torch.softmax(_, dim = -1))\n",
    "      .pipe(torch.argmax(_, dim = -1, keepdim = true))\n",
    "      .pipe(nextEncodedOutput => torch.cat((currentEncodedOutput, nextEncodedOutput), dim = 1))\n",
    "  }.drop(maxNewTokens).head\n",
    "\n",
    "def textToTokenIds(\n",
    "  text: String, \n",
    "  tokenizer: Tokenizer\n",
    "): TorchTensor = {\n",
    "  val allowedSpecial = py.Dynamic.global.set(Seq(\"<|endoftext|>\").toPythonProxy)\n",
    "  val encodedText = tokenizer.encode(text, allowed_special = allowedSpecial)\n",
    "  torch.tensor(encodedText).unsqueeze(0)\n",
    "}\n",
    "    \n",
    "def tokenIdsToText(\n",
    "  tokenIds: TorchTensor, \n",
    "  tokenizer: Tokenizer\n",
    "): String =\n",
    "  tokenizer.decode(tokenIds.squeeze(0).tolist()).as[String]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f216c1f-ff87-4596-9648-5af234849168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response text: \n",
      "### Error:\n",
      "\n",
      "The car is not as fast as lightning.\n",
      "\n",
      "### Error:\n",
      "\n",
      "The car is not as fast as lightning.\n",
      "\n",
      "### Error\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres30_0\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = GPTModel(\n",
       "  (tokenEmbeddingLayer): Embedding(50257, 768)\n",
       "  (positionEmbeddingLayer): Embedding(1024, 768)\n",
       "  (dropoutEmbeddingLayer): Dropout(p=0.1, inplace=False)\n",
       "  (transformerBlocksLayer): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (normalization1): NormalizationLayer()\n",
       "      (normalization2): NormalizationLayer()\n",
       "      (dropoutShortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "...\n",
       "\u001b[36mres30_1\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch._C.Generator object at 0xffff902d0790>\n",
       "\u001b[36mexampleText\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
       "\n",
       "### Instruction:\n",
       "Rewrite the sentence using a simile.\n",
       "\n",
       "### Input:\n",
       "The car is very fast.\n",
       "\n",
       "### Response:\n",
       "The car is as fast as lightning.\n",
       "\"\"\"\u001b[39m\n",
       "\u001b[36moutputTextIds\u001b[39m: \u001b[32mTorchTensor\u001b[39m = tensor([[21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
       "           257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
       "         21017, 46486,    25,   198, 30003,  6525,   262,  6827,  1262,   257,\n",
       "           985,   576,    13,   198,   198, 21017, 23412,    25,   198,   464,\n",
       "          1097,   318,   845,  3049,    13,   198,   198, 21017, 18261,    25,\n",
       "           198,   464,  1097,   318,   355,  3049,   355, 14357,    13,   198,\n",
       "           198, 21017, 13047,    25,   198,   198,   464,  1097,   318,   407,\n",
       "           355,  3049,   355, 14357,    13,   198,   198, 21017, 13047,    25,\n",
       "           198,   198,   464,  1097,   318,   407,   355,  3049,   355, 14357,\n",
       "            13,   198,   198, 21017, 13047]])\n",
       "\u001b[36mdecodedOutputText\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
       "\n",
       "### Instruction:\n",
       "Rewrite the sentence using a simile.\n",
       "\n",
       "### Input:\n",
       "The car is very fast.\n",
       "\n",
       "### Response:\n",
       "The car is as fast as lightning.\n",
       "\n",
       "### Error:\n",
       "\n",
       "The car is not as fast as lightning.\n",
       "\n",
       "### Error:\n",
       "\n",
       "The car is not as fast as lightning.\n",
       "\n",
       "### Error\"\"\"\u001b[39m\n",
       "\u001b[36mresponseText\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"\n",
       "### Error:\n",
       "\n",
       "The car is not as fast as lightning.\n",
       "\n",
       "### Error:\n",
       "\n",
       "The car is not as fast as lightning.\n",
       "\n",
       "### Error\"\"\"\u001b[39m"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "torch.manual_seed(123)\n",
    "\n",
    "val exampleText = validation.head.alpacaFormat\n",
    "val outputTextIds = generateTextSimple(\n",
    "  model = model, \n",
    "  maxNewTokens = 35,\n",
    "  contextLength = gptConfig.contextLength\n",
    ")(\n",
    "  encodedInput = textToTokenIds(exampleText, tokenizer)\n",
    ")\n",
    "val decodedOutputText = tokenIdsToText(outputTextIds, tokenizer)\n",
    "val responseText = decodedOutputText.drop(exampleText.length)\n",
    "println(s\"Response text: $responseText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e3e1141-20f9-4905-aa69-02a18012e718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mscala.annotation.tailrec\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mforeachPy\u001b[39m"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.annotation.tailrec\n",
    "\n",
    "def foreachPy(iterable: py.Dynamic)(f: py.Dynamic => Unit): Unit = {\n",
    "  val iterator = py\"iter($iterable)\"\n",
    "\n",
    "  @tailrec\n",
    "  def loop(): Unit = {\n",
    "    val currentValue = py\"next($iterator, None)\"\n",
    "    if (currentValue != py.Dynamic.global.None) {\n",
    "      f(currentValue)\n",
    "      loop()\n",
    "    }\n",
    "  }\n",
    "\n",
    "  loop()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f926fd7-f699-43eb-842a-2205d2ff8eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcalculateBatchLoss\u001b[39m\n",
       "defined \u001b[32mtype\u001b[39m \u001b[36mDataLoader\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcalculateDataLoaderLoss\u001b[39m"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculateBatchLoss(\n",
    "  model: Model,\n",
    "  device: Device\n",
    ")(\n",
    "  inputBatch: TorchTensor,\n",
    "  targetBatch: TorchTensor\n",
    "): TorchTensor = {\n",
    "  val outputs = model(inputBatch.to(device))\n",
    "  torch.nn.functional.cross_entropy(outputs.flatten(0, 1), targetBatch.flatten())\n",
    "}\n",
    "\n",
    "type DataLoader = py.Dynamic\n",
    "\n",
    "def calculateDataLoaderLoss(\n",
    "  model: Model,\n",
    "  device: Device\n",
    ")(\n",
    "  dataLoader: DataLoader,\n",
    "  batchesCountOpt: Option[Int] = None\n",
    "): Double = { \n",
    "  val batchesCount = batchesCountOpt match {\n",
    "    case Some(batchesCount) => batchesCount\n",
    "    case None => py\"len($dataLoader)\".as[Int]\n",
    "  }\n",
    "  assert(batchesCount > 0, \"There were no batches to process\")\n",
    "  var totalLoss = 0.0\n",
    "  var currentBatchIndex = 0\n",
    "  foreachPy(dataLoader) { currentBatch =>\n",
    "    if (currentBatchIndex < batchesCount)\n",
    "      py.local {\n",
    "        val Seq(inputBatch, targetBatch) = currentBatch.as[Seq[TorchTensor]]\n",
    "        totalLoss += calculateBatchLoss(model, device)(inputBatch, targetBatch).item().as[Double]\n",
    "      }\n",
    "    currentBatchIndex += 1\n",
    "  }\n",
    "  totalLoss / batchesCount\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5664fbca-517b-4ff9-87d9-fb8659048619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating training loss...\n",
      "Training loss: 4.236046409606933\n",
      "Calculating validation loss...\n",
      "Validation loss: 4.268288326263428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres33_0\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = GPTModel(\n",
       "  (tokenEmbeddingLayer): Embedding(50257, 768)\n",
       "  (positionEmbeddingLayer): Embedding(1024, 768)\n",
       "  (dropoutEmbeddingLayer): Dropout(p=0.1, inplace=False)\n",
       "  (transformerBlocksLayer): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (normalization1): NormalizationLayer()\n",
       "      (normalization2): NormalizationLayer()\n",
       "      (dropoutShortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "...\n",
       "\u001b[36mres33_1\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch._C.Generator object at 0xffff902d0790>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "py.`with`(torch.no_grad()) { _ =>\n",
    "  println(s\"Calculating training loss...\")\n",
    "  val trainingLoss = calculateDataLoaderLoss(model, device)(trainingLoader, batchesCountOpt = Some(5))\n",
    "  println(s\"Training loss: $trainingLoss\")\n",
    "  println(s\"Calculating validation loss...\")\n",
    "  val validationLoss = calculateDataLoaderLoss(model, device)(validationLoader, batchesCountOpt = Some(5))\n",
    "  println(s\"Validation loss: $validationLoss\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9bae639c-bc23-485e-955c-6938fc67c9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mLoss\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mTrainingStep\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mModelEvaluator\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mSampleGenerator\u001b[39m\n",
       "defined \u001b[32mtype\u001b[39m \u001b[36mOptimizer\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtrainModelSimple\u001b[39m"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Loss(\n",
    "  trainingLoss: Double,\n",
    "  validationLoss: Double\n",
    ")\n",
    "\n",
    "case class TrainingStep(\n",
    "  loss: Loss,\n",
    "  tokensSeen: Long\n",
    ")\n",
    "\n",
    "class ModelEvaluator(\n",
    "  device: Device,\n",
    "  trainingLoader: DataLoader,\n",
    "  validationLoader: DataLoader,\n",
    "  evaluationEpochsCount: Int,\n",
    "  evaluationFrequencySteps: Int\n",
    ") {\n",
    "\n",
    "  def evaluateCond(currentStep: Int)(model: Model): Option[Loss] =\n",
    "    Option.when(currentStep % evaluationFrequencySteps == 0) {\n",
    "      println(s\"Step $currentStep\")\n",
    "      evaluate(model)\n",
    "    }\n",
    "    \n",
    "  def evaluate(model: Model): Loss = {\n",
    "    model.eval()\n",
    "    py.`with`(torch.no_grad()) { _ =>\n",
    "      val trainingLoss = calculateDataLoaderLoss(model, device)(trainingLoader, batchesCountOpt = Some(evaluationEpochsCount))\n",
    "      val validationLoss = calculateDataLoaderLoss(model, device)(validationLoader, batchesCountOpt = Some(evaluationEpochsCount))\n",
    "      println(\n",
    "        s\"\"\"- training loss: $trainingLoss\n",
    "           |- validation loss: $validationLoss\"\"\".stripMargin\n",
    "      )\n",
    "      model.train()\n",
    "      Loss(trainingLoss, validationLoss)\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "class SampleGenerator(\n",
    "  device: Device,\n",
    "  tokenizer: Tokenizer,\n",
    "  text: String,\n",
    "  contextLength: Int\n",
    ") {\n",
    "\n",
    "  def generateAndPrintSample(model: Model): Unit = {\n",
    "    model.eval()\n",
    "    val encodedText = textToTokenIds(text, tokenizer)\n",
    "    py.`with`(torch.no_grad()) { _ =>\n",
    "      val tokenIds = generateTextSimple(\n",
    "        model = model, \n",
    "        maxNewTokens = 50, \n",
    "        contextLength = contextLength\n",
    "      )(encodedText)\n",
    "      val decodedText = tokenIdsToText(tokenIds, tokenizer).replace(\"\\n\", \" \")\n",
    "      println(s\"Sample text for '$text':\\n'$decodedText'\")\n",
    "      model.train()\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "type Optimizer = py.Dynamic\n",
    "def trainModelSimple(\n",
    "  model: Model,\n",
    "  device: Device,\n",
    "  trainingLoader: DataLoader,\n",
    "  validationLoader: DataLoader,\n",
    "  optimizer: Optimizer,\n",
    "  epochsCount: Int,\n",
    "  modelEvaluator: ModelEvaluator,\n",
    "  sampleGenerator: SampleGenerator\n",
    "): List[TrainingStep] = {\n",
    "  var stepsCount = 0\n",
    "  var tokensSeen = 0L\n",
    "  val losses =\n",
    "    for {\n",
    "      epoch <- 1 to epochsCount\n",
    "    } yield py.local {\n",
    "      println(s\"=> Epoch $epoch\")\n",
    "      model.train()\n",
    "      val batchesIterator = py\"iter($trainingLoader)\"\n",
    "      val losses = LazyList\n",
    "        .continually(py\"next($batchesIterator, None)\")\n",
    "        .takeWhile(_ != py.Dynamic.global.None)\n",
    "        .map(_.as[Seq[TorchTensor]])\n",
    "        .map { batch =>\n",
    "          py.local {\n",
    "            val Seq(inputBatch, targetBatch) = batch\n",
    "            optimizer.zero_grad()\n",
    "            val loss = calculateBatchLoss(model, device)(inputBatch, targetBatch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            stepsCount += 1\n",
    "            tokensSeen += inputBatch.numel().as[Long]\n",
    "            modelEvaluator.evaluateCond(stepsCount)(model).map { loss =>\n",
    "              TrainingStep(loss, tokensSeen)\n",
    "            }\n",
    "          }\n",
    "        }.flatten.toList\n",
    "      sampleGenerator.generateAndPrintSample(model)\n",
    "      losses\n",
    "    }\n",
    "  losses.toList.flatten\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f121b8b-6ac7-4e8b-83aa-f537253dca14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres35_0\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch._C.Generator object at 0xffff902d0790>\n",
       "\u001b[36mmodel\u001b[39m: \u001b[32mModel\u001b[39m = GPTModel(\n",
       "  (tokenEmbeddingLayer): Embedding(50257, 768)\n",
       "  (positionEmbeddingLayer): Embedding(1024, 768)\n",
       "  (dropoutEmbeddingLayer): Dropout(p=0.1, inplace=False)\n",
       "  (transformerBlocksLayer): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (normalization1): NormalizationLayer()\n",
       "      (normalization2): NormalizationLayer()\n",
       "      (dropoutShortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "...\n",
       "\u001b[36mres35_2\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = GPTModel(\n",
       "  (tokenEmbeddingLayer): Embedding(50257, 768)\n",
       "  (positionEmbeddingLayer): Embedding(1024, 768)\n",
       "  (dropoutEmbeddingLayer): Dropout(p=0.1, inplace=False)\n",
       "  (transformerBlocksLayer): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (normalization1): NormalizationLayer()\n",
       "      (normalization2): NormalizationLayer()\n",
       "      (dropoutShortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "...\n",
       "\u001b[36moptimizer\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 5e-05\n",
       "    maximize: False\n",
       "    weight_decay: 0.1\n",
       ")\n",
       "\u001b[36mepochsCount\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m2\u001b[39m\n",
       "\u001b[36mmodelEvaluator\u001b[39m: \u001b[32mModelEvaluator\u001b[39m = ammonite.$sess.cmd34$Helper$ModelEvaluator@5f08b873\n",
       "\u001b[36msampleGenerator\u001b[39m: \u001b[32mSampleGenerator\u001b[39m = ammonite.$sess.cmd34$Helper$SampleGenerator@47c6143e"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "val model = GPTModel(gptConfig)\n",
    "model.to(device)\n",
    "val optimizer = torch.optim.AdamW(model.parameters(), lr = 0.00005, weight_decay = 0.1)\n",
    "val epochsCount = 2\n",
    "val modelEvaluator = new ModelEvaluator(device, trainingLoader, validationLoader, evaluationEpochsCount = 5, evaluationFrequencySteps = 5)\n",
    "val sampleGenerator = new SampleGenerator(device, tokenizer, text = validation.head.alpacaFormat, contextLength = gptConfig.contextLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15804408-8cf6-4110-bd59-e128d6567052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch 1\n",
      "Step 5\n",
      "- training loss: 7.127060222625732\n",
      "- validation loss: 7.10926570892334\n",
      "Step 10\n",
      "- training loss: 5.577646255493164\n",
      "- validation loss: 5.450537204742432\n",
      "Step 15\n",
      "- training loss: 4.880376815795898\n",
      "- validation loss: 4.607197093963623\n",
      "Step 20\n",
      "- training loss: 4.215584230422974\n",
      "- validation loss: 4.243081378936767\n",
      "Step 25\n",
      "- training loss: 4.043199634552002\n",
      "- validation loss: 4.0049824714660645\n",
      "Step 30\n",
      "- training loss: 3.8942723274230957\n",
      "- validation loss: 3.8340887069702148\n",
      "Step 35\n",
      "- training loss: 3.7197072982788084\n",
      "- validation loss: 3.6962204933166505\n",
      "Step 40\n",
      "- training loss: 3.7818166255950927\n",
      "- validation loss: 3.575327825546265\n",
      "Step 45\n",
      "- training loss: 3.3824785709381104\n",
      "- validation loss: 3.4686116218566894\n",
      "Step 50\n",
      "- training loss: 3.525093698501587\n",
      "- validation loss: 3.379911470413208\n",
      "Step 55\n",
      "- training loss: 3.3228762626647947\n",
      "- validation loss: 3.290615606307983\n",
      "Step 60\n",
      "- training loss: 3.247799348831177\n",
      "- validation loss: 3.20725736618042\n",
      "Step 65\n",
      "- training loss: 3.1169456958770754\n",
      "- validation loss: 3.136827230453491\n",
      "Step 70\n",
      "- training loss: 2.941383171081543\n",
      "- validation loss: 3.0776655673980713\n",
      "Step 75\n",
      "- training loss: 3.1384324550628664\n",
      "- validation loss: 3.0130829334259035\n",
      "Step 80\n",
      "- training loss: 3.121018886566162\n",
      "- validation loss: 2.9569437503814697\n",
      "Step 85\n",
      "- training loss: 2.8253982067108154\n",
      "- validation loss: 2.912667751312256\n",
      "Step 90\n",
      "- training loss: 2.9749001026153565\n",
      "- validation loss: 2.8708152294158937\n",
      "Step 95\n",
      "- training loss: 2.8973459243774413\n",
      "- validation loss: 2.8353589534759522\n",
      "Step 100\n",
      "- training loss: 2.7384571552276613\n",
      "- validation loss: 2.7949643611907957\n",
      "Step 105\n",
      "- training loss: 2.814553213119507\n",
      "- validation loss: 2.7604424953460693\n",
      "Step 110\n",
      "- training loss: 2.6027143001556396\n",
      "- validation loss: 2.7384047985076903\n",
      "Step 115\n",
      "- training loss: 2.5667447090148925\n",
      "- validation loss: 2.7034890174865724\n",
      "Sample text for 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "### Response:\n",
      "The car is as fast as lightning.\n",
      "':\n",
      "'Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Rewrite the sentence using a simile.  ### Input: The car is very fast.  ### Response: The car is as fast as lightning. <|endoftext|>. <|endoftext|>. <|endoftext|>. <|endoftext|>. <|endoftext|>. <|endoftext|>. <|endoftext|>. <|endoftext|>. <|endoftext|>. <|endoftext|>. <|endoftext|>. <|endoftext|>. <|endoftext|>. <|endoftext|>. <|endoftext|>. <|endoftext|>. <|endoftext|>.'\n",
      "=> Epoch 2\n",
      "Step 120\n",
      "- training loss: 2.6264421939849854\n",
      "- validation loss: 2.6757388591766356\n",
      "Step 125\n",
      "- training loss: 2.6029566287994386\n",
      "- validation loss: 2.6546771049499513\n",
      "Step 130\n",
      "- training loss: 2.46907057762146\n",
      "- validation loss: 2.6381171703338624\n",
      "Step 135\n",
      "- training loss: 2.7151292324066163\n",
      "- validation loss: 2.626419734954834\n",
      "Step 140\n",
      "- training loss: 2.2891454219818117\n",
      "- validation loss: 2.5951868057250977\n",
      "Step 145\n",
      "- training loss: 2.2557409763336183\n",
      "- validation loss: 2.5712454319000244\n",
      "Step 150\n",
      "- training loss: 2.4720608711242678\n",
      "- validation loss: 2.5540287494659424\n",
      "Step 155\n",
      "- training loss: 2.3609713077545167\n",
      "- validation loss: 2.541324090957642\n",
      "Step 160\n",
      "- training loss: 2.193418765068054\n",
      "- validation loss: 2.518134832382202\n",
      "Step 165\n",
      "- training loss: 2.24983925819397\n",
      "- validation loss: 2.4978191375732424\n",
      "Step 170\n",
      "- training loss: 2.340534734725952\n",
      "- validation loss: 2.4878336906433107\n",
      "Step 175\n",
      "- training loss: 2.2783969402313233\n",
      "- validation loss: 2.4761061668395996\n",
      "Step 180\n",
      "- training loss: 2.272768020629883\n",
      "- validation loss: 2.4761581420898438\n",
      "Step 185\n",
      "- training loss: 2.3465392112731935\n",
      "- validation loss: 2.4607606410980223\n",
      "Step 190\n",
      "- training loss: 2.048973894119263\n",
      "- validation loss: 2.4463003158569334\n",
      "Step 195\n",
      "- training loss: 2.253487396240234\n",
      "- validation loss: 2.435351610183716\n",
      "Step 200\n",
      "- training loss: 2.2945210933685303\n",
      "- validation loss: 2.421572685241699\n",
      "Step 205\n",
      "- training loss: 2.1125584125518797\n",
      "- validation loss: 2.4143182277679442\n",
      "Step 210\n",
      "- training loss: 2.1324856758117674\n",
      "- validation loss: 2.40118727684021\n",
      "Step 215\n",
      "- training loss: 2.1631389617919923\n",
      "- validation loss: 2.385039138793945\n",
      "Step 220\n",
      "- training loss: 2.206794500350952\n",
      "- validation loss: 2.3792866706848144\n",
      "Step 225\n",
      "- training loss: 2.201306939125061\n",
      "- validation loss: 2.371427631378174\n",
      "Step 230\n",
      "- training loss: 1.8808055877685548\n",
      "- validation loss: 2.3807934284210206\n",
      "Sample text for 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "### Response:\n",
      "The car is as fast as lightning.\n",
      "':\n",
      "'Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Rewrite the sentence using a simile.  ### Input: The car is very fast.  ### Response: The car is as fast as lightning. <|endoftext|>, the following sentence. <|endoftext|>. <|endoftext|>. <|endoftext|>, and the following sentence. <|endoftext|>. <|endoftext|>, and the following sentence. <|endoftext|> the word ' the following sentence. <|endoftext|> the following sentence. <|endoftext|> is a'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainingSteps\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mTrainingStep\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[33mTrainingStep\u001b[39m(\n",
       "    loss = \u001b[33mLoss\u001b[39m(\n",
       "      trainingLoss = \u001b[32m7.127060222625732\u001b[39m,\n",
       "      validationLoss = \u001b[32m7.10926570892334\u001b[39m\n",
       "    ),\n",
       "    tokensSeen = \u001b[32m2808L\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mTrainingStep\u001b[39m(\n",
       "    loss = \u001b[33mLoss\u001b[39m(\n",
       "      trainingLoss = \u001b[32m5.577646255493164\u001b[39m,\n",
       "      validationLoss = \u001b[32m5.450537204742432\u001b[39m\n",
       "    ),\n",
       "    tokensSeen = \u001b[32m5512L\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mTrainingStep\u001b[39m(\n",
       "    loss = \u001b[33mLoss\u001b[39m(\n",
       "      trainingLoss = \u001b[32m4.880376815795898\u001b[39m,\n",
       "      validationLoss = \u001b[32m4.607197093963623\u001b[39m\n",
       "    ),\n",
       "    tokensSeen = \u001b[32m8520L\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mTrainingStep\u001b[39m(\n",
       "    loss = \u001b[33mLoss\u001b[39m(\n",
       "      trainingLoss = \u001b[32m4.215584230422974\u001b[39m,\n",
       "      validationLoss = \u001b[32m4.243081378936767\u001b[39m\n",
       "    ),\n",
       "    tokensSeen = \u001b[32m11224L\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mTrainingStep\u001b[39m(\n",
       "    loss = \u001b[33mLoss\u001b[39m(\n",
       "      trainingLoss = \u001b[32m4.043199634552002\u001b[39m,\n",
       "      validationLoss = \u001b[32m4.0049824714660645\u001b[39m\n",
       "    ),\n",
       "    tokensSeen = \u001b[32m14024L\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mTrainingStep\u001b[39m(\n",
       "    loss = \u001b[33mLoss\u001b[39m(\n",
       "      trainingLoss = \u001b[32m3.8942723274230957\u001b[39m,\n",
       "..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainingSteps = trainModelSimple(model, device, trainingLoader, validationLoader, optimizer, epochsCount, modelEvaluator, sampleGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6eeff81d-c8c8-4442-9a57-70e25100bf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib==3.9.* in /usr/local/lib/python3.12/site-packages (3.9.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/site-packages (from matplotlib==3.9.*) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/site-packages (from matplotlib==3.9.*) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/site-packages (from matplotlib==3.9.*) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/site-packages (from matplotlib==3.9.*) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/site-packages (from matplotlib==3.9.*) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/site-packages (from matplotlib==3.9.*) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/site-packages (from matplotlib==3.9.*) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/site-packages (from matplotlib==3.9.*) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/site-packages (from matplotlib==3.9.*) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib==3.9.*) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "Magic.!(\"pip\", \"install\", \"matplotlib==3.9.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77306a20-4ba2-4054-bc1a-cebc44e3e0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling /workspace/DisplaySupport.sc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$\u001b[39m"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $file.^.DisplaySupport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bae8a9ac-f7d4-4ef8-a752-24174591d0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mplot\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mModule\u001b[39m = <module 'matplotlib.pyplot' from '/usr/local/lib/python3.12/site-packages/matplotlib/pyplot.py'>\n",
       "\u001b[36mticker\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mModule\u001b[39m = <module 'matplotlib.ticker' from '/usr/local/lib/python3.12/site-packages/matplotlib/ticker.py'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val plot = py.module(\"matplotlib.pyplot\")\n",
    "val ticker = py.module(\"matplotlib.ticker\")\n",
    "\n",
    "try {\n",
    "  type Figure = py.Dynamic\n",
    "  type Axis = py.Dynamic\n",
    "    \n",
    "  val (tokensSeen, trainingLoss, validationLoss) = trainingSteps.map {\n",
    "    case TrainingStep(Loss(trainingLoss, validationLoss), tokensSeen) => (tokensSeen, trainingLoss, validationLoss)\n",
    "  }.unzip3\n",
    "  val epochs = torch.linspace(0, epochsCount, tokensSeen.length)\n",
    "  val (figure, axis1) = plot.subplots(figsize = (5, 3)).as[(Figure, Axis)]\n",
    "  axis1.plot(epochs, trainingLoss.toPythonProxy, label = \"Training loss\")\n",
    "  axis1.plot(epochs, validationLoss.toPythonProxy, linestyle = \"-.\", label = \"Validation loss\")\n",
    "  axis1.set_xlabel(\"Epochs\")\n",
    "  axis1.set_ylabel(\"Loss\")\n",
    "  axis1.legend(loc = \"upper right\")\n",
    "  axis1.xaxis.set_major_locator(ticker.MaxNLocator(integer = true))\n",
    "  val axis2 = axis1.twiny()\n",
    "  axis2.plot(tokensSeen.toPythonProxy, trainingLoss.toPythonProxy, alpha = 0)\n",
    "  axis2.set_xlabel(\"Tokens seen\")\n",
    "  figure.tight_layout()\n",
    "  DisplaySupport.showPlot(plot)\n",
    "} catch {\n",
    "  case e: py.PythonException =>\n",
    "    println(\"(!) If the exception below says 'Numpy is not available', restart the Jupyter kernel. It's an issue with Matplotlib in Jupyter.\\n\")\n",
    "    throw e\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b71ea708-bf23-49c2-b823-d01ab29a2c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmodelStateKey\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"model\"\u001b[39m\n",
       "\u001b[36moptimizerStateKey\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"optimizer\"\u001b[39m\n",
       "\u001b[36mstatesMap\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = {'model': OrderedDict({'tokenEmbeddingLayer.weight': tensor([[ 1.7467e+00,  1.6750e+00,  2.4550e-01,  ...,  8.2859e-01,\n",
       "         -2.2970e-01,  1.4564e+00],\n",
       "        [-4.4679e-01,  1.1839e+00,  1.0902e+00,  ..., -7.5460e-01,\n",
       "          4.2087e-01,  5.8719e-01],\n",
       "        [ 1.5945e+00, -1.2873e+00, -3.3599e-03,  ...,  1.5097e+00,\n",
       "         -1.7991e-01,  1.5387e-01],\n",
       "        ...,\n",
       "        [-1.1012e+00,  3.5025e-01, -1.9819e-01,  ...,  1.4751e+00,\n",
       "         -6.8413e-01, -9.2930e-01],\n",
       "        [-1.2830e+00,  1.1319e+00, -1.5019e+00,  ..., -1.1252e+00,\n",
       "         -6.7377e-01, -8.8464e-01],\n",
       "        [-1.0690e-01, -7.7091e-01,  1.5382e-01,  ...,  1.2587e-01,\n",
       "          8.4397e-04,  9.9906e-01]]), 'positionEmbeddingLayer.weight': tensor([[ 2.8500e+00,  9.4432e-02, -6.4361e-01,  ..., -6.8505e-02,\n",
       "          1.3166e+00, -6.2983e-01],\n",
       "        [ 1.4499e+00,  5.0159e-01,  1.0971e+00,  ..., -2.2200e-01,\n",
       "          2.4517e-01, -2.3939e-01],\n",
       "        [-7.8571e-01, -5.4469e-01,  1.5396e+00,  ...,  3.0527e-01,\n",
       "          4.8201e-01,  4.0175e-01],\n",
       "        ...,\n",
       "        [-5.5355e-01, -3.0836e-01, -9.3897e-01,  ..., -4.9950e-01,\n",
       "         -3.3963e-01,  5.2340e-01],\n",
       "        [-1.7820e-01,  1.5635e-01,  2.0383e+00,  ...,  1.6944e+00,\n",
       "          3.0975e-01, -4.6069e-01],\n",
       "        [-4.3511e-01, -2.4559e-01,  7.3841e-01,  ...,  7.2764e-02,\n",
       "         -1.2494e+00,  1.4188e-03]]), 'transformerBlocksLayer.0.multiHeadAttention.mask': tensor([[0., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'transformerBlocksLayer.0.multiHeadAttention.weightsQuery.weight': tensor([[-0.0145,  0.0009, -0.0159,  ...,  0.0345,  0.0248, -0.0005],\n",
       "        [-0.0192, -0.0138, -0.0155,  ...,  0.0027, -0.0016,  0.0211],\n",
       "        [ 0.0154, -0.0178, -0.0071,  ...,  0.0292,  0.0002,  0.0068],\n",
       "        ...,\n",
       "...\n",
       "\u001b[36mres40_3\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = None"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val modelStateKey = \"model\"\n",
    "val optimizerStateKey = \"optimizer\"\n",
    "val statesMap = py\"{$modelStateKey: ${model.state_dict()}, $optimizerStateKey: ${optimizer.state_dict()}}\"\n",
    "torch.save(statesMap, \"model_and_optimizer.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.13.14",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
