{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee7f466c-940b-4615-b173-79c22ad855b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 20479 characters from data/the_verdict.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mscala.io.Source\u001b[39m\n",
       "\u001b[36mfilePath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"data/the_verdict.txt\"\u001b[39m\n",
       "\u001b[36mrawText\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
       "\n",
       "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\n",
       "\n",
       "Well!--even through the prism of Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own trade hardly a murmur. Professional jealousy? Perhaps. If it were, the honour of the craft was vindicated by little Claude Nutley, who, in all good faith, brought out in the Burlington a very handsome \"obituary\" on Jack--one of those showy articles stocked with random technicalities that I have heard (I won't say by whom) compared to Gisburn's painting. And so--his resolve being apparently irrevocable--the discussion gradually died out, and, as Mrs. Thwing had predicted, the price of \"Gisburns\" went up.\n",
       "\n",
       "It was not till three years later that, in the course of a few weeks' idling on the Riviera, it suddenly occurred to me to wonder why Gisburn had given up his painting. On reflection, it really was a tempting problem. To accuse his wife would have been too easy--his fair sitters had been denied the solace of saying that Mrs. Gisburn had \"dragged him down.\" For Mrs. Gisburn--as such--had not existed till nearly a year after Jack's resolve had been taken. It might be that he had married her--since he liked his ease--because he didn't want to go on painting; but it would have been hard to prove that he had given up his painting because he had married her.\n",
       "\n",
       "Of course, if she had not dragged him down, she had equally, as Miss Croft contended, failed to \"lift him up\"--she had not led him back to the easel. To put the\u001b[39m..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.io.Source\n",
    "\n",
    "val filePath = \"data/the_verdict.txt\"\n",
    "val rawText = Source.fromFile(filePath).mkString\n",
    "\n",
    "println(s\"Read ${rawText.length} characters from $filePath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a9979c3-f35c-4e06-ac9a-33e38e46782b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector(Hello, ,, world, ., Is, this, --, a, test, ?)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtokenize\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text: String): Vector[String] = {\n",
    "  val splitBy = \"\"\"[,.:;?_!\"()\\']|--|\\s\"\"\"\n",
    "  text.split(s\"(?<=$splitBy)|(?=$splitBy)\").filter(!_.isBlank).toVector\n",
    "}\n",
    "\n",
    "println(tokenize(\"Hello, world. Is this-- a test?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b94cbbe9-68d1-449a-b8db-1a8f72dc4143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 4690 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtokenizedText\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m\"I\"\u001b[39m,\n",
       "  \u001b[32m\"HAD\"\u001b[39m,\n",
       "  \u001b[32m\"always\"\u001b[39m,\n",
       "  \u001b[32m\"thought\"\u001b[39m,\n",
       "  \u001b[32m\"Jack\"\u001b[39m,\n",
       "  \u001b[32m\"Gisburn\"\u001b[39m,\n",
       "  \u001b[32m\"rather\"\u001b[39m,\n",
       "  \u001b[32m\"a\"\u001b[39m,\n",
       "  \u001b[32m\"cheap\"\u001b[39m,\n",
       "  \u001b[32m\"genius\"\u001b[39m,\n",
       "  \u001b[32m\"--\"\u001b[39m,\n",
       "  \u001b[32m\"though\"\u001b[39m,\n",
       "  \u001b[32m\"a\"\u001b[39m,\n",
       "  \u001b[32m\"good\"\u001b[39m,\n",
       "  \u001b[32m\"fellow\"\u001b[39m,\n",
       "  \u001b[32m\"enough\"\u001b[39m,\n",
       "  \u001b[32m\"--\"\u001b[39m,\n",
       "  \u001b[32m\"so\"\u001b[39m,\n",
       "  \u001b[32m\"it\"\u001b[39m,\n",
       "  \u001b[32m\"was\"\u001b[39m,\n",
       "  \u001b[32m\"no\"\u001b[39m,\n",
       "  \u001b[32m\"great\"\u001b[39m,\n",
       "  \u001b[32m\"surprise\"\u001b[39m,\n",
       "  \u001b[32m\"to\"\u001b[39m,\n",
       "  \u001b[32m\"me\"\u001b[39m,\n",
       "  \u001b[32m\"to\"\u001b[39m,\n",
       "  \u001b[32m\"hear\"\u001b[39m,\n",
       "  \u001b[32m\"that\"\u001b[39m,\n",
       "  \u001b[32m\",\"\u001b[39m,\n",
       "  \u001b[32m\"in\"\u001b[39m,\n",
       "  \u001b[32m\"the\"\u001b[39m,\n",
       "  \u001b[32m\"height\"\u001b[39m,\n",
       "  \u001b[32m\"of\"\u001b[39m,\n",
       "  \u001b[32m\"his\"\u001b[39m,\n",
       "  \u001b[32m\"glory\"\u001b[39m,\n",
       "  \u001b[32m\",\"\u001b[39m,\n",
       "  \u001b[32m\"he\"\u001b[39m,\n",
       "  \u001b[32m\"had\"\u001b[39m,\n",
       "..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizedText = tokenize(rawText)\n",
    "\n",
    "println(s\"Extracted ${tokenizedText.length} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fa3b124-3fc6-4084-b461-bcf9972f1b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130 distinct tokens in total\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msortedDistinctTokens\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m\"!\"\u001b[39m,\n",
       "  \u001b[32m\"\\\"\"\u001b[39m,\n",
       "  \u001b[32m\"'\"\u001b[39m,\n",
       "  \u001b[32m\"(\"\u001b[39m,\n",
       "  \u001b[32m\")\"\u001b[39m,\n",
       "  \u001b[32m\",\"\u001b[39m,\n",
       "  \u001b[32m\"--\"\u001b[39m,\n",
       "  \u001b[32m\".\"\u001b[39m,\n",
       "  \u001b[32m\":\"\u001b[39m,\n",
       "  \u001b[32m\";\"\u001b[39m,\n",
       "  \u001b[32m\"?\"\u001b[39m,\n",
       "  \u001b[32m\"A\"\u001b[39m,\n",
       "  \u001b[32m\"Ah\"\u001b[39m,\n",
       "  \u001b[32m\"Among\"\u001b[39m,\n",
       "  \u001b[32m\"And\"\u001b[39m,\n",
       "  \u001b[32m\"Are\"\u001b[39m,\n",
       "  \u001b[32m\"Arrt\"\u001b[39m,\n",
       "  \u001b[32m\"As\"\u001b[39m,\n",
       "  \u001b[32m\"At\"\u001b[39m,\n",
       "  \u001b[32m\"Be\"\u001b[39m,\n",
       "  \u001b[32m\"Begin\"\u001b[39m,\n",
       "  \u001b[32m\"Burlington\"\u001b[39m,\n",
       "  \u001b[32m\"But\"\u001b[39m,\n",
       "  \u001b[32m\"By\"\u001b[39m,\n",
       "  \u001b[32m\"Carlo\"\u001b[39m,\n",
       "  \u001b[32m\"Chicago\"\u001b[39m,\n",
       "  \u001b[32m\"Claude\"\u001b[39m,\n",
       "  \u001b[32m\"Come\"\u001b[39m,\n",
       "  \u001b[32m\"Croft\"\u001b[39m,\n",
       "  \u001b[32m\"Destroyed\"\u001b[39m,\n",
       "  \u001b[32m\"Devonshire\"\u001b[39m,\n",
       "  \u001b[32m\"Don\"\u001b[39m,\n",
       "  \u001b[32m\"Dubarry\"\u001b[39m,\n",
       "  \u001b[32m\"Emperors\"\u001b[39m,\n",
       "  \u001b[32m\"Florence\"\u001b[39m,\n",
       "  \u001b[32m\"For\"\u001b[39m,\n",
       "  \u001b[32m\"Gallery\"\u001b[39m,\n",
       "  \u001b[32m\"Gideon\"\u001b[39m,\n",
       "..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sortedDistinctTokens = tokenizedText.sorted.distinct\n",
    "\n",
    "println(s\"${sortedDistinctTokens.length} distinct tokens in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9bc07ee-6629-47a2-8406-73fab54aa5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mvocabulary\u001b[39m: \u001b[32mMap\u001b[39m[\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m] = \u001b[33mHashMap\u001b[39m(\n",
       "  \u001b[32m\"inevitable\"\u001b[39m -> \u001b[32m571\u001b[39m,\n",
       "  \u001b[32m\"Monte\"\u001b[39m -> \u001b[32m64\u001b[39m,\n",
       "  \u001b[32m\"down\"\u001b[39m -> \u001b[32m362\u001b[39m,\n",
       "  \u001b[32m\"economy\"\u001b[39m -> \u001b[32m377\u001b[39m,\n",
       "  \u001b[32m\"interesting\"\u001b[39m -> \u001b[32m578\u001b[39m,\n",
       "  \u001b[32m\"luxury\"\u001b[39m -> \u001b[32m652\u001b[39m,\n",
       "  \u001b[32m\"serious\"\u001b[39m -> \u001b[32m870\u001b[39m,\n",
       "  \u001b[32m\"forgotten\"\u001b[39m -> \u001b[32m463\u001b[39m,\n",
       "  \u001b[32m\"muscles\"\u001b[39m -> \u001b[32m695\u001b[39m,\n",
       "  \u001b[32m\"beneath\"\u001b[39m -> \u001b[32m215\u001b[39m,\n",
       "  \u001b[32m\"used\"\u001b[39m -> \u001b[32m1057\u001b[39m,\n",
       "  \u001b[32m\"eye\"\u001b[39m -> \u001b[32m415\u001b[39m,\n",
       "  \u001b[32m\"straining\"\u001b[39m -> \u001b[32m934\u001b[39m,\n",
       "  \u001b[32m\"At\"\u001b[39m -> \u001b[32m18\u001b[39m,\n",
       "  \u001b[32m\"hooded\"\u001b[39m -> \u001b[32m554\u001b[39m,\n",
       "  \u001b[32m\"murmur\"\u001b[39m -> \u001b[32m694\u001b[39m,\n",
       "  \u001b[32m\"adulation\"\u001b[39m -> \u001b[32m133\u001b[39m,\n",
       "  \u001b[32m\"gloried\"\u001b[39m -> \u001b[32m495\u001b[39m,\n",
       "  \u001b[32m\"widow\"\u001b[39m -> \u001b[32m1102\u001b[39m,\n",
       "  \u001b[32m\"panel\"\u001b[39m -> \u001b[32m752\u001b[39m,\n",
       "  \u001b[32m\"sitters\"\u001b[39m -> \u001b[32m898\u001b[39m,\n",
       "  \u001b[32m\"quality\"\u001b[39m -> \u001b[32m808\u001b[39m,\n",
       "  \u001b[32m\"On\"\u001b[39m -> \u001b[32m75\u001b[39m,\n",
       "  \u001b[32m\"Has\"\u001b[39m -> \u001b[32m47\u001b[39m,\n",
       "  \u001b[32m\"secret\"\u001b[39m -> \u001b[32m861\u001b[39m,\n",
       "  \u001b[32m\"Money\"\u001b[39m -> \u001b[32m63\u001b[39m,\n",
       "  \u001b[32m\"ourselves\"\u001b[39m -> \u001b[32m737\u001b[39m,\n",
       "  \u001b[32m\"able\"\u001b[39m -> \u001b[32m117\u001b[39m,\n",
       "  \u001b[32m\"bravura\"\u001b[39m -> \u001b[32m226\u001b[39m,\n",
       "  \u001b[32m\"behind\"\u001b[39m -> \u001b[32m212\u001b[39m,\n",
       "  \u001b[32m\"failure\"\u001b[39m -> \u001b[32m423\u001b[39m,\n",
       "  \u001b[32m\"deprecating\"\u001b[39m -> \u001b[32m327\u001b[39m,\n",
       "  \u001b[32m\"for\"\u001b[39m -> \u001b[32m456\u001b[39m,\n",
       "  \u001b[32m\"canvases\"\u001b[39m -> \u001b[32m246\u001b[39m,\n",
       "  \u001b[32m\"wits\"\u001b[39m -> \u001b[32m1110\u001b[39m,\n",
       "  \u001b[32m\"s\"\u001b[39m -> \u001b[32m850\u001b[39m,\n",
       "  \u001b[32m\"twenty-four\"\u001b[39m -> \u001b[32m1039\u001b[39m,\n",
       "  \u001b[32m\"paint\"\u001b[39m -> \u001b[32m745\u001b[39m,\n",
       "...\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mSimpleTokenizerV1\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vocabulary = sortedDistinctTokens.zipWithIndex.toMap\n",
    "\n",
    "class SimpleTokenizerV1(\n",
    "  vocabulary: Map[String, Int]\n",
    ") {\n",
    "  val inverseVocabulary = vocabulary.map(_.swap)\n",
    "\n",
    "  def encode(text: String): Vector[Int] = \n",
    "    tokenize(text).map(vocabulary(_))\n",
    "\n",
    "  def tokenize(text: String): Vector[String] = {\n",
    "    val splitBy = \"\"\"[,.:;?_!\"()\\']|--|\\s\"\"\"\n",
    "    val tokenizer = s\"(?<=$splitBy)|(?=$splitBy)\"\n",
    "    text.split(tokenizer).filter(!_.isBlank).toVector\n",
    "  }\n",
    "\n",
    "  def decode(ids: Vector[Int]): String = \n",
    "    ids\n",
    "      .map(inverseVocabulary(_))\n",
    "      .mkString(\" \")\n",
    "      .replaceAll(\"\\\\s+([,.?!\\\"()\\'])\", \"$1\") \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c700d5f3-cace-49ca-b5c7-055c07418673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtokenizer\u001b[39m: \u001b[32mSimpleTokenizerV1\u001b[39m = ammonite.$sess.cmd5$Helper$SimpleTokenizerV1@40e94690\n",
       "\u001b[36mtextToEncode\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"\"It's the last he painted, you know,\"\n",
       "Mrs. Gisburn said with pardonable pride.\"\"\"\u001b[39m\n",
       "\u001b[36mids\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m1\u001b[39m,\n",
       "  \u001b[32m56\u001b[39m,\n",
       "  \u001b[32m2\u001b[39m,\n",
       "  \u001b[32m850\u001b[39m,\n",
       "  \u001b[32m988\u001b[39m,\n",
       "  \u001b[32m602\u001b[39m,\n",
       "  \u001b[32m533\u001b[39m,\n",
       "  \u001b[32m746\u001b[39m,\n",
       "  \u001b[32m5\u001b[39m,\n",
       "  \u001b[32m1126\u001b[39m,\n",
       "  \u001b[32m596\u001b[39m,\n",
       "  \u001b[32m5\u001b[39m,\n",
       "  \u001b[32m1\u001b[39m,\n",
       "  \u001b[32m67\u001b[39m,\n",
       "  \u001b[32m7\u001b[39m,\n",
       "  \u001b[32m38\u001b[39m,\n",
       "  \u001b[32m851\u001b[39m,\n",
       "  \u001b[32m1108\u001b[39m,\n",
       "  \u001b[32m754\u001b[39m,\n",
       "  \u001b[32m793\u001b[39m,\n",
       "  \u001b[32m7\u001b[39m\n",
       ")\n",
       "\u001b[36mdecodedText\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\\\" It' s the last he painted, you know,\\\" Mrs. Gisburn said with pardonable pride.\"\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = new SimpleTokenizerV1(vocabulary)\n",
    "\n",
    "val textToEncode = \"\"\"\"It's the last he painted, you know,\"\n",
    "Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "val ids = tokenizer.encode(textToEncode)\n",
    "val decodedText = tokenizer.decode(ids)\n",
    "\n",
    "println(decodedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcd775a6-0fa2-4bbf-bcd0-4903f73db1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mendOfText\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"<|endoftext|>\"\u001b[39m\n",
       "\u001b[36mvocabularyWithEndOfText\u001b[39m: \u001b[32mMap\u001b[39m[\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m] = \u001b[33mHashMap\u001b[39m(\n",
       "  \u001b[32m\"inevitable\"\u001b[39m -> \u001b[32m571\u001b[39m,\n",
       "  \u001b[32m\"Monte\"\u001b[39m -> \u001b[32m64\u001b[39m,\n",
       "  \u001b[32m\"down\"\u001b[39m -> \u001b[32m362\u001b[39m,\n",
       "  \u001b[32m\"economy\"\u001b[39m -> \u001b[32m377\u001b[39m,\n",
       "  \u001b[32m\"interesting\"\u001b[39m -> \u001b[32m578\u001b[39m,\n",
       "  \u001b[32m\"luxury\"\u001b[39m -> \u001b[32m652\u001b[39m,\n",
       "  \u001b[32m\"serious\"\u001b[39m -> \u001b[32m870\u001b[39m,\n",
       "  \u001b[32m\"forgotten\"\u001b[39m -> \u001b[32m463\u001b[39m,\n",
       "  \u001b[32m\"muscles\"\u001b[39m -> \u001b[32m695\u001b[39m,\n",
       "  \u001b[32m\"beneath\"\u001b[39m -> \u001b[32m215\u001b[39m,\n",
       "  \u001b[32m\"used\"\u001b[39m -> \u001b[32m1057\u001b[39m,\n",
       "  \u001b[32m\"eye\"\u001b[39m -> \u001b[32m415\u001b[39m,\n",
       "  \u001b[32m\"straining\"\u001b[39m -> \u001b[32m934\u001b[39m,\n",
       "  \u001b[32m\"At\"\u001b[39m -> \u001b[32m18\u001b[39m,\n",
       "  \u001b[32m\"hooded\"\u001b[39m -> \u001b[32m554\u001b[39m,\n",
       "  \u001b[32m\"murmur\"\u001b[39m -> \u001b[32m694\u001b[39m,\n",
       "  \u001b[32m\"adulation\"\u001b[39m -> \u001b[32m133\u001b[39m,\n",
       "  \u001b[32m\"gloried\"\u001b[39m -> \u001b[32m495\u001b[39m,\n",
       "  \u001b[32m\"widow\"\u001b[39m -> \u001b[32m1102\u001b[39m,\n",
       "  \u001b[32m\"panel\"\u001b[39m -> \u001b[32m752\u001b[39m,\n",
       "  \u001b[32m\"sitters\"\u001b[39m -> \u001b[32m898\u001b[39m,\n",
       "  \u001b[32m\"quality\"\u001b[39m -> \u001b[32m808\u001b[39m,\n",
       "  \u001b[32m\"On\"\u001b[39m -> \u001b[32m75\u001b[39m,\n",
       "  \u001b[32m\"Has\"\u001b[39m -> \u001b[32m47\u001b[39m,\n",
       "  \u001b[32m\"secret\"\u001b[39m -> \u001b[32m861\u001b[39m,\n",
       "  \u001b[32m\"Money\"\u001b[39m -> \u001b[32m63\u001b[39m,\n",
       "  \u001b[32m\"ourselves\"\u001b[39m -> \u001b[32m737\u001b[39m,\n",
       "  \u001b[32m\"able\"\u001b[39m -> \u001b[32m117\u001b[39m,\n",
       "  \u001b[32m\"bravura\"\u001b[39m -> \u001b[32m226\u001b[39m,\n",
       "  \u001b[32m\"behind\"\u001b[39m -> \u001b[32m212\u001b[39m,\n",
       "  \u001b[32m\"failure\"\u001b[39m -> \u001b[32m423\u001b[39m,\n",
       "  \u001b[32m\"deprecating\"\u001b[39m -> \u001b[32m327\u001b[39m,\n",
       "  \u001b[32m\"for\"\u001b[39m -> \u001b[32m456\u001b[39m,\n",
       "  \u001b[32m\"canvases\"\u001b[39m -> \u001b[32m246\u001b[39m,\n",
       "  \u001b[32m\"wits\"\u001b[39m -> \u001b[32m1110\u001b[39m,\n",
       "  \u001b[32m\"s\"\u001b[39m -> \u001b[32m850\u001b[39m,\n",
       "  \u001b[32m\"twenty-four\"\u001b[39m -> \u001b[32m1039\u001b[39m,\n",
       "  \u001b[32m\"paint\"\u001b[39m -> \u001b[32m745\u001b[39m,\n",
       "...\n",
       "\u001b[36munknownTokenId\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m-1\u001b[39m\n",
       "\u001b[36munknownToken\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"<|unknown|>\"\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mSimpleTokenizerV2\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val endOfText = \"<|endoftext|>\"\n",
    "val vocabularyWithEndOfText = (sortedDistinctTokens :+ endOfText).zipWithIndex.toMap\n",
    "val (unknownTokenId, unknownToken) = -1 -> \"<|unknown|>\"\n",
    "\n",
    "class SimpleTokenizerV2(\n",
    "  vocabulary: Map[String, Int]\n",
    ") {\n",
    "  val inverseVocabulary = vocabulary.map(_.swap)\n",
    "\n",
    "  def encode(text: String): Vector[Int] = \n",
    "    tokenize(text).map(vocabulary.getOrElse(_, unknownTokenId))\n",
    "\n",
    "  def tokenize(text: String): Vector[String] = {\n",
    "    val splitBy = \"\"\"[,.:;?_!\"()\\']|--|\\s\"\"\"\n",
    "    val tokenizer = s\"(?<=$splitBy)|(?=$splitBy)\"\n",
    "    text.split(tokenizer).filter(!_.isBlank).toVector\n",
    "  }\n",
    "\n",
    "  def decode(ids: Vector[Int]): String = \n",
    "    ids\n",
    "      .map(inverseVocabulary.getOrElse(_, unknownToken))\n",
    "      .mkString(\" \")\n",
    "      .replaceAll(\"\\\\s+([,.?!\\\"()\\'])\", \"$1\") \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e5544b2-7df0-4b2e-aa8b-227ee4973596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unknown|>, do you like tea? <|unknown|> In the sunlit terraces of the <|unknown|>.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtokenizerV2\u001b[39m: \u001b[32mSimpleTokenizerV2\u001b[39m = ammonite.$sess.cmd7$Helper$SimpleTokenizerV2@77d6290b\n",
       "\u001b[36mconcatenatedText\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\"\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.chaining._\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizerV2 = new SimpleTokenizerV2(vocabulary)\n",
    "\n",
    "val concatenatedText = \"Hello, do you like tea?\" + s\" $endOfText \" + \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "import scala.util.chaining._\n",
    "println(concatenatedText.pipe(tokenizerV2.encode).pipe(tokenizerV2.decode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a21b222f-514e-4c34-a78d-2b1df8ebeab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://repo1.maven.org/maven2/dev/scalapy/scalapy-core_2.13/0.5.3/scalapy-core_2.13-0.5.3.pom\n",
      "Downloading https://repo1.maven.org/maven2/sh/almond/almond-scalapy_2.13/0.14.0-RC15/almond-scalapy_2.13-0.14.0-RC15.pom\n",
      "Downloaded https://repo1.maven.org/maven2/dev/scalapy/scalapy-core_2.13/0.5.3/scalapy-core_2.13-0.5.3.pom\n",
      "Downloaded https://repo1.maven.org/maven2/sh/almond/almond-scalapy_2.13/0.14.0-RC15/almond-scalapy_2.13-0.14.0-RC15.pom\n",
      "Downloading https://repo1.maven.org/maven2/dev/scalapy/scalapy-macros_2.13/0.5.3/scalapy-macros_2.13-0.5.3.pom\n",
      "Downloaded https://repo1.maven.org/maven2/dev/scalapy/scalapy-macros_2.13/0.5.3/scalapy-macros_2.13-0.5.3.pom\n",
      "Downloading https://repo1.maven.org/maven2/sh/almond/almond-scalapy_2.13/0.14.0-RC15/almond-scalapy_2.13-0.14.0-RC15-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/dev/scalapy/scalapy-macros_2.13/0.5.3/scalapy-macros_2.13-0.5.3-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/dev/scalapy/scalapy-core_2.13/0.5.3/scalapy-core_2.13-0.5.3-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/dev/scalapy/scalapy-core_2.13/0.5.3/scalapy-core_2.13-0.5.3.jar\n",
      "Downloading https://repo1.maven.org/maven2/dev/scalapy/scalapy-macros_2.13/0.5.3/scalapy-macros_2.13-0.5.3.jar\n",
      "Downloading https://repo1.maven.org/maven2/sh/almond/almond-scalapy_2.13/0.14.0-RC15/almond-scalapy_2.13-0.14.0-RC15.jar\n",
      "Downloaded https://repo1.maven.org/maven2/sh/almond/almond-scalapy_2.13/0.14.0-RC15/almond-scalapy_2.13-0.14.0-RC15-sources.jar\n",
      "Downloaded https://repo1.maven.org/maven2/dev/scalapy/scalapy-macros_2.13/0.5.3/scalapy-macros_2.13-0.5.3-sources.jar\n",
      "Downloaded https://repo1.maven.org/maven2/dev/scalapy/scalapy-core_2.13/0.5.3/scalapy-core_2.13-0.5.3-sources.jar\n",
      "Downloaded https://repo1.maven.org/maven2/dev/scalapy/scalapy-macros_2.13/0.5.3/scalapy-macros_2.13-0.5.3.jar\n",
      "Downloaded https://repo1.maven.org/maven2/sh/almond/almond-scalapy_2.13/0.14.0-RC15/almond-scalapy_2.13-0.14.0-RC15.jar\n",
      "Downloaded https://repo1.maven.org/maven2/dev/scalapy/scalapy-core_2.13/0.5.3/scalapy-core_2.13-0.5.3.jar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`dev.scalapy::scalapy-core:0.5.3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e17ff4e1-c28d-4b30-b89e-66989ca9d0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling /workspace/Magic.sc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $file.^.Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cf50989-6789-41e5-89ab-0998764abeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken==0.7.*\n",
      "  Downloading tiktoken-0.7.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (6.6 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken==0.7.*)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/site-packages (from tiktoken==0.7.*) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.7.*) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.7.*) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.7.*) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.7.*) (2024.12.14)\n",
      "Downloading tiktoken-0.7.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.0 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 1.7 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (794 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 795.0/795.0 kB 1.9 MB/s eta 0:00:00\n",
      "Installing collected packages: regex, tiktoken\n",
      "Successfully installed regex-2024.11.6 tiktoken-0.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "Magic.!(\"pip\", \"install\", \"tiktoken==0.7.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61f69f6d-354c-45cc-a011-5b217fb906ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mme.shadaj.scalapy.py\u001b[39m\n",
       "\u001b[36mtiktoken\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mModule\u001b[39m = <module 'tiktoken' from '/usr/local/lib/python3.12/site-packages/tiktoken/__init__.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import me.shadaj.scalapy.py\n",
    "\n",
    "val tiktoken = py.module(\"tiktoken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38cf2816-76b3-4efe-838c-c30dd81654f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mpy.SeqConverters\u001b[39m\n",
       "\u001b[36mtiktokenizer\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <Encoding 'gpt2'>\n",
       "\u001b[36mtiktext\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace\"\u001b[39m\n",
       "\u001b[36mallowedSpecial\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = {'<|endoftext|>'}\n",
       "\u001b[36mtiktokens\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m15496\u001b[39m,\n",
       "  \u001b[32m11\u001b[39m,\n",
       "  \u001b[32m466\u001b[39m,\n",
       "  \u001b[32m345\u001b[39m,\n",
       "  \u001b[32m588\u001b[39m,\n",
       "  \u001b[32m8887\u001b[39m,\n",
       "  \u001b[32m30\u001b[39m,\n",
       "  \u001b[32m220\u001b[39m,\n",
       "  \u001b[32m50256\u001b[39m,\n",
       "  \u001b[32m554\u001b[39m,\n",
       "  \u001b[32m262\u001b[39m,\n",
       "  \u001b[32m4252\u001b[39m,\n",
       "  \u001b[32m18250\u001b[39m,\n",
       "  \u001b[32m8812\u001b[39m,\n",
       "  \u001b[32m2114\u001b[39m,\n",
       "  \u001b[32m286\u001b[39m,\n",
       "  \u001b[32m617\u001b[39m,\n",
       "  \u001b[32m34680\u001b[39m,\n",
       "  \u001b[32m27271\u001b[39m\n",
       ")\n",
       "\u001b[36mdecodedTiktokens\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace\"\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import py.SeqConverters\n",
    "\n",
    "val tiktokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "val tiktext = s\"Hello, do you like tea? $endOfText In the sunlit terraces of someunknownPlace\"\n",
    "val allowedSpecial = py.Dynamic.global.set(Seq(endOfText).toPythonProxy)\n",
    "val tiktokens = tiktokenizer.encode(tiktext, allowed_special = allowedSpecial).as[Vector[Int]]\n",
    "val decodedTiktokens = tiktokenizer.decode(tiktokens.toPythonProxy).as[String]\n",
    "\n",
    "println(decodedTiktokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8633662-62f7-4276-b36c-e20a8ced8ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Akwirw ier\n",
      "Encoding: \n",
      "  \" \" -> 220\n",
      "  \"Ak\" -> 33901\n",
      "  \"ier\" -> 959\n",
      "  \"ir\" -> 343\n",
      "  \"w\" -> 86\n",
      "Decoded: Akwirw ier\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36munknownWords\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Akwirw ier\"\u001b[39m\n",
       "\u001b[36mencodedUnknownWords\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mVector\u001b[39m(\u001b[32m33901\u001b[39m, \u001b[32m86\u001b[39m, \u001b[32m343\u001b[39m, \u001b[32m86\u001b[39m, \u001b[32m220\u001b[39m, \u001b[32m959\u001b[39m)\n",
       "\u001b[36mencoding\u001b[39m: \u001b[32mMap\u001b[39m[\u001b[32mInt\u001b[39m, \u001b[32mString\u001b[39m] = \u001b[33mHashMap\u001b[39m(\n",
       "  \u001b[32m220\u001b[39m -> \u001b[32m\" \"\u001b[39m,\n",
       "  \u001b[32m33901\u001b[39m -> \u001b[32m\"Ak\"\u001b[39m,\n",
       "  \u001b[32m343\u001b[39m -> \u001b[32m\"ir\"\u001b[39m,\n",
       "  \u001b[32m86\u001b[39m -> \u001b[32m\"w\"\u001b[39m,\n",
       "  \u001b[32m959\u001b[39m -> \u001b[32m\"ier\"\u001b[39m\n",
       ")\n",
       "\u001b[36mdecodedUnknownWords\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Akwirw ier\"\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Exercise 2.1\n",
    "val unknownWords = \"Akwirw ier\"\n",
    "println(s\"Input: $unknownWords\")\n",
    "\n",
    "val encodedUnknownWords = tiktokenizer.encode(unknownWords).as[Vector[Int]]\n",
    "val encoding = encodedUnknownWords.map(int => int -> tiktokenizer.decode(Seq(int).toPythonProxy).as[String]).toMap\n",
    "\n",
    "println(\"Encoding: \")\n",
    "encoding.toList.sortBy { case (_, subword) => subword }.foreach { case (int, subword) =>\n",
    "  println(s\"  \\\"$subword\\\" -> $int\")\n",
    "}\n",
    "\n",
    "val decodedUnknownWords = tiktokenizer.decode(encodedUnknownWords.toPythonProxy).as[String]\n",
    "println(s\"Decoded: $decodedUnknownWords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f28f09e1-c4fa-4c40-8bf3-0e191b402715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text token count: 5145\n",
      " and --->  established\n",
      " and established --->  himself\n",
      " and established himself --->  in\n",
      " and established himself in --->  a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrawTextTokenized\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m40\u001b[39m,\n",
       "  \u001b[32m367\u001b[39m,\n",
       "  \u001b[32m2885\u001b[39m,\n",
       "  \u001b[32m1464\u001b[39m,\n",
       "  \u001b[32m1807\u001b[39m,\n",
       "  \u001b[32m3619\u001b[39m,\n",
       "  \u001b[32m402\u001b[39m,\n",
       "  \u001b[32m271\u001b[39m,\n",
       "  \u001b[32m10899\u001b[39m,\n",
       "  \u001b[32m2138\u001b[39m,\n",
       "  \u001b[32m257\u001b[39m,\n",
       "  \u001b[32m7026\u001b[39m,\n",
       "  \u001b[32m15632\u001b[39m,\n",
       "  \u001b[32m438\u001b[39m,\n",
       "  \u001b[32m2016\u001b[39m,\n",
       "  \u001b[32m257\u001b[39m,\n",
       "  \u001b[32m922\u001b[39m,\n",
       "  \u001b[32m5891\u001b[39m,\n",
       "  \u001b[32m1576\u001b[39m,\n",
       "  \u001b[32m438\u001b[39m,\n",
       "  \u001b[32m568\u001b[39m,\n",
       "  \u001b[32m340\u001b[39m,\n",
       "  \u001b[32m373\u001b[39m,\n",
       "  \u001b[32m645\u001b[39m,\n",
       "  \u001b[32m1049\u001b[39m,\n",
       "  \u001b[32m5975\u001b[39m,\n",
       "  \u001b[32m284\u001b[39m,\n",
       "  \u001b[32m502\u001b[39m,\n",
       "  \u001b[32m284\u001b[39m,\n",
       "  \u001b[32m3285\u001b[39m,\n",
       "  \u001b[32m326\u001b[39m,\n",
       "  \u001b[32m11\u001b[39m,\n",
       "  \u001b[32m287\u001b[39m,\n",
       "  \u001b[32m262\u001b[39m,\n",
       "  \u001b[32m6001\u001b[39m,\n",
       "  \u001b[32m286\u001b[39m,\n",
       "  \u001b[32m465\u001b[39m,\n",
       "  \u001b[32m13476\u001b[39m,\n",
       "...\n",
       "\u001b[36mrawTextTokensSampled\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m290\u001b[39m,\n",
       "  \u001b[32m4920\u001b[39m,\n",
       "  \u001b[32m2241\u001b[39m,\n",
       "  \u001b[32m287\u001b[39m,\n",
       "  \u001b[32m257\u001b[39m,\n",
       "  \u001b[32m4489\u001b[39m,\n",
       "  \u001b[32m64\u001b[39m,\n",
       "  \u001b[32m319\u001b[39m,\n",
       "  \u001b[32m262\u001b[39m,\n",
       "  \u001b[32m34686\u001b[39m,\n",
       "  \u001b[32m41976\u001b[39m,\n",
       "  \u001b[32m13\u001b[39m,\n",
       "  \u001b[32m357\u001b[39m,\n",
       "  \u001b[32m10915\u001b[39m,\n",
       "  \u001b[32m314\u001b[39m,\n",
       "  \u001b[32m2138\u001b[39m,\n",
       "  \u001b[32m1807\u001b[39m,\n",
       "  \u001b[32m340\u001b[39m,\n",
       "  \u001b[32m561\u001b[39m,\n",
       "  \u001b[32m423\u001b[39m,\n",
       "  \u001b[32m587\u001b[39m,\n",
       "  \u001b[32m10598\u001b[39m,\n",
       "  \u001b[32m393\u001b[39m,\n",
       "  \u001b[32m28537\u001b[39m,\n",
       "  \u001b[32m2014\u001b[39m,\n",
       "  \u001b[32m198\u001b[39m,\n",
       "  \u001b[32m198\u001b[39m,\n",
       "  \u001b[32m1\u001b[39m,\n",
       "  \u001b[32m464\u001b[39m,\n",
       "  \u001b[32m6001\u001b[39m,\n",
       "  \u001b[32m286\u001b[39m,\n",
       "  \u001b[32m465\u001b[39m,\n",
       "  \u001b[32m13476\u001b[39m,\n",
       "  \u001b[32m1\u001b[39m,\n",
       "  \u001b[32m438\u001b[39m,\n",
       "  \u001b[32m5562\u001b[39m,\n",
       "  \u001b[32m373\u001b[39m,\n",
       "  \u001b[32m644\u001b[39m,\n",
       "...\n",
       "\u001b[36mcontextSize\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m4\u001b[39m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawTextTokenized = tiktokenizer.encode(rawText).as[Vector[Int]]\n",
    "\n",
    "println(s\"Raw text token count: ${rawTextTokenized.length}\")\n",
    "\n",
    "val rawTextTokensSampled = rawTextTokenized.drop(50)\n",
    "\n",
    "val contextSize = 4\n",
    "\n",
    "(1 to contextSize).foreach { size =>\n",
    "  val context = rawTextTokensSampled.take(size)\n",
    "  val desired = rawTextTokensSampled(size)\n",
    "  println(s\"${tiktokenizer.decode(context.toPythonProxy)} ---> ${tiktokenizer.decode(Seq(desired).toPythonProxy)}\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45178bc2-f7b0-4a0a-b9d2-f181720a879d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.4.*\n",
      "  Downloading torch-2.4.1-cp312-cp312-manylinux2014_aarch64.whl.metadata (26 kB)\n",
      "Collecting filelock (from torch==2.4.*)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (4.12.2)\n",
      "Collecting sympy (from torch==2.4.*)\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.4.*)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (3.1.5)\n",
      "Collecting fsspec (from torch==2.4.*)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (75.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->torch==2.4.*) (3.0.2)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.4.*)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.4.1-cp312-cp312-manylinux2014_aarch64.whl (89.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 89.6/89.6 MB 18.6 MB/s eta 0:00:00\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 52.8 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/6.2 MB 24.7 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 536.2/536.2 kB 22.7 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, networkx, fsspec, filelock, torch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed filelock-3.16.1 fsspec-2024.12.0 mpmath-1.3.0 networkx-3.4.2 sympy-1.13.3 torch-2.4.1\n"
     ]
    }
   ],
   "source": [
    "Magic.!(\"pip\", \"install\", \"torch==2.4.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7462d58f-c77a-4a8f-afa2-39c33b391c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mme.shadaj.scalapy.interpreter.CPythonInterpreter\u001b[39m\n",
       "\u001b[36mtorch\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mModule\u001b[39m = <module 'torch' from '/usr/local/lib/python3.12/site-packages/torch/__init__.py'>\n",
       "defined \u001b[32mtype\u001b[39m \u001b[36mTokenizer\u001b[39m\n",
       "defined \u001b[32mtype\u001b[39m \u001b[36mTorchTensor\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mGPTDatasetV1\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcreateDataLoaderV1\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import me.shadaj.scalapy.interpreter.CPythonInterpreter\n",
    "\n",
    "val torch = py.module(\"torch\")\n",
    "\n",
    "// Workaround to define a class that inherits from a Python class\n",
    "CPythonInterpreter.execManyLines {\n",
    "  s\"\"\"from torch.utils.data import Dataset\n",
    "     |\n",
    "     |class GPTDatasetV1(Dataset):\n",
    "     |  def __init__(self, input_tokens, target_tokens):\n",
    "     |    self.input_tokens = input_tokens\n",
    "     |    self.target_tokens = target_tokens\n",
    "     |  \n",
    "     |  def __len__(self):\n",
    "     |    return len(self.input_tokens)\n",
    "     |\n",
    "     |  def __getitem__(self, index):\n",
    "     |    return self.input_tokens[index], self.target_tokens[index]\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "type Tokenizer = py.Dynamic\n",
    "type TorchTensor = py.Dynamic\n",
    "def GPTDatasetV1(\n",
    "  text: String,\n",
    "  tokenizer: Tokenizer,\n",
    "  maxLength: Int,\n",
    "  step: Int\n",
    "): py.Dynamic = {\n",
    "  val tokens = tokenizer.encode(text).as[Vector[Int]]\n",
    "  val (inputTokens, outputTokens) = (0 until tokens.length by step).foldLeft(\n",
    "    (\n",
    "      Vector.empty[TorchTensor], \n",
    "      Vector.empty[TorchTensor]\n",
    "    )\n",
    "  ) {\n",
    "    case ((inputTokens, outputTokens), i) =>\n",
    "      val inputChunk = tokens.slice(i, i + maxLength)\n",
    "      val outputChunk = tokens.slice(i + 1, i + 1 + maxLength)\n",
    "      (\n",
    "       inputTokens :+ torch.tensor(inputChunk.toPythonProxy), \n",
    "       outputTokens :+ torch.tensor(outputChunk.toPythonProxy)\n",
    "      )\n",
    "  }\n",
    "  py.Dynamic.global.GPTDatasetV1(inputTokens.toPythonProxy, outputTokens.toPythonProxy)\n",
    "}\n",
    "\n",
    "def createDataLoaderV1(\n",
    "  text: String, \n",
    "  batchSize: Int = 4, \n",
    "  maxLength: Int = 256,                       \n",
    "  step: Int = 128, \n",
    "  shuffle: Boolean = true, \n",
    "  dropLast: Boolean = true,\n",
    "  numWorkers: Int = 0 \n",
    "): py.Dynamic = {\n",
    "  val tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "  val dataset = GPTDatasetV1(text, tokenizer, maxLength, step)\n",
    "  torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size = batchSize,\n",
    "    shuffle = shuffle,\n",
    "    drop_last = dropLast,\n",
    "    num_workers = numWorkers\n",
    "  )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9542e984-338d-410f-bc59-386fcf5c0de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdataLoader\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch.utils.data.dataloader.DataLoader object at 0xfffe93433770>\n",
       "\u001b[36mdataLoaderIterator\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0xfffe93589f70>\n",
       "\u001b[36mfirstBatch\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = [tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
       "\u001b[36msecondBatch\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = [tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataLoader = createDataLoaderV1(\n",
    "  text = rawText, \n",
    "  batchSize = 1, \n",
    "  maxLength = 4, \n",
    "  step = 1, \n",
    "  shuffle = false\n",
    ")\n",
    "val dataLoaderIterator = py.Dynamic.global.iter(dataLoader)\n",
    "val firstBatch = py.Dynamic.global.next(dataLoaderIterator)\n",
    "val secondBatch = py.Dynamic.global.next(dataLoaderIterator)\n",
    "println(firstBatch)\n",
    "println(secondBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e406374c-dd93-46cf-b5e3-05b202d972c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxLength = 2, step = 2\n",
      "[tensor([[ 40, 367]]), tensor([[ 367, 2885]])]\n",
      "[tensor([[2885, 1464]]), tensor([[1464, 1807]])]\n",
      "maxLength = 8, step = 2\n",
      "[tensor([[  40,  367, 2885, 1464, 1807, 3619,  402,  271]]), tensor([[  367,  2885,  1464,  1807,  3619,   402,   271, 10899]])]\n",
      "[tensor([[ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138]]), tensor([[ 1464,  1807,  3619,   402,   271, 10899,  2138,   257]])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mprintFirst2Batches\u001b[39m"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Exercise 2.2\n",
    "def printFirst2Batches(maxLength: Int, step: Int) = {\n",
    "  println(s\"maxLength = $maxLength, step = $step\")\n",
    "  val dataLoader = createDataLoaderV1(\n",
    "    text = rawText, \n",
    "    batchSize = 1, \n",
    "    maxLength = maxLength, \n",
    "    step = step, \n",
    "    shuffle = false\n",
    "  )\n",
    "  val dataLoaderIterator = py.Dynamic.global.iter(dataLoader)\n",
    "  val firstBatch = py.Dynamic.global.next(dataLoaderIterator)\n",
    "  val secondBatch = py.Dynamic.global.next(dataLoaderIterator)\n",
    "  println(firstBatch)\n",
    "  println(secondBatch)\n",
    "}\n",
    "\n",
    "printFirst2Batches(maxLength = 2, step = 2)\n",
    "printFirst2Batches(maxLength = 8, step = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e6214c1-587a-41e2-905b-ddea12dbcf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Outputs:\n",
      "tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mbatchedDataloader\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch.utils.data.dataloader.DataLoader object at 0xfffe986167e0>\n",
       "\u001b[36minputTokens\u001b[39m: \u001b[32mTorchTensor\u001b[39m = tensor([[   40,   367,  2885,  1464],\n",
       "        [ 1807,  3619,   402,   271],\n",
       "        [10899,  2138,   257,  7026],\n",
       "        [15632,   438,  2016,   257],\n",
       "        [  922,  5891,  1576,   438],\n",
       "        [  568,   340,   373,   645],\n",
       "        [ 1049,  5975,   284,   502],\n",
       "        [  284,  3285,   326,    11]])\n",
       "\u001b[36moutputTokens\u001b[39m: \u001b[32mTorchTensor\u001b[39m = tensor([[  367,  2885,  1464,  1807],\n",
       "        [ 3619,   402,   271, 10899],\n",
       "        [ 2138,   257,  7026, 15632],\n",
       "        [  438,  2016,   257,   922],\n",
       "        [ 5891,  1576,   438,   568],\n",
       "        [  340,   373,   645,  1049],\n",
       "        [ 5975,   284,   502,   284],\n",
       "        [ 3285,   326,    11,   287]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val batchedDataloader = createDataLoaderV1(\n",
    "  text = rawText, \n",
    "  batchSize = 8, \n",
    "  maxLength = 4, \n",
    "  step = 4, // same as maxLength to prevent overlaps between inputs and outputs\n",
    "  shuffle = false\n",
    ")\n",
    "val Seq(inputTokens, outputTokens) = py.Dynamic.global.next(py.Dynamic.global.iter(batchedDataloader)).as[Seq[TorchTensor]]\n",
    "println(s\"Inputs:\\n$inputTokens\")\n",
    "println(s\"Outputs:\\n$outputTokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb63a764-1b0c-46f6-a1b1-66eb332573bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch to embed:\n",
      "tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Input embedding shape: torch.Size([8, 4, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mvocabularySize\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m50257\u001b[39m\n",
       "\u001b[36moutputDimension\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m256\u001b[39m\n",
       "\u001b[36mtokenEmbeddingLayer\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = Embedding(50257, 256)\n",
       "\u001b[36mmaxLength\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m4\u001b[39m\n",
       "\u001b[36mdataLoader\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch.utils.data.dataloader.DataLoader object at 0xfffe93f02870>\n",
       "\u001b[36minputTokensBatchToEmbed\u001b[39m: \u001b[32mTorchTensor\u001b[39m = tensor([[   40,   367,  2885,  1464],\n",
       "        [ 1807,  3619,   402,   271],\n",
       "        [10899,  2138,   257,  7026],\n",
       "        [15632,   438,  2016,   257],\n",
       "        [  922,  5891,  1576,   438],\n",
       "        [  568,   340,   373,   645],\n",
       "        [ 1049,  5975,   284,   502],\n",
       "        [  284,  3285,   326,    11]])\n",
       "\u001b[36minputTokensEmbedding\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = tensor([[[-7.5324e-01,  1.4207e+00,  1.1090e-01,  ...,  1.1820e+00,\n",
       "          -9.7022e-01,  3.3539e-02],\n",
       "         [-1.9319e+00,  2.4897e-01, -7.4391e-01,  ..., -4.6399e-01,\n",
       "          -5.6382e-01, -1.3706e+00],\n",
       "         [-5.9705e-01, -4.9029e-02, -1.9657e+00,  ...,  2.1607e-02,\n",
       "          -8.0000e-01,  2.5539e+00],\n",
       "         [ 1.8441e+00,  2.0269e-01, -9.0610e-01,  ..., -5.3811e-01,\n",
       "           7.3627e-01,  1.3739e+00]],\n",
       "\n",
       "        [[ 4.9395e-01, -3.8213e-01, -7.1031e-01,  ..., -1.2503e-01,\n",
       "          -3.9488e-02,  9.2930e-02],\n",
       "         [ 1.0507e+00, -1.3080e+00, -5.6091e-01,  ...,  2.3107e+00,\n",
       "           1.4606e-01,  9.6618e-01],\n",
       "         [ 6.3449e-01, -1.9989e+00, -1.4916e+00,  ...,  1.4797e-03,\n",
       "          -5.5906e-01,  8.5424e-01],\n",
       "         [-3.7891e-01, -2.6331e-01, -8.1994e-02,  ...,  1.0110e+00,\n",
       "           5.6035e-01, -4.4973e-01]],\n",
       "\n",
       "        [[-7.0407e-01,  1.7869e+00,  2.9226e-01,  ..., -1.5903e+00,\n",
       "          -4.4316e-01,  1.7257e+00],\n",
       "         [ 1.3900e+00,  6.9916e-01, -3.6281e-01,  ..., -1.3546e+00,\n",
       "          -7.2490e-01, -2.7186e-01],\n",
       "         [-1.9113e+00,  1.0787e+00, -3.9272e-01,  ...,  2.8719e-01,\n",
       "          -7.1628e-01,  4.7808e-01],\n",
       "         [-9.5176e-01, -1.2801e+00, -6.7023e-01,  ..., -4.1467e-01,\n",
       "          -7.6049e-01,  7.8191e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-2.2804e-01, -1.0691e+00, -1.8945e+00,  ..., -1.8082e-02,\n",
       "           1.7095e+00,  8.2832e-01],\n",
       "         [ 3.3820e-01, -4.6803e-01, -4.7421e-01,  ..., -4.0461e-01,\n",
       "          -6.0061e-01, -1.2679e+00],\n",
       "         [ 6.7996e-02,  9.1227e-01,  5.8476e-02,  ...,  1.9384e+00,\n",
       "           1.7118e+00, -2.4351e-01],\n",
       "         [-5.8516e-01, -1.2749e+00, -2.0975e-01,  ...,  1.3114e+00,\n",
       "           3.5171e-03, -6.2072e-03]],\n",
       "\n",
       "        [[ 7.2515e-02,  7.3216e-01,  6.1524e-01,  ..., -4.4987e-01,\n",
       "..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vocabularySize = 50257 // tiktoken vocabulary size\n",
    "val outputDimension = 256\n",
    "val tokenEmbeddingLayer = torch.nn.Embedding(vocabularySize, outputDimension)\n",
    "\n",
    "val maxLength = 4\n",
    "val dataLoader = createDataLoaderV1(\n",
    "  text = rawText, \n",
    "  batchSize = 8, \n",
    "  maxLength = maxLength, \n",
    "  step = maxLength,\n",
    "  shuffle = false\n",
    ")\n",
    "val Seq(inputTokensBatchToEmbed, _) = py.Dynamic.global.next(py.Dynamic.global.iter(dataLoader)).as[Seq[TorchTensor]]\n",
    "println(s\"Input batch to embed:\\n$inputTokensBatchToEmbed\")\n",
    "\n",
    "val inputTokensEmbedding = tokenEmbeddingLayer(inputTokensBatchToEmbed)\n",
    "println(s\"Input embedding shape: ${inputTokensEmbedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77401586-15ee-4de5-9010-27bfb0caf756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional embedding shape: torch.Size([4, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mcontextLength\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m4\u001b[39m\n",
       "\u001b[36mpositionalEmbeddingLayer\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = Embedding(4, 256)\n",
       "\u001b[36mpositionalEmbedding\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = tensor([[-0.3846,  0.3801,  0.3639,  ...,  0.2006, -1.9019, -0.7987],\n",
       "        [-0.2371, -0.7840, -0.6029,  ...,  0.7719, -0.4472,  1.4853],\n",
       "        [ 0.3948,  0.2845, -1.8637,  ...,  0.1639,  1.8620,  0.4120],\n",
       "        [-0.3974, -1.2032,  1.8989,  ..., -0.3128, -0.7291, -1.0689]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val contextLength = maxLength\n",
    "val positionalEmbeddingLayer = torch.nn.Embedding(contextLength, outputDimension)\n",
    "val positionalEmbedding = positionalEmbeddingLayer(torch.arange(contextLength))\n",
    "println(s\"Positional embedding shape: ${positionalEmbedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "469038a6-b9b0-4047-b9ad-596d82501835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined embedding shape: torch.Size([8, 4, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mpy.PyQuote\u001b[39m\n",
       "\u001b[36mcombinedEmbedding\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = tensor([[[-1.1378e+00,  1.8008e+00,  4.7484e-01,  ...,  1.3826e+00,\n",
       "          -2.8721e+00, -7.6515e-01],\n",
       "         [-2.1690e+00, -5.3505e-01, -1.3468e+00,  ...,  3.0795e-01,\n",
       "          -1.0110e+00,  1.1464e-01],\n",
       "         [-2.0230e-01,  2.3546e-01, -3.8294e+00,  ...,  1.8550e-01,\n",
       "           1.0620e+00,  2.9659e+00],\n",
       "         [ 1.4467e+00, -1.0005e+00,  9.9284e-01,  ..., -8.5095e-01,\n",
       "           7.1353e-03,  3.0498e-01]],\n",
       "\n",
       "        [[ 1.0937e-01, -2.0513e-03, -3.4637e-01,  ...,  7.5539e-02,\n",
       "          -1.9414e+00, -7.0576e-01],\n",
       "         [ 8.1356e-01, -2.0920e+00, -1.1638e+00,  ...,  3.0826e+00,\n",
       "          -3.0113e-01,  2.4515e+00],\n",
       "         [ 1.0292e+00, -1.7144e+00, -3.3554e+00,  ...,  1.6538e-01,\n",
       "           1.3030e+00,  1.2663e+00],\n",
       "         [-7.7631e-01, -1.4665e+00,  1.8170e+00,  ...,  6.9816e-01,\n",
       "          -1.6878e-01, -1.5186e+00]],\n",
       "\n",
       "        [[-1.0886e+00,  2.1670e+00,  6.5620e-01,  ..., -1.3898e+00,\n",
       "          -2.3450e+00,  9.2697e-01],\n",
       "         [ 1.1529e+00, -8.4855e-02, -9.6567e-01,  ..., -5.8267e-01,\n",
       "          -1.1721e+00,  1.2134e+00],\n",
       "         [-1.5166e+00,  1.3632e+00, -2.2565e+00,  ...,  4.5109e-01,\n",
       "           1.1457e+00,  8.9009e-01],\n",
       "         [-1.3492e+00, -2.4833e+00,  1.2287e+00,  ..., -7.2750e-01,\n",
       "          -1.4896e+00, -9.9070e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-6.1262e-01, -6.8901e-01, -1.5306e+00,  ...,  1.8249e-01,\n",
       "          -1.9235e-01,  2.9629e-02],\n",
       "         [ 1.0109e-01, -1.2521e+00, -1.0771e+00,  ...,  3.6733e-01,\n",
       "          -1.0478e+00,  2.1742e-01],\n",
       "         [ 4.6275e-01,  1.1968e+00, -1.8053e+00,  ...,  2.1023e+00,\n",
       "           3.5738e+00,  1.6851e-01],\n",
       "         [-9.8257e-01, -2.4781e+00,  1.6892e+00,  ...,  9.9858e-01,\n",
       "          -7.2561e-01, -1.0751e+00]],\n",
       "\n",
       "        [[-3.1206e-01,  1.1122e+00,  9.7917e-01,  ..., -2.4930e-01,\n",
       "..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import py.PyQuote\n",
    "\n",
    "val combinedEmbedding = py\"$inputTokensEmbedding + $positionalEmbedding\"\n",
    "println(s\"Combined embedding shape: ${combinedEmbedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303585a3-d5aa-4b20-92d5-b60947b96401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.13.14",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
