{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee7f466c-940b-4615-b173-79c22ad855b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 20479 characters from data/the_verdict.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mscala.io.Source\u001b[39m\n",
       "\u001b[36mfilePath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"data/the_verdict.txt\"\u001b[39m\n",
       "\u001b[36mrawText\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
       "\n",
       "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\n",
       "\n",
       "Well!--even through the prism of Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own trade hardly a murmur. Professional jealousy? Perhaps. If it were, the honour of the craft was vindicated by little Claude Nutley, who, in all good faith, brought out in the Burlington a very handsome \"obituary\" on Jack--one of those showy articles stocked with random technicalities that I have heard (I won't say by whom) compared to Gisburn's painting. And so--his resolve being apparently irrevocable--the discussion gradually died out, and, as Mrs. Thwing had predicted, the price of \"Gisburns\" went up.\n",
       "\n",
       "It was not till three years later that, in the course of a few weeks' idling on the Riviera, it suddenly occurred to me to wonder why Gisburn had given up his painting. On reflection, it really was a tempting problem. To accuse his wife would have been too easy--his fair sitters had been denied the solace of saying that Mrs. Gisburn had \"dragged him down.\" For Mrs. Gisburn--as such--had not existed till nearly a year after Jack's resolve had been taken. It might be that he had married her--since he liked his ease--because he didn't want to go on painting; but it would have been hard to prove that he had given up his painting because he had married her.\n",
       "\n",
       "Of course, if she had not dragged him down, she had equally, as Miss Croft contended, failed to \"lift him up\"--she had not led him back to the easel. To put the\u001b[39m..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.io.Source\n",
    "\n",
    "val filePath = \"data/the_verdict.txt\"\n",
    "val rawText = Source.fromFile(filePath).mkString\n",
    "\n",
    "println(s\"Read ${rawText.length} characters from $filePath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a9979c3-f35c-4e06-ac9a-33e38e46782b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector(Hello, ,, world, ., Is, this, --, a, test, ?)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtokenize\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text: String): Vector[String] = {\n",
    "  val splitBy = \"\"\"[,.:;?_!\"()\\']|--|\\s\"\"\"\n",
    "  text.split(s\"(?<=$splitBy)|(?=$splitBy)\").filter(!_.isBlank).toVector\n",
    "}\n",
    "\n",
    "println(tokenize(\"Hello, world. Is this-- a test?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b94cbbe9-68d1-449a-b8db-1a8f72dc4143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 4690 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtokenizedText\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m\"I\"\u001b[39m,\n",
       "  \u001b[32m\"HAD\"\u001b[39m,\n",
       "  \u001b[32m\"always\"\u001b[39m,\n",
       "  \u001b[32m\"thought\"\u001b[39m,\n",
       "  \u001b[32m\"Jack\"\u001b[39m,\n",
       "  \u001b[32m\"Gisburn\"\u001b[39m,\n",
       "  \u001b[32m\"rather\"\u001b[39m,\n",
       "  \u001b[32m\"a\"\u001b[39m,\n",
       "  \u001b[32m\"cheap\"\u001b[39m,\n",
       "  \u001b[32m\"genius\"\u001b[39m,\n",
       "  \u001b[32m\"--\"\u001b[39m,\n",
       "  \u001b[32m\"though\"\u001b[39m,\n",
       "  \u001b[32m\"a\"\u001b[39m,\n",
       "  \u001b[32m\"good\"\u001b[39m,\n",
       "  \u001b[32m\"fellow\"\u001b[39m,\n",
       "  \u001b[32m\"enough\"\u001b[39m,\n",
       "  \u001b[32m\"--\"\u001b[39m,\n",
       "  \u001b[32m\"so\"\u001b[39m,\n",
       "  \u001b[32m\"it\"\u001b[39m,\n",
       "  \u001b[32m\"was\"\u001b[39m,\n",
       "  \u001b[32m\"no\"\u001b[39m,\n",
       "  \u001b[32m\"great\"\u001b[39m,\n",
       "  \u001b[32m\"surprise\"\u001b[39m,\n",
       "  \u001b[32m\"to\"\u001b[39m,\n",
       "  \u001b[32m\"me\"\u001b[39m,\n",
       "  \u001b[32m\"to\"\u001b[39m,\n",
       "  \u001b[32m\"hear\"\u001b[39m,\n",
       "  \u001b[32m\"that\"\u001b[39m,\n",
       "  \u001b[32m\",\"\u001b[39m,\n",
       "  \u001b[32m\"in\"\u001b[39m,\n",
       "  \u001b[32m\"the\"\u001b[39m,\n",
       "  \u001b[32m\"height\"\u001b[39m,\n",
       "  \u001b[32m\"of\"\u001b[39m,\n",
       "  \u001b[32m\"his\"\u001b[39m,\n",
       "  \u001b[32m\"glory\"\u001b[39m,\n",
       "  \u001b[32m\",\"\u001b[39m,\n",
       "  \u001b[32m\"he\"\u001b[39m,\n",
       "  \u001b[32m\"had\"\u001b[39m,\n",
       "..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizedText = tokenize(rawText)\n",
    "\n",
    "println(s\"Extracted ${tokenizedText.length} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fa3b124-3fc6-4084-b461-bcf9972f1b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130 distinct tokens in total\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msortedDistinctTokens\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m\"!\"\u001b[39m,\n",
       "  \u001b[32m\"\\\"\"\u001b[39m,\n",
       "  \u001b[32m\"'\"\u001b[39m,\n",
       "  \u001b[32m\"(\"\u001b[39m,\n",
       "  \u001b[32m\")\"\u001b[39m,\n",
       "  \u001b[32m\",\"\u001b[39m,\n",
       "  \u001b[32m\"--\"\u001b[39m,\n",
       "  \u001b[32m\".\"\u001b[39m,\n",
       "  \u001b[32m\":\"\u001b[39m,\n",
       "  \u001b[32m\";\"\u001b[39m,\n",
       "  \u001b[32m\"?\"\u001b[39m,\n",
       "  \u001b[32m\"A\"\u001b[39m,\n",
       "  \u001b[32m\"Ah\"\u001b[39m,\n",
       "  \u001b[32m\"Among\"\u001b[39m,\n",
       "  \u001b[32m\"And\"\u001b[39m,\n",
       "  \u001b[32m\"Are\"\u001b[39m,\n",
       "  \u001b[32m\"Arrt\"\u001b[39m,\n",
       "  \u001b[32m\"As\"\u001b[39m,\n",
       "  \u001b[32m\"At\"\u001b[39m,\n",
       "  \u001b[32m\"Be\"\u001b[39m,\n",
       "  \u001b[32m\"Begin\"\u001b[39m,\n",
       "  \u001b[32m\"Burlington\"\u001b[39m,\n",
       "  \u001b[32m\"But\"\u001b[39m,\n",
       "  \u001b[32m\"By\"\u001b[39m,\n",
       "  \u001b[32m\"Carlo\"\u001b[39m,\n",
       "  \u001b[32m\"Chicago\"\u001b[39m,\n",
       "  \u001b[32m\"Claude\"\u001b[39m,\n",
       "  \u001b[32m\"Come\"\u001b[39m,\n",
       "  \u001b[32m\"Croft\"\u001b[39m,\n",
       "  \u001b[32m\"Destroyed\"\u001b[39m,\n",
       "  \u001b[32m\"Devonshire\"\u001b[39m,\n",
       "  \u001b[32m\"Don\"\u001b[39m,\n",
       "  \u001b[32m\"Dubarry\"\u001b[39m,\n",
       "  \u001b[32m\"Emperors\"\u001b[39m,\n",
       "  \u001b[32m\"Florence\"\u001b[39m,\n",
       "  \u001b[32m\"For\"\u001b[39m,\n",
       "  \u001b[32m\"Gallery\"\u001b[39m,\n",
       "  \u001b[32m\"Gideon\"\u001b[39m,\n",
       "..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sortedDistinctTokens = tokenizedText.sorted.distinct\n",
    "\n",
    "println(s\"${sortedDistinctTokens.length} distinct tokens in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9bc07ee-6629-47a2-8406-73fab54aa5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mvocabulary\u001b[39m: \u001b[32mMap\u001b[39m[\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m] = \u001b[33mHashMap\u001b[39m(\n",
       "  \u001b[32m\"inevitable\"\u001b[39m -> \u001b[32m571\u001b[39m,\n",
       "  \u001b[32m\"Monte\"\u001b[39m -> \u001b[32m64\u001b[39m,\n",
       "  \u001b[32m\"down\"\u001b[39m -> \u001b[32m362\u001b[39m,\n",
       "  \u001b[32m\"economy\"\u001b[39m -> \u001b[32m377\u001b[39m,\n",
       "  \u001b[32m\"interesting\"\u001b[39m -> \u001b[32m578\u001b[39m,\n",
       "  \u001b[32m\"luxury\"\u001b[39m -> \u001b[32m652\u001b[39m,\n",
       "  \u001b[32m\"serious\"\u001b[39m -> \u001b[32m870\u001b[39m,\n",
       "  \u001b[32m\"forgotten\"\u001b[39m -> \u001b[32m463\u001b[39m,\n",
       "  \u001b[32m\"muscles\"\u001b[39m -> \u001b[32m695\u001b[39m,\n",
       "  \u001b[32m\"beneath\"\u001b[39m -> \u001b[32m215\u001b[39m,\n",
       "  \u001b[32m\"used\"\u001b[39m -> \u001b[32m1057\u001b[39m,\n",
       "  \u001b[32m\"eye\"\u001b[39m -> \u001b[32m415\u001b[39m,\n",
       "  \u001b[32m\"straining\"\u001b[39m -> \u001b[32m934\u001b[39m,\n",
       "  \u001b[32m\"At\"\u001b[39m -> \u001b[32m18\u001b[39m,\n",
       "  \u001b[32m\"hooded\"\u001b[39m -> \u001b[32m554\u001b[39m,\n",
       "  \u001b[32m\"murmur\"\u001b[39m -> \u001b[32m694\u001b[39m,\n",
       "  \u001b[32m\"adulation\"\u001b[39m -> \u001b[32m133\u001b[39m,\n",
       "  \u001b[32m\"gloried\"\u001b[39m -> \u001b[32m495\u001b[39m,\n",
       "  \u001b[32m\"widow\"\u001b[39m -> \u001b[32m1102\u001b[39m,\n",
       "  \u001b[32m\"panel\"\u001b[39m -> \u001b[32m752\u001b[39m,\n",
       "  \u001b[32m\"sitters\"\u001b[39m -> \u001b[32m898\u001b[39m,\n",
       "  \u001b[32m\"quality\"\u001b[39m -> \u001b[32m808\u001b[39m,\n",
       "  \u001b[32m\"On\"\u001b[39m -> \u001b[32m75\u001b[39m,\n",
       "  \u001b[32m\"Has\"\u001b[39m -> \u001b[32m47\u001b[39m,\n",
       "  \u001b[32m\"secret\"\u001b[39m -> \u001b[32m861\u001b[39m,\n",
       "  \u001b[32m\"Money\"\u001b[39m -> \u001b[32m63\u001b[39m,\n",
       "  \u001b[32m\"ourselves\"\u001b[39m -> \u001b[32m737\u001b[39m,\n",
       "  \u001b[32m\"able\"\u001b[39m -> \u001b[32m117\u001b[39m,\n",
       "  \u001b[32m\"bravura\"\u001b[39m -> \u001b[32m226\u001b[39m,\n",
       "  \u001b[32m\"behind\"\u001b[39m -> \u001b[32m212\u001b[39m,\n",
       "  \u001b[32m\"failure\"\u001b[39m -> \u001b[32m423\u001b[39m,\n",
       "  \u001b[32m\"deprecating\"\u001b[39m -> \u001b[32m327\u001b[39m,\n",
       "  \u001b[32m\"for\"\u001b[39m -> \u001b[32m456\u001b[39m,\n",
       "  \u001b[32m\"canvases\"\u001b[39m -> \u001b[32m246\u001b[39m,\n",
       "  \u001b[32m\"wits\"\u001b[39m -> \u001b[32m1110\u001b[39m,\n",
       "  \u001b[32m\"s\"\u001b[39m -> \u001b[32m850\u001b[39m,\n",
       "  \u001b[32m\"twenty-four\"\u001b[39m -> \u001b[32m1039\u001b[39m,\n",
       "  \u001b[32m\"paint\"\u001b[39m -> \u001b[32m745\u001b[39m,\n",
       "...\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mSimpleTokenizerV1\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vocabulary = sortedDistinctTokens.zipWithIndex.toMap\n",
    "\n",
    "class SimpleTokenizerV1(\n",
    "  vocabulary: Map[String, Int]\n",
    ") {\n",
    "  val inverseVocabulary = vocabulary.map(_.swap)\n",
    "\n",
    "  def encode(text: String): Vector[Int] = \n",
    "    tokenize(text).map(vocabulary(_))\n",
    "\n",
    "  def tokenize(text: String): Vector[String] = {\n",
    "    val splitBy = \"\"\"[,.:;?_!\"()\\']|--|\\s\"\"\"\n",
    "    val tokenizer = s\"(?<=$splitBy)|(?=$splitBy)\"\n",
    "    text.split(tokenizer).filter(!_.isBlank).toVector\n",
    "  }\n",
    "\n",
    "  def decode(ids: Vector[Int]): String = \n",
    "    ids\n",
    "      .map(inverseVocabulary(_))\n",
    "      .mkString(\" \")\n",
    "      .replaceAll(\"\\\\s+([,.?!\\\"()\\'])\", \"$1\") \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c700d5f3-cace-49ca-b5c7-055c07418673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtokenizer\u001b[39m: \u001b[32mSimpleTokenizerV1\u001b[39m = ammonite.$sess.cmd5$Helper$SimpleTokenizerV1@5a3caefb\n",
       "\u001b[36mtextToEncode\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"\"It's the last he painted, you know,\"\n",
       "Mrs. Gisburn said with pardonable pride.\"\"\"\u001b[39m\n",
       "\u001b[36mids\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m1\u001b[39m,\n",
       "  \u001b[32m56\u001b[39m,\n",
       "  \u001b[32m2\u001b[39m,\n",
       "  \u001b[32m850\u001b[39m,\n",
       "  \u001b[32m988\u001b[39m,\n",
       "  \u001b[32m602\u001b[39m,\n",
       "  \u001b[32m533\u001b[39m,\n",
       "  \u001b[32m746\u001b[39m,\n",
       "  \u001b[32m5\u001b[39m,\n",
       "  \u001b[32m1126\u001b[39m,\n",
       "  \u001b[32m596\u001b[39m,\n",
       "  \u001b[32m5\u001b[39m,\n",
       "  \u001b[32m1\u001b[39m,\n",
       "  \u001b[32m67\u001b[39m,\n",
       "  \u001b[32m7\u001b[39m,\n",
       "  \u001b[32m38\u001b[39m,\n",
       "  \u001b[32m851\u001b[39m,\n",
       "  \u001b[32m1108\u001b[39m,\n",
       "  \u001b[32m754\u001b[39m,\n",
       "  \u001b[32m793\u001b[39m,\n",
       "  \u001b[32m7\u001b[39m\n",
       ")\n",
       "\u001b[36mdecodedText\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\\\" It' s the last he painted, you know,\\\" Mrs. Gisburn said with pardonable pride.\"\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = new SimpleTokenizerV1(vocabulary)\n",
    "\n",
    "val textToEncode = \"\"\"\"It's the last he painted, you know,\"\n",
    "Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "val ids = tokenizer.encode(textToEncode)\n",
    "val decodedText = tokenizer.decode(ids)\n",
    "\n",
    "println(decodedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcd775a6-0fa2-4bbf-bcd0-4903f73db1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mendOfText\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"<|endoftext|>\"\u001b[39m\n",
       "\u001b[36mvocabularyWithEndOfText\u001b[39m: \u001b[32mMap\u001b[39m[\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m] = \u001b[33mHashMap\u001b[39m(\n",
       "  \u001b[32m\"inevitable\"\u001b[39m -> \u001b[32m571\u001b[39m,\n",
       "  \u001b[32m\"Monte\"\u001b[39m -> \u001b[32m64\u001b[39m,\n",
       "  \u001b[32m\"down\"\u001b[39m -> \u001b[32m362\u001b[39m,\n",
       "  \u001b[32m\"economy\"\u001b[39m -> \u001b[32m377\u001b[39m,\n",
       "  \u001b[32m\"interesting\"\u001b[39m -> \u001b[32m578\u001b[39m,\n",
       "  \u001b[32m\"luxury\"\u001b[39m -> \u001b[32m652\u001b[39m,\n",
       "  \u001b[32m\"serious\"\u001b[39m -> \u001b[32m870\u001b[39m,\n",
       "  \u001b[32m\"forgotten\"\u001b[39m -> \u001b[32m463\u001b[39m,\n",
       "  \u001b[32m\"muscles\"\u001b[39m -> \u001b[32m695\u001b[39m,\n",
       "  \u001b[32m\"beneath\"\u001b[39m -> \u001b[32m215\u001b[39m,\n",
       "  \u001b[32m\"used\"\u001b[39m -> \u001b[32m1057\u001b[39m,\n",
       "  \u001b[32m\"eye\"\u001b[39m -> \u001b[32m415\u001b[39m,\n",
       "  \u001b[32m\"straining\"\u001b[39m -> \u001b[32m934\u001b[39m,\n",
       "  \u001b[32m\"At\"\u001b[39m -> \u001b[32m18\u001b[39m,\n",
       "  \u001b[32m\"hooded\"\u001b[39m -> \u001b[32m554\u001b[39m,\n",
       "  \u001b[32m\"murmur\"\u001b[39m -> \u001b[32m694\u001b[39m,\n",
       "  \u001b[32m\"adulation\"\u001b[39m -> \u001b[32m133\u001b[39m,\n",
       "  \u001b[32m\"gloried\"\u001b[39m -> \u001b[32m495\u001b[39m,\n",
       "  \u001b[32m\"widow\"\u001b[39m -> \u001b[32m1102\u001b[39m,\n",
       "  \u001b[32m\"panel\"\u001b[39m -> \u001b[32m752\u001b[39m,\n",
       "  \u001b[32m\"sitters\"\u001b[39m -> \u001b[32m898\u001b[39m,\n",
       "  \u001b[32m\"quality\"\u001b[39m -> \u001b[32m808\u001b[39m,\n",
       "  \u001b[32m\"On\"\u001b[39m -> \u001b[32m75\u001b[39m,\n",
       "  \u001b[32m\"Has\"\u001b[39m -> \u001b[32m47\u001b[39m,\n",
       "  \u001b[32m\"secret\"\u001b[39m -> \u001b[32m861\u001b[39m,\n",
       "  \u001b[32m\"Money\"\u001b[39m -> \u001b[32m63\u001b[39m,\n",
       "  \u001b[32m\"ourselves\"\u001b[39m -> \u001b[32m737\u001b[39m,\n",
       "  \u001b[32m\"able\"\u001b[39m -> \u001b[32m117\u001b[39m,\n",
       "  \u001b[32m\"bravura\"\u001b[39m -> \u001b[32m226\u001b[39m,\n",
       "  \u001b[32m\"behind\"\u001b[39m -> \u001b[32m212\u001b[39m,\n",
       "  \u001b[32m\"failure\"\u001b[39m -> \u001b[32m423\u001b[39m,\n",
       "  \u001b[32m\"deprecating\"\u001b[39m -> \u001b[32m327\u001b[39m,\n",
       "  \u001b[32m\"for\"\u001b[39m -> \u001b[32m456\u001b[39m,\n",
       "  \u001b[32m\"canvases\"\u001b[39m -> \u001b[32m246\u001b[39m,\n",
       "  \u001b[32m\"wits\"\u001b[39m -> \u001b[32m1110\u001b[39m,\n",
       "  \u001b[32m\"s\"\u001b[39m -> \u001b[32m850\u001b[39m,\n",
       "  \u001b[32m\"twenty-four\"\u001b[39m -> \u001b[32m1039\u001b[39m,\n",
       "  \u001b[32m\"paint\"\u001b[39m -> \u001b[32m745\u001b[39m,\n",
       "...\n",
       "\u001b[36munknownTokenId\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m-1\u001b[39m\n",
       "\u001b[36munknownToken\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"<|unknown|>\"\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mSimpleTokenizerV2\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val endOfText = \"<|endoftext|>\"\n",
    "val vocabularyWithEndOfText = (sortedDistinctTokens :+ endOfText).zipWithIndex.toMap\n",
    "val (unknownTokenId, unknownToken) = -1 -> \"<|unknown|>\"\n",
    "\n",
    "class SimpleTokenizerV2(\n",
    "  vocabulary: Map[String, Int]\n",
    ") {\n",
    "  val inverseVocabulary = vocabulary.map(_.swap)\n",
    "\n",
    "  def encode(text: String): Vector[Int] = \n",
    "    tokenize(text).map(vocabulary.getOrElse(_, unknownTokenId))\n",
    "\n",
    "  def tokenize(text: String): Vector[String] = {\n",
    "    val splitBy = \"\"\"[,.:;?_!\"()\\']|--|\\s\"\"\"\n",
    "    val tokenizer = s\"(?<=$splitBy)|(?=$splitBy)\"\n",
    "    text.split(tokenizer).filter(!_.isBlank).toVector\n",
    "  }\n",
    "\n",
    "  def decode(ids: Vector[Int]): String = \n",
    "    ids\n",
    "      .map(inverseVocabulary.getOrElse(_, unknownToken))\n",
    "      .mkString(\" \")\n",
    "      .replaceAll(\"\\\\s+([,.?!\\\"()\\'])\", \"$1\") \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e5544b2-7df0-4b2e-aa8b-227ee4973596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unknown|>, do you like tea? <|unknown|> In the sunlit terraces of the <|unknown|>.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtokenizerV2\u001b[39m: \u001b[32mSimpleTokenizerV2\u001b[39m = ammonite.$sess.cmd7$Helper$SimpleTokenizerV2@4c89cbb\n",
       "\u001b[36mconcatenatedText\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\"\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.chaining._\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizerV2 = new SimpleTokenizerV2(vocabulary)\n",
    "\n",
    "val concatenatedText = \"Hello, do you like tea?\" + s\" $endOfText \" + \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "import scala.util.chaining._\n",
    "println(concatenatedText.pipe(tokenizerV2.encode).pipe(tokenizerV2.decode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a21b222f-514e-4c34-a78d-2b1df8ebeab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://repo1.maven.org/maven2/sh/almond/almond-scalapy_2.13/0.14.0-RC15/almond-scalapy_2.13-0.14.0-RC15.pom\n",
      "Downloading https://repo1.maven.org/maven2/dev/scalapy/scalapy-core_2.13/0.5.3/scalapy-core_2.13-0.5.3.pom\n",
      "Downloaded https://repo1.maven.org/maven2/dev/scalapy/scalapy-core_2.13/0.5.3/scalapy-core_2.13-0.5.3.pom\n",
      "Downloaded https://repo1.maven.org/maven2/sh/almond/almond-scalapy_2.13/0.14.0-RC15/almond-scalapy_2.13-0.14.0-RC15.pom\n",
      "Downloading https://repo1.maven.org/maven2/dev/scalapy/scalapy-macros_2.13/0.5.3/scalapy-macros_2.13-0.5.3.pom\n",
      "Downloaded https://repo1.maven.org/maven2/dev/scalapy/scalapy-macros_2.13/0.5.3/scalapy-macros_2.13-0.5.3.pom\n",
      "Downloading https://repo1.maven.org/maven2/dev/scalapy/scalapy-core_2.13/0.5.3/scalapy-core_2.13-0.5.3-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/sh/almond/almond-scalapy_2.13/0.14.0-RC15/almond-scalapy_2.13-0.14.0-RC15.jar\n",
      "Downloading https://repo1.maven.org/maven2/sh/almond/almond-scalapy_2.13/0.14.0-RC15/almond-scalapy_2.13-0.14.0-RC15-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/dev/scalapy/scalapy-macros_2.13/0.5.3/scalapy-macros_2.13-0.5.3-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/dev/scalapy/scalapy-macros_2.13/0.5.3/scalapy-macros_2.13-0.5.3.jar\n",
      "Downloading https://repo1.maven.org/maven2/dev/scalapy/scalapy-core_2.13/0.5.3/scalapy-core_2.13-0.5.3.jar\n",
      "Downloaded https://repo1.maven.org/maven2/dev/scalapy/scalapy-core_2.13/0.5.3/scalapy-core_2.13-0.5.3-sources.jar\n",
      "Downloaded https://repo1.maven.org/maven2/dev/scalapy/scalapy-macros_2.13/0.5.3/scalapy-macros_2.13-0.5.3-sources.jar\n",
      "Downloaded https://repo1.maven.org/maven2/sh/almond/almond-scalapy_2.13/0.14.0-RC15/almond-scalapy_2.13-0.14.0-RC15.jar\n",
      "Downloaded https://repo1.maven.org/maven2/dev/scalapy/scalapy-macros_2.13/0.5.3/scalapy-macros_2.13-0.5.3.jar\n",
      "Downloaded https://repo1.maven.org/maven2/sh/almond/almond-scalapy_2.13/0.14.0-RC15/almond-scalapy_2.13-0.14.0-RC15-sources.jar\n",
      "Downloaded https://repo1.maven.org/maven2/dev/scalapy/scalapy-core_2.13/0.5.3/scalapy-core_2.13-0.5.3.jar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`dev.scalapy::scalapy-core:0.5.3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61f69f6d-354c-45cc-a011-5b217fb906ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mme.shadaj.scalapy.py\u001b[39m\n",
       "\u001b[36mtiktoken\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mModule\u001b[39m = <module 'tiktoken' from '/usr/local/lib/python3.12/site-packages/tiktoken/__init__.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import me.shadaj.scalapy.py\n",
    "\n",
    "val tiktoken = py.module(\"tiktoken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38cf2816-76b3-4efe-838c-c30dd81654f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mpy.SeqConverters\u001b[39m\n",
       "\u001b[36mtiktokenizer\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <Encoding 'gpt2'>\n",
       "\u001b[36mtiktext\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace\"\u001b[39m\n",
       "\u001b[36mallowedSpecial\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = {'<|endoftext|>'}\n",
       "\u001b[36mtiktokens\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m15496\u001b[39m,\n",
       "  \u001b[32m11\u001b[39m,\n",
       "  \u001b[32m466\u001b[39m,\n",
       "  \u001b[32m345\u001b[39m,\n",
       "  \u001b[32m588\u001b[39m,\n",
       "  \u001b[32m8887\u001b[39m,\n",
       "  \u001b[32m30\u001b[39m,\n",
       "  \u001b[32m220\u001b[39m,\n",
       "  \u001b[32m50256\u001b[39m,\n",
       "  \u001b[32m554\u001b[39m,\n",
       "  \u001b[32m262\u001b[39m,\n",
       "  \u001b[32m4252\u001b[39m,\n",
       "  \u001b[32m18250\u001b[39m,\n",
       "  \u001b[32m8812\u001b[39m,\n",
       "  \u001b[32m2114\u001b[39m,\n",
       "  \u001b[32m286\u001b[39m,\n",
       "  \u001b[32m617\u001b[39m,\n",
       "  \u001b[32m34680\u001b[39m,\n",
       "  \u001b[32m27271\u001b[39m\n",
       ")\n",
       "\u001b[36mdecodedTiktokens\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace\"\u001b[39m"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import py.SeqConverters\n",
    "\n",
    "val tiktokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "val tiktext = s\"Hello, do you like tea? $endOfText In the sunlit terraces of someunknownPlace\"\n",
    "val allowedSpecial = py.Dynamic.global.set(Seq(endOfText).toPythonProxy)\n",
    "val tiktokens = tiktokenizer.encode(tiktext, allowed_special = allowedSpecial).as[Vector[Int]]\n",
    "val decodedTiktokens = tiktokenizer.decode(tiktokens.toPythonProxy).as[String]\n",
    "\n",
    "println(decodedTiktokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e8633662-62f7-4276-b36c-e20a8ced8ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Akwirw ier\n",
      "Encoding: \n",
      "  \" \" -> 220\n",
      "  \"Ak\" -> 33901\n",
      "  \"ier\" -> 959\n",
      "  \"ir\" -> 343\n",
      "  \"w\" -> 86\n",
      "Decoded: Akwirw ier\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36munknownWords\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Akwirw ier\"\u001b[39m\n",
       "\u001b[36mencodedUnknownWords\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mVector\u001b[39m(\u001b[32m33901\u001b[39m, \u001b[32m86\u001b[39m, \u001b[32m343\u001b[39m, \u001b[32m86\u001b[39m, \u001b[32m220\u001b[39m, \u001b[32m959\u001b[39m)\n",
       "\u001b[36mencoding\u001b[39m: \u001b[32mMap\u001b[39m[\u001b[32mInt\u001b[39m, \u001b[32mString\u001b[39m] = \u001b[33mHashMap\u001b[39m(\n",
       "  \u001b[32m220\u001b[39m -> \u001b[32m\" \"\u001b[39m,\n",
       "  \u001b[32m33901\u001b[39m -> \u001b[32m\"Ak\"\u001b[39m,\n",
       "  \u001b[32m343\u001b[39m -> \u001b[32m\"ir\"\u001b[39m,\n",
       "  \u001b[32m86\u001b[39m -> \u001b[32m\"w\"\u001b[39m,\n",
       "  \u001b[32m959\u001b[39m -> \u001b[32m\"ier\"\u001b[39m\n",
       ")\n",
       "\u001b[36mdecodedUnknownWords\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Akwirw ier\"\u001b[39m"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Exercise 2.1\n",
    "val unknownWords = \"Akwirw ier\"\n",
    "println(s\"Input: $unknownWords\")\n",
    "\n",
    "val encodedUnknownWords = tiktokenizer.encode(unknownWords).as[Vector[Int]]\n",
    "val encoding = encodedUnknownWords.map(int => int -> tiktokenizer.decode(Seq(int).toPythonProxy).as[String]).toMap\n",
    "\n",
    "println(\"Encoding: \")\n",
    "encoding.toList.sortBy { case (_, subword) => subword }.foreach { case (int, subword) =>\n",
    "  println(s\"  \\\"$subword\\\" -> $int\")\n",
    "}\n",
    "\n",
    "val decodedUnknownWords = tiktokenizer.decode(encodedUnknownWords.toPythonProxy).as[String]\n",
    "println(s\"Decoded: $decodedUnknownWords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f28f09e1-c4fa-4c40-8bf3-0e191b402715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text token count: 5145\n",
      " and --->  established\n",
      " and established --->  himself\n",
      " and established himself --->  in\n",
      " and established himself in --->  a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrawTextTokenized\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m40\u001b[39m,\n",
       "  \u001b[32m367\u001b[39m,\n",
       "  \u001b[32m2885\u001b[39m,\n",
       "  \u001b[32m1464\u001b[39m,\n",
       "  \u001b[32m1807\u001b[39m,\n",
       "  \u001b[32m3619\u001b[39m,\n",
       "  \u001b[32m402\u001b[39m,\n",
       "  \u001b[32m271\u001b[39m,\n",
       "  \u001b[32m10899\u001b[39m,\n",
       "  \u001b[32m2138\u001b[39m,\n",
       "  \u001b[32m257\u001b[39m,\n",
       "  \u001b[32m7026\u001b[39m,\n",
       "  \u001b[32m15632\u001b[39m,\n",
       "  \u001b[32m438\u001b[39m,\n",
       "  \u001b[32m2016\u001b[39m,\n",
       "  \u001b[32m257\u001b[39m,\n",
       "  \u001b[32m922\u001b[39m,\n",
       "  \u001b[32m5891\u001b[39m,\n",
       "  \u001b[32m1576\u001b[39m,\n",
       "  \u001b[32m438\u001b[39m,\n",
       "  \u001b[32m568\u001b[39m,\n",
       "  \u001b[32m340\u001b[39m,\n",
       "  \u001b[32m373\u001b[39m,\n",
       "  \u001b[32m645\u001b[39m,\n",
       "  \u001b[32m1049\u001b[39m,\n",
       "  \u001b[32m5975\u001b[39m,\n",
       "  \u001b[32m284\u001b[39m,\n",
       "  \u001b[32m502\u001b[39m,\n",
       "  \u001b[32m284\u001b[39m,\n",
       "  \u001b[32m3285\u001b[39m,\n",
       "  \u001b[32m326\u001b[39m,\n",
       "  \u001b[32m11\u001b[39m,\n",
       "  \u001b[32m287\u001b[39m,\n",
       "  \u001b[32m262\u001b[39m,\n",
       "  \u001b[32m6001\u001b[39m,\n",
       "  \u001b[32m286\u001b[39m,\n",
       "  \u001b[32m465\u001b[39m,\n",
       "  \u001b[32m13476\u001b[39m,\n",
       "...\n",
       "\u001b[36mrawTextTokensSampled\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m290\u001b[39m,\n",
       "  \u001b[32m4920\u001b[39m,\n",
       "  \u001b[32m2241\u001b[39m,\n",
       "  \u001b[32m287\u001b[39m,\n",
       "  \u001b[32m257\u001b[39m,\n",
       "  \u001b[32m4489\u001b[39m,\n",
       "  \u001b[32m64\u001b[39m,\n",
       "  \u001b[32m319\u001b[39m,\n",
       "  \u001b[32m262\u001b[39m,\n",
       "  \u001b[32m34686\u001b[39m,\n",
       "  \u001b[32m41976\u001b[39m,\n",
       "  \u001b[32m13\u001b[39m,\n",
       "  \u001b[32m357\u001b[39m,\n",
       "  \u001b[32m10915\u001b[39m,\n",
       "  \u001b[32m314\u001b[39m,\n",
       "  \u001b[32m2138\u001b[39m,\n",
       "  \u001b[32m1807\u001b[39m,\n",
       "  \u001b[32m340\u001b[39m,\n",
       "  \u001b[32m561\u001b[39m,\n",
       "  \u001b[32m423\u001b[39m,\n",
       "  \u001b[32m587\u001b[39m,\n",
       "  \u001b[32m10598\u001b[39m,\n",
       "  \u001b[32m393\u001b[39m,\n",
       "  \u001b[32m28537\u001b[39m,\n",
       "  \u001b[32m2014\u001b[39m,\n",
       "  \u001b[32m198\u001b[39m,\n",
       "  \u001b[32m198\u001b[39m,\n",
       "  \u001b[32m1\u001b[39m,\n",
       "  \u001b[32m464\u001b[39m,\n",
       "  \u001b[32m6001\u001b[39m,\n",
       "  \u001b[32m286\u001b[39m,\n",
       "  \u001b[32m465\u001b[39m,\n",
       "  \u001b[32m13476\u001b[39m,\n",
       "  \u001b[32m1\u001b[39m,\n",
       "  \u001b[32m438\u001b[39m,\n",
       "  \u001b[32m5562\u001b[39m,\n",
       "  \u001b[32m373\u001b[39m,\n",
       "  \u001b[32m644\u001b[39m,\n",
       "...\n",
       "\u001b[36mcontextSize\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m4\u001b[39m"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawTextTokenized = tiktokenizer.encode(rawText).as[Vector[Int]]\n",
    "\n",
    "println(s\"Raw text token count: ${rawTextTokenized.length}\")\n",
    "\n",
    "val rawTextTokensSampled = rawTextTokenized.drop(50)\n",
    "\n",
    "val contextSize = 4\n",
    "\n",
    "(1 to contextSize).foreach { size =>\n",
    "  val context = rawTextTokensSampled.take(size)\n",
    "  val desired = rawTextTokensSampled(size)\n",
    "  println(s\"${tiktokenizer.decode(context.toPythonProxy)} ---> ${tiktokenizer.decode(Seq(desired).toPythonProxy)}\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7462d58f-c77a-4a8f-afa2-39c33b391c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mme.shadaj.scalapy.interpreter.CPythonInterpreter\u001b[39m\n",
       "\u001b[36mtorch\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mModule\u001b[39m = <module 'torch' from '/usr/local/lib/python3.12/site-packages/torch/__init__.py'>\n",
       "defined \u001b[32mtype\u001b[39m \u001b[36mTokenizer\u001b[39m\n",
       "defined \u001b[32mtype\u001b[39m \u001b[36mTorchTensor\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mGPTDatasetV1\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcreateDataLoaderV1\u001b[39m"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import me.shadaj.scalapy.interpreter.CPythonInterpreter\n",
    "\n",
    "val torch = py.module(\"torch\")\n",
    "\n",
    "// Workaround to define a class that inherits from a Python class\n",
    "CPythonInterpreter.execManyLines {\n",
    "  s\"\"\"from torch.utils.data import Dataset\n",
    "     |\n",
    "     |class GPTDatasetV1(Dataset):\n",
    "     |  def __init__(self, input_tokens, target_tokens):\n",
    "     |    self.input_tokens = input_tokens\n",
    "     |    self.target_tokens = target_tokens\n",
    "     |  \n",
    "     |  def __len__(self):\n",
    "     |    return len(self.input_tokens)\n",
    "     |\n",
    "     |  def __getitem__(self, index):\n",
    "     |    return self.input_tokens[index], self.target_tokens[index]\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "type Tokenizer = py.Dynamic\n",
    "type TorchTensor = py.Dynamic\n",
    "def GPTDatasetV1(\n",
    "  text: String,\n",
    "  tokenizer: Tokenizer,\n",
    "  maxLength: Int,\n",
    "  step: Int\n",
    "): py.Dynamic = {\n",
    "  val tokens = tokenizer.encode(text).as[Vector[Int]]\n",
    "  val (inputTokens, outputTokens) = (0 until tokens.length by step).foldLeft(\n",
    "    (\n",
    "      Vector.empty[TorchTensor], \n",
    "      Vector.empty[TorchTensor]\n",
    "    )\n",
    "  ) {\n",
    "    case ((inputTokens, outputTokens), i) =>\n",
    "      val inputChunk = tokens.slice(i, i + maxLength)\n",
    "      val outputChunk = tokens.slice(i + 1, i + 1 + maxLength)\n",
    "      (\n",
    "       inputTokens :+ torch.tensor(inputChunk.toPythonProxy), \n",
    "       outputTokens :+ torch.tensor(outputChunk.toPythonProxy)\n",
    "      )\n",
    "  }\n",
    "  py.Dynamic.global.GPTDatasetV1(inputTokens.toPythonProxy, outputTokens.toPythonProxy)\n",
    "}\n",
    "\n",
    "def createDataLoaderV1(\n",
    "  text: String, \n",
    "  batchSize: Int = 4, \n",
    "  maxLength: Int = 256,                       \n",
    "  step: Int = 128, \n",
    "  shuffle: Boolean = true, \n",
    "  dropLast: Boolean = true,\n",
    "  numWorkers: Int = 0 \n",
    "): py.Dynamic = {\n",
    "  val tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "  val dataset = GPTDatasetV1(text, tokenizer, maxLength, step)\n",
    "  torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size = batchSize,\n",
    "    shuffle = shuffle,\n",
    "    drop_last = dropLast,\n",
    "    num_workers = numWorkers\n",
    "  )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9542e984-338d-410f-bc59-386fcf5c0de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[10899,  2138,   257,  7026]]), tensor([[ 2138,   257,  7026, 15632]])]\n",
      "[tensor([[ 2138,   257,  7026, 15632]]), tensor([[  257,  7026, 15632,   438]])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdataLoader\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch.utils.data.dataloader.DataLoader object at 0xfffe87b76990>\n",
       "\u001b[36mdataLoaderIterator\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0xfffe87b6a000>\n",
       "\u001b[36mfirstBatch\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = [tensor([[10899,  2138,   257,  7026]]), tensor([[ 2138,   257,  7026, 15632]])]\n",
       "\u001b[36msecondBatch\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = [tensor([[ 2138,   257,  7026, 15632]]), tensor([[  257,  7026, 15632,   438]])]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataLoader = createDataLoaderV1(\n",
    "  text = rawText, \n",
    "  batchSize = 1, \n",
    "  maxLength = 4, \n",
    "  step = 1, \n",
    "  shuffle = false\n",
    ")\n",
    "val dataLoaderIterator = py.Dynamic.global.iter(dataloader)\n",
    "val firstBatch = py.Dynamic.global.next(dataloaderIterator)\n",
    "val secondBatch = py.Dynamic.global.next(dataloaderIterator)\n",
    "println(firstBatch)\n",
    "println(secondBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e406374c-dd93-46cf-b5e3-05b202d972c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxLength = 2, step = 2\n",
      "[tensor([[  257,  7026, 15632,   438]]), tensor([[ 7026, 15632,   438,  2016]])]\n",
      "[tensor([[ 7026, 15632,   438,  2016]]), tensor([[15632,   438,  2016,   257]])]\n",
      "maxLength = 8, step = 2\n",
      "[tensor([[15632,   438,  2016,   257]]), tensor([[ 438, 2016,  257,  922]])]\n",
      "[tensor([[ 438, 2016,  257,  922]]), tensor([[2016,  257,  922, 5891]])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mprintFirst2Batches\u001b[39m"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Exercise 2.2\n",
    "def printFirst2Batches(maxLength: Int, step: Int) = {\n",
    "  println(s\"maxLength = $maxLength, step = $step\")\n",
    "  val dataLoader = createDataLoaderV1(\n",
    "    text = rawText, \n",
    "    batchSize = 1, \n",
    "    maxLength = maxLength, \n",
    "    step = step, \n",
    "    shuffle = false\n",
    "  )\n",
    "  val dataLoaderIterator = py.Dynamic.global.iter(dataloader)\n",
    "  val firstBatch = py.Dynamic.global.next(dataloaderIterator)\n",
    "  val secondBatch = py.Dynamic.global.next(dataloaderIterator)\n",
    "  println(firstBatch)\n",
    "  println(secondBatch)\n",
    "}\n",
    "\n",
    "printFirst2Batches(maxLength = 2, step = 2)\n",
    "printFirst2Batches(maxLength = 8, step = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5e6214c1-587a-41e2-905b-ddea12dbcf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Outputs:\n",
      "tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mbatchedDataloader\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch.utils.data.dataloader.DataLoader object at 0xfffea3c4fef0>\n",
       "\u001b[36minputTokens\u001b[39m: \u001b[32mTorchTensor\u001b[39m = tensor([[   40,   367,  2885,  1464],\n",
       "        [ 1807,  3619,   402,   271],\n",
       "        [10899,  2138,   257,  7026],\n",
       "        [15632,   438,  2016,   257],\n",
       "        [  922,  5891,  1576,   438],\n",
       "        [  568,   340,   373,   645],\n",
       "        [ 1049,  5975,   284,   502],\n",
       "        [  284,  3285,   326,    11]])\n",
       "\u001b[36moutputTokens\u001b[39m: \u001b[32mTorchTensor\u001b[39m = tensor([[  367,  2885,  1464,  1807],\n",
       "        [ 3619,   402,   271, 10899],\n",
       "        [ 2138,   257,  7026, 15632],\n",
       "        [  438,  2016,   257,   922],\n",
       "        [ 5891,  1576,   438,   568],\n",
       "        [  340,   373,   645,  1049],\n",
       "        [ 5975,   284,   502,   284],\n",
       "        [ 3285,   326,    11,   287]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val batchedDataloader = createDataLoaderV1(\n",
    "  text = rawText, \n",
    "  batchSize = 8, \n",
    "  maxLength = 4, \n",
    "  step = 4, // same as maxLength to prevent overlaps between inputs and outputs\n",
    "  shuffle = false\n",
    ")\n",
    "val Seq(inputTokens, outputTokens) = py.Dynamic.global.next(py.Dynamic.global.iter(batchedDataloader)).as[Seq[TorchTensor]]\n",
    "println(s\"Inputs:\\n$inputTokens\")\n",
    "println(s\"Outputs:\\n$outputTokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cb63a764-1b0c-46f6-a1b1-66eb332573bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch to embed:\n",
      "tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Input embedding shape: torch.Size([8, 4, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mvocabularySize\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m50257\u001b[39m\n",
       "\u001b[36moutputDimension\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m256\u001b[39m\n",
       "\u001b[36mtokenEmbeddingLayer\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = Embedding(50257, 256)\n",
       "\u001b[36mmaxLength\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m4\u001b[39m\n",
       "\u001b[36mdataLoader\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch.utils.data.dataloader.DataLoader object at 0xfffea1079f10>\n",
       "\u001b[36minputTokensBatchToEmbed\u001b[39m: \u001b[32mTorchTensor\u001b[39m = tensor([[   40,   367,  2885,  1464],\n",
       "        [ 1807,  3619,   402,   271],\n",
       "        [10899,  2138,   257,  7026],\n",
       "        [15632,   438,  2016,   257],\n",
       "        [  922,  5891,  1576,   438],\n",
       "        [  568,   340,   373,   645],\n",
       "        [ 1049,  5975,   284,   502],\n",
       "        [  284,  3285,   326,    11]])\n",
       "\u001b[36minputTokensEmbedding\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = tensor([[[ 0.3741,  1.0727, -0.4914,  ..., -0.3521, -0.0961, -0.3009],\n",
       "         [-2.6548,  0.3741, -0.7383,  ..., -0.2487,  0.3779, -1.7325],\n",
       "         [-0.4795,  1.5245, -1.3921,  ...,  2.4053, -0.5496, -1.6613],\n",
       "         [ 0.3048,  0.9452,  0.3642,  ..., -0.2629, -0.2786, -0.6059]],\n",
       "\n",
       "        [[ 0.2343,  0.4025, -0.2537,  ...,  0.1743, -0.8743, -0.0252],\n",
       "         [-0.4101,  0.0729, -0.4906,  ..., -0.5663,  0.2609,  0.6785],\n",
       "         [-1.5919,  0.7826,  0.3554,  ...,  0.9624, -0.5171, -0.2473],\n",
       "         [ 2.5831, -0.4564, -1.1157,  ...,  0.6527,  1.0259,  0.0770]],\n",
       "\n",
       "        [[-0.9834, -2.2190,  1.4152,  ..., -0.8881, -0.0664,  0.1344],\n",
       "         [-0.0633,  0.5807,  0.8776,  ..., -0.0102,  0.0140, -0.0543],\n",
       "         [ 1.1862,  0.5873,  1.8984,  ..., -0.1505,  1.0706, -0.2591],\n",
       "         [ 0.0318,  0.5147, -0.6347,  ...,  0.4426, -0.4129,  0.5425]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.1184, -0.1752,  0.2939,  ..., -1.9547, -1.4505, -2.1072],\n",
       "         [-0.5517,  0.1160,  0.3699,  ...,  0.5899, -0.2720,  1.0691],\n",
       "         [-0.9315, -0.0759, -0.4678,  ..., -0.4622, -0.6760,  0.4843],\n",
       "         [ 0.8108,  0.6518, -0.3573,  ...,  0.4050,  2.1295,  1.0018]],\n",
       "\n",
       "        [[-0.1941, -0.9460, -0.7416,  ...,  0.8120, -0.0528, -0.6916],\n",
       "         [ 0.1794, -0.6539, -0.0927,  ..., -0.3277,  0.0067,  0.4967],\n",
       "         [-1.1177, -1.4036, -1.4595,  ...,  0.6622, -0.3392, -0.0781],\n",
       "         [-0.2128, -0.7701, -0.5707,  ..., -0.6628,  1.3341,  1.1277]],\n",
       "\n",
       "        [[-1.1177, -1.4036, -1.4595,  ...,  0.6622, -0.3392, -0.0781],\n",
       "         [ 1.6261,  0.6651,  0.7827,  ...,  0.1116, -0.5039,  1.3941],\n",
       "         [-0.0404,  1.0439, -0.4968,  ..., -1.1943,  0.1977, -0.0757],\n",
       "         [-1.8073, -0.3629, -1.4246,  ..., -0.8436, -0.2409,  0.1816]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vocabularySize = 50257 // tiktoken vocabulary size\n",
    "val outputDimension = 256\n",
    "val tokenEmbeddingLayer = torch.nn.Embedding(vocabularySize, outputDimension)\n",
    "\n",
    "val maxLength = 4\n",
    "val dataLoader = createDataLoaderV1(\n",
    "  text = rawText, \n",
    "  batchSize = 8, \n",
    "  maxLength = maxLength, \n",
    "  step = maxLength,\n",
    "  shuffle = false\n",
    ")\n",
    "val Seq(inputTokensBatchToEmbed, _) = py.Dynamic.global.next(py.Dynamic.global.iter(dataLoader)).as[Seq[TorchTensor]]\n",
    "println(s\"Input batch to embed:\\n$inputTokensBatchToEmbed\")\n",
    "\n",
    "val inputTokensEmbedding = tokenEmbeddingLayer(inputTokensBatchToEmbed)\n",
    "println(s\"Input embedding shape: ${inputTokensEmbedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "77401586-15ee-4de5-9010-27bfb0caf756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional embedding shape: torch.Size([4, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mcontextLength\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m4\u001b[39m\n",
       "\u001b[36mpositionalEmbeddingLayer\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = Embedding(4, 256)\n",
       "\u001b[36mpositionalEmbedding\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = tensor([[ 0.3819, -0.2297,  0.0915,  ..., -0.0584,  0.0603, -1.3830],\n",
       "        [ 0.3645, -1.3929,  1.6670,  ..., -0.0599, -1.9545, -0.5537],\n",
       "        [ 1.1878, -0.3290,  0.1845,  ...,  1.4459,  0.2684,  0.4107],\n",
       "        [ 1.7994, -0.3165, -0.5567,  ...,  0.3275,  1.0984,  0.0583]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val contextLength = maxLength\n",
    "val positionalEmbeddingLayer = torch.nn.Embedding(contextLength, outputDimension)\n",
    "val positionalEmbedding = positionalEmbeddingLayer(torch.arange(contextLength))\n",
    "println(s\"Positional embedding shape: ${positionalEmbedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "469038a6-b9b0-4047-b9ad-596d82501835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined embedding shape: torch.Size([8, 4, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mpy.PyQuote\u001b[39m\n",
       "\u001b[36mcombinedEmbedding\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = tensor([[[ 0.7559,  0.8430, -0.3999,  ..., -0.4105, -0.0358, -1.6839],\n",
       "         [-2.2902, -1.0188,  0.9288,  ..., -0.3086, -1.5767, -2.2863],\n",
       "         [ 0.7083,  1.1955, -1.2076,  ...,  3.8512, -0.2812, -1.2506],\n",
       "         [ 2.1042,  0.6287, -0.1925,  ...,  0.0645,  0.8198, -0.5476]],\n",
       "\n",
       "        [[ 0.6161,  0.1728, -0.1622,  ...,  0.1159, -0.8140, -1.4082],\n",
       "         [-0.0456, -1.3199,  1.1764,  ..., -0.6262, -1.6936,  0.1247],\n",
       "         [-0.4041,  0.4535,  0.5399,  ...,  2.4083, -0.2487,  0.1634],\n",
       "         [ 4.3825, -0.7730, -1.6724,  ...,  0.9802,  2.1243,  0.1353]],\n",
       "\n",
       "        [[-0.6015, -2.4486,  1.5067,  ..., -0.9465, -0.0062, -1.2486],\n",
       "         [ 0.3013, -0.8122,  2.5446,  ..., -0.0702, -1.9405, -0.6081],\n",
       "         [ 2.3740,  0.2583,  2.0829,  ...,  1.2954,  1.3390,  0.1517],\n",
       "         [ 1.8312,  0.1981, -1.1914,  ...,  0.7700,  0.6855,  0.6008]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.2635, -0.4048,  0.3854,  ..., -2.0131, -1.3902, -3.4902],\n",
       "         [-0.1872, -1.2769,  2.0369,  ...,  0.5300, -2.2265,  0.5154],\n",
       "         [ 0.2563, -0.4049, -0.2833,  ...,  0.9838, -0.4076,  0.8951],\n",
       "         [ 2.6102,  0.3353, -0.9140,  ...,  0.7325,  3.2280,  1.0601]],\n",
       "\n",
       "        [[ 0.1878, -1.1756, -0.6501,  ...,  0.7536,  0.0075, -2.0746],\n",
       "         [ 0.5440, -2.0468,  1.5743,  ..., -0.3876, -1.9478, -0.0571],\n",
       "         [ 0.0701, -1.7326, -1.2750,  ...,  2.1081, -0.0708,  0.3326],\n",
       "         [ 1.5866, -1.0866, -1.1275,  ..., -0.3353,  2.4325,  1.1860]],\n",
       "\n",
       "        [[-0.7358, -1.6333, -1.3680,  ...,  0.6038, -0.2789, -1.4611],\n",
       "         [ 1.9906, -0.7278,  2.4497,  ...,  0.0517, -2.4585,  0.8404],\n",
       "         [ 1.1474,  0.7149, -0.3123,  ...,  0.2516,  0.4661,  0.3350],\n",
       "         [-0.0079, -0.6795, -1.9814,  ..., -0.5161,  0.8575,  0.2399]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import py.PyQuote\n",
    "\n",
    "val combinedEmbedding = py\"$inputTokensEmbedding + $positionalEmbedding\"\n",
    "println(s\"Combined embedding shape: ${combinedEmbedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303585a3-d5aa-4b20-92d5-b60947b96401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.13.14",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
