{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43422675-d20a-48f9-b6fd-de7c6ff15e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling /workspace/Magic.sc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $file.^.Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e49e87a-5c99-462f-b5a0-cff300073cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.4.* in /usr/local/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (75.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->torch==2.4.*) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/site-packages (from sympy->torch==2.4.*) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "Magic.!(\"pip\", \"install\", \"torch==2.4.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf5b2722-e6a8-4de4-a1cc-7babf95c0cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`dev.scalapy::scalapy-core:0.5.3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "020af021-e9d2-4b9d-a0fe-9e5d89e8ecd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mme.shadaj.scalapy.py\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mpy.SeqConverters\u001b[39m\n",
       "\u001b[36mtorch\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mModule\u001b[39m = <module 'torch' from '/usr/local/lib/python3.12/site-packages/torch/__init__.py'>\n",
       "\u001b[36minputs\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import me.shadaj.scalapy.py\n",
    "import py.SeqConverters\n",
    "\n",
    "val torch = py.module(\"torch\")\n",
    "\n",
    "val inputs = torch.tensor(\n",
    "  Seq(\n",
    "    Seq(0.43, 0.15, 0.89), // word1\n",
    "    Seq(0.55, 0.87, 0.66), // word2\n",
    "    Seq(0.57, 0.85, 0.64), // word3\n",
    "    Seq(0.22, 0.58, 0.33), // word4\n",
    "    Seq(0.77, 0.25, 0.10), // word5\n",
    "    Seq(0.05, 0.80, 0.55)  // word6\n",
    "  ).toPythonProxy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6227db2f-bd46-4436-ad67-dafd35a25c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mpy.PyQuote\u001b[39m\n",
       "\u001b[36mattentionScores\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import py.PyQuote\n",
    "\n",
    "val attentionScores = py\"$inputs @ $inputs.T\"\n",
    "println(attentionScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ad86a2e-f634-4b62-9d87-46225fd8d656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mattentionWeights\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val attentionWeights = torch.softmax(attentionScores, dim = -1)\n",
    "println(attentionWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52b98dfb-d6de-4dc4-95ba-7f357624871a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mcontextVectors\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val contextVectors = py\"$attentionWeights @ $inputs\"\n",
    "println(contextVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bce060a7-571a-4bb0-a9f9-d8fbbf2502ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mtype\u001b[39m \u001b[36mTorchTensor\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mSelfAttentionV1\u001b[39m"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class SelfAttention(nn.Module):\n",
    "     |  def __init__(self, forward_method):\n",
    "     |    super().__init__()\n",
    "     |    self.forward_method = forward_method\n",
    "     |  \n",
    "     |  def forward(self, inputs):\n",
    "     |    return self.forward_method(inputs)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "type TorchTensor = py.Dynamic\n",
    "def SelfAttentionV1(\n",
    "  inputDimension: Int,\n",
    "  outputDimension: Int\n",
    "): py.Dynamic = {\n",
    "  val weightsQuery = torch.nn.Parameter(torch.rand(inputDimension, outputDimension))\n",
    "  val weightsKey = torch.nn.Parameter(torch.rand(inputDimension, outputDimension))\n",
    "  val weightsValue = torch.nn.Parameter(torch.rand(inputDimension, outputDimension))\n",
    "  val forwardMethod = (inputs: TorchTensor) => {\n",
    "    val queries = py\"$inputs @ $weightsQuery\"\n",
    "    val keys = py\"$inputs @ $weightsKey\"\n",
    "    val values = py\"$inputs @ $weightsValue\"\n",
    "    val attentionScores = py\"$queries @ $keys.T\"\n",
    "    val attentionWeights = torch.softmax(py\"$attentionScores / $outputDimension**0.5\", dim = -1)\n",
    "    py\"$attentionWeights @ $values\"\n",
    "  }\n",
    "  py.Dynamic.global.SelfAttention(forwardMethod)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48f9887e-2dcf-46a9-bbb5-0918e26aa9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres33_0\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch._C.Generator object at 0xfffeb3d798f0>\n",
       "\u001b[36mselfAttentionV1\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = SelfAttention()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "val selfAttentionV1 = SelfAttentionV1(inputDimension = inputs.shape.bracketAccess(1).as[Int], outputDimension = 2)\n",
    "println(selfAttentionV1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75391d89-b663-4ff6-9946-3e28e9f340d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mSelfAttentionV2\u001b[39m"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def SelfAttentionV2(\n",
    "  inputDimension: Int,\n",
    "  outputDimension: Int\n",
    "): py.Dynamic = {\n",
    "  val weightsQuery = torch.nn.Linear(inputDimension, outputDimension, bias = false)\n",
    "  val weightsKey = torch.nn.Linear(inputDimension, outputDimension, bias = false)\n",
    "  val weightsValue = torch.nn.Linear(inputDimension, outputDimension, bias = false)\n",
    "  val forwardMethod = (inputs: TorchTensor) => {\n",
    "    val queries = weightsQuery(inputs)\n",
    "    val keys = weightsKey(inputs)\n",
    "    val values = weightsValue(inputs)\n",
    "    val attentionScores = py\"$queries @ $keys.T\"\n",
    "    val attentionWeights = torch.softmax(py\"$attentionScores / $outputDimension**0.5\", dim = -1)\n",
    "    py\"$attentionWeights @ $values\"\n",
    "  }\n",
    "  py.Dynamic.global.SelfAttention(forwardMethod)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a9e8578-2cd4-4c2f-adc0-064fbb0849e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres35_0\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch._C.Generator object at 0xfffeb3d798f0>\n",
       "\u001b[36mselfAttentionV2\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = SelfAttention()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "val selfAttentionV2 = SelfAttentionV2(inputDimension = inputs.shape.bracketAccess(1).as[Int], outputDimension = 2)\n",
    "println(selfAttentionV2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "19173e51-51f8-4afd-b467-152c4cffd9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mtype\u001b[39m \u001b[36mMask\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mCasualAttention\u001b[39m"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class CasualAttention(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "type Mask = py.Dynamic\n",
    "def CasualAttention(\n",
    "  inputDimension: Int,\n",
    "  outputDimension: Int,\n",
    "  dropoutProbability: Double,\n",
    "  contextLength: Int\n",
    "): py.Dynamic = {\n",
    "  val weightsQuery = torch.nn.Linear(inputDimension, outputDimension, bias = false)\n",
    "  val weightsKey = torch.nn.Linear(inputDimension, outputDimension, bias = false)\n",
    "  val weightsValue = torch.nn.Linear(inputDimension, outputDimension, bias = false)\n",
    "  val dropout = torch.nn.Dropout(dropoutProbability)\n",
    "\n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.register_buffer(\"mask\", torch.triu(torch.ones(contextLength, contextLength), diagonal = 1))\n",
    "    val mask = self.mask\n",
    "      \n",
    "    val forward = (batchedInputs: TorchTensor) => {\n",
    "      val queries = weightsQuery(batchedInputs)\n",
    "      val keys = weightsKey(batchedInputs)\n",
    "      val values = weightsValue(batchedInputs)\n",
    "      val attentionScores = py\"$queries @ $keys.transpose(1, 2)\"\n",
    "      val tokensCount = batchedInputs.shape.bracketAccess(1).as[Int]\n",
    "      attentionScores.masked_fill_(py\"$mask.bool()[:$tokensCount, :$tokensCount]\", -torch.inf)\n",
    "      val attentionWeights = dropout(torch.softmax(py\"$attentionScores / $outputDimension**0.5\", dim = -1))\n",
    "      py\"$attentionWeights @ $values\"\n",
    "    }\n",
    "    py.Dynamic.global.CasualAttention.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.CasualAttention(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4511edab-1add-458e-a9eb-1f4d12834da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchedInputs.shape = torch.Size([2, 6, 3])\n",
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres63_0\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch._C.Generator object at 0xfffeb3d798f0>\n",
       "\u001b[36mbatchedInputs\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = tensor([[[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]],\n",
       "\n",
       "        [[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]]])\n",
       "\u001b[36mcontextLength\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m6\u001b[39m\n",
       "\u001b[36mcasualAttention\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = CasualAttention()"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "val batchedInputs = torch.stack((inputs, inputs), dim = 0)\n",
    "println(s\"batchedInputs.shape = ${batchedInputs.shape}\")\n",
    "val contextLength = batchedInputs.shape.bracketAccess(1).as[Int]\n",
    "val casualAttention = CasualAttention(\n",
    "  inputDimension = batchedInputs.shape.bracketAccess(2).as[Int], \n",
    "  outputDimension = 2, \n",
    "  dropoutProbability = 0.0, \n",
    "  contextLength\n",
    ")\n",
    "println(casualAttention(batchedInputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f5e126df-32c9-49ff-8e6a-d7f422c45fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mMultiHeadAttentionWrapper\u001b[39m"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class MultiHeadAttentionWrapper(nn.Module):\n",
    "     |  def __init__(self):\n",
    "     |    super().__init__()\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def MultiHeadAttentionWrapper(\n",
    "  inputDimension: Int,\n",
    "  outputDimension: Int,\n",
    "  dropoutProbability: Double,\n",
    "  contextLength: Int,\n",
    "  headsCount: Int\n",
    "): py.Dynamic = {\n",
    "  val heads = torch.nn.ModuleList(\n",
    "    Seq.fill(headsCount)(\n",
    "      CasualAttention(inputDimension, outputDimension, dropoutProbability, contextLength)\n",
    "    ).toPythonProxy\n",
    "  )\n",
    "  val forward = (batchedInputs: TorchTensor) => {\n",
    "    torch.cat(py\"[head($batchedInputs) for head in $heads]\", dim = -1)\n",
    "  }\n",
    "  py.Dynamic.global.MultiHeadAttentionWrapper.forward = forward\n",
    "  py.Dynamic.global.MultiHeadAttentionWrapper()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "23fb5fb6-3611-4a03-9091-dff181079d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4772, 0.1063, 0.4772, 0.1063],\n",
      "         [0.5891, 0.3257, 0.5891, 0.3257],\n",
      "         [0.6202, 0.3860, 0.6202, 0.3860],\n",
      "         [0.5478, 0.3589, 0.5478, 0.3589],\n",
      "         [0.5321, 0.3428, 0.5321, 0.3428],\n",
      "         [0.5077, 0.3493, 0.5077, 0.3493]],\n",
      "\n",
      "        [[0.4772, 0.1063, 0.4772, 0.1063],\n",
      "         [0.5891, 0.3257, 0.5891, 0.3257],\n",
      "         [0.6202, 0.3860, 0.6202, 0.3860],\n",
      "         [0.5478, 0.3589, 0.5478, 0.3589],\n",
      "         [0.5321, 0.3428, 0.5321, 0.3428],\n",
      "         [0.5077, 0.3493, 0.5077, 0.3493]]], grad_fn=<CatBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres71_0\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch._C.Generator object at 0xfffeb3d798f0>\n",
       "\u001b[36mmultiHeadAttentionWrapper\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = MultiHeadAttentionWrapper()"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "val multiHeadAttentionWrapper = MultiHeadAttentionWrapper(\n",
    "  inputDimension = batchedInputs.shape.bracketAccess(2).as[Int], \n",
    "  outputDimension = 2, \n",
    "  dropoutProbability = 0.0, \n",
    "  contextLength,\n",
    "  headsCount = 2\n",
    ")\n",
    "println(multiHeadAttentionWrapper(batchedInputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f4d8f07c-d249-47a0-ab99-bce5dce8c926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2216,  0.2216],\n",
      "         [ 0.0155,  0.0155],\n",
      "         [-0.0546, -0.0546],\n",
      "         [-0.0817, -0.0817],\n",
      "         [-0.0957, -0.0957],\n",
      "         [-0.1065, -0.1065]],\n",
      "\n",
      "        [[ 0.2216,  0.2216],\n",
      "         [ 0.0155,  0.0155],\n",
      "         [-0.0546, -0.0546],\n",
      "         [-0.0817, -0.0817],\n",
      "         [-0.0957, -0.0957],\n",
      "         [-0.1065, -0.1065]]], grad_fn=<CatBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres72_0\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch._C.Generator object at 0xfffeb3d798f0>\n",
       "\u001b[36mmultiHeadAttentionWrapper2\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = MultiHeadAttentionWrapper()"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Exercise 3.2\n",
    "torch.manual_seed(123)\n",
    "val multiHeadAttentionWrapper2 = MultiHeadAttentionWrapper(\n",
    "  inputDimension = batchedInputs.shape.bracketAccess(2).as[Int], \n",
    "  outputDimension = 1, \n",
    "  dropoutProbability = 0.0, \n",
    "  contextLength,\n",
    "  headsCount = 2\n",
    ")\n",
    "println(multiHeadAttentionWrapper2(batchedInputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2e70e624-8a6f-4178-b005-4f449034f4c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mtype\u001b[39m \u001b[36mMask\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mMultiHeadAttention\u001b[39m"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class MultiHeadAttention(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "type Mask = py.Dynamic\n",
    "def MultiHeadAttention(\n",
    "  inputDimension: Int,\n",
    "  outputDimension: Int,\n",
    "  dropoutProbability: Double,\n",
    "  contextLength: Int,\n",
    "  headsCount: Int\n",
    "): py.Dynamic = {\n",
    "  assert(outputDimension % headsCount == 0, \"Output dimension must be a multiple of heads count\")\n",
    "  val headDimension = outputDimension / headsCount\n",
    "    \n",
    "  val weightsQuery = torch.nn.Linear(inputDimension, outputDimension, bias = false)\n",
    "  val weightsKey = torch.nn.Linear(inputDimension, outputDimension, bias = false)\n",
    "  val weightsValue = torch.nn.Linear(inputDimension, outputDimension, bias = false)\n",
    "  val outputProjection = torch.nn.Linear(outputDimension, outputDimension)\n",
    "  val dropout = torch.nn.Dropout(dropoutProbability)\n",
    "    \n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.register_buffer(\"mask\", torch.triu(torch.ones(contextLength, contextLength), diagonal = 1))\n",
    "    val mask = self.mask\n",
    "      \n",
    "    val forward = (batchedInputs: TorchTensor) => {\n",
    "      val (batchesCount, tokensCount, tokenDimension) = batchedInputs.shape.as[(Int, Int, Int)]\n",
    "      val queries = weightsQuery(batchedInputs)\n",
    "        .view(batchesCount, tokensCount, headsCount, headDimension)\n",
    "        .transpose(1, 2)\n",
    "      val keys = weightsKey(batchedInputs)\n",
    "        .view(batchesCount, tokensCount, headsCount, headDimension)\n",
    "        .transpose(1, 2)\n",
    "      val values = weightsValue(batchedInputs)\n",
    "        .view(batchesCount, tokensCount, headsCount, headDimension)\n",
    "        .transpose(1, 2)\n",
    "      val attentionScores = py\"$queries @ $keys.transpose(2, 3)\"\n",
    "      attentionScores.masked_fill_(py\"$mask.bool()[:$tokensCount, :$tokensCount]\", -torch.inf)\n",
    "      val attentionWeights = dropout(torch.softmax(py\"$attentionScores / $outputDimension**0.5\", dim = -1))\n",
    "      outputProjection(\n",
    "        py\"$attentionWeights @ $values\"\n",
    "          .transpose(1, 2)\n",
    "          .contiguous()\n",
    "          .view(batchesCount, tokensCount, outputDimension)\n",
    "      )\n",
    "    }\n",
    "    py.Dynamic.global.CasualAttention.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.CasualAttention(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7502b604-3bbd-4127-ba66-1916c00897c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2941, 0.3906],\n",
      "         [0.2854, 0.3600],\n",
      "         [0.2692, 0.3879],\n",
      "         [0.2636, 0.3935],\n",
      "         [0.2574, 0.4033]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2941, 0.3906],\n",
      "         [0.2854, 0.3600],\n",
      "         [0.2692, 0.3879],\n",
      "         [0.2636, 0.3935],\n",
      "         [0.2574, 0.4033]]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres74_0\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch._C.Generator object at 0xfffeb3d798f0>\n",
       "\u001b[36mmultiHeadAttention\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = CasualAttention()"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "val multiHeadAttention = MultiHeadAttention(\n",
    "  inputDimension = batchedInputs.shape.bracketAccess(2).as[Int], \n",
    "  outputDimension = 2, \n",
    "  dropoutProbability = 0.0,\n",
    "  contextLength,\n",
    "  headsCount = 2\n",
    ")\n",
    "println(multiHeadAttention(batchedInputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445c8b22-c6aa-4b07-92c4-83b276687540",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.13.14",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
