{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dcbe069-f199-45af-99a0-57bfa2df6e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $file.^.Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd9bd9ae-61c1-403a-9871-fb89a11647bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.4.* in /usr/local/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (75.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->torch==2.4.*) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/site-packages (from sympy->torch==2.4.*) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken==0.7.* in /usr/local/lib/python3.12/site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/site-packages (from tiktoken==0.7.*) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/site-packages (from tiktoken==0.7.*) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.7.*) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.7.*) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.7.*) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.7.*) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "Magic.!(\"pip\", \"install\", \"torch==2.4.*\")\n",
    "Magic.!(\"pip\", \"install\", \"tiktoken==0.7.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34306985-2652-45df-b0ce-bd3c1a880e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mme.shadaj.scalapy.py\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mpy.SeqConverters\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mpy.PyQuote\u001b[39m\n",
       "\u001b[36mtorch\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mModule\u001b[39m = <module 'torch' from '/usr/local/lib/python3.12/site-packages/torch/__init__.py'>\n",
       "\u001b[36mtiktoken\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mModule\u001b[39m = <module 'tiktoken' from '/usr/local/lib/python3.12/site-packages/tiktoken/__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`dev.scalapy::scalapy-core:0.5.3`\n",
    "\n",
    "import me.shadaj.scalapy.py\n",
    "import py.SeqConverters\n",
    "import py.PyQuote\n",
    "\n",
    "val torch = py.module(\"torch\")\n",
    "val tiktoken = py.module(\"tiktoken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "359a8e96-546e-476e-b635-3ade29735b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mGPTConfig\u001b[39m\n",
       "\u001b[36mgptConfig\u001b[39m: \u001b[32mGPTConfig\u001b[39m = \u001b[33mGPTConfig\u001b[39m(\n",
       "  vocabularySize = \u001b[32m50257\u001b[39m,\n",
       "  contextLength = \u001b[32m256\u001b[39m,\n",
       "  embeddingDimension = \u001b[32m768\u001b[39m,\n",
       "  attentionHeadsCount = \u001b[32m12\u001b[39m,\n",
       "  layersCount = \u001b[32m12\u001b[39m,\n",
       "  dropoutRate = \u001b[32m0.1\u001b[39m,\n",
       "  queryKeyValueBias = \u001b[32mfalse\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class GPTConfig(\n",
    "  vocabularySize: Int,\n",
    "  contextLength: Int,\n",
    "  embeddingDimension: Int,\n",
    "  attentionHeadsCount: Int,\n",
    "  layersCount: Int,\n",
    "  dropoutRate: Double,\n",
    "  queryKeyValueBias: Boolean\n",
    ")\n",
    "\n",
    "val gptConfig = GPTConfig(\n",
    "  vocabularySize = 50_257,\n",
    "  contextLength = 256,\n",
    "  embeddingDimension = 768,\n",
    "  attentionHeadsCount = 12,\n",
    "  layersCount = 12,\n",
    "  dropoutRate = 0.1,\n",
    "  queryKeyValueBias = false\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30b2b4e1-5ddb-44a2-9acc-a304ce5c0e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mtype\u001b[39m \u001b[36mTorchTensor\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mMultiHeadAttention\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class MultiHeadAttention(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "type TorchTensor = py.Dynamic\n",
    "def MultiHeadAttention(\n",
    "  inputDimension: Int,\n",
    "  outputDimension: Int,\n",
    "  dropoutProbability: Double,\n",
    "  contextLength: Int,\n",
    "  headsCount: Int,\n",
    "  queryKeyValueBias: Boolean\n",
    "): py.Dynamic = {\n",
    "  assert(outputDimension % headsCount == 0, \"Output dimension must be a multiple of heads count\")\n",
    "  val headDimension = outputDimension / headsCount\n",
    "    \n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.weightsQuery = torch.nn.Linear(inputDimension, outputDimension, bias = queryKeyValueBias)\n",
    "    self.weightsKey = torch.nn.Linear(inputDimension, outputDimension, bias = queryKeyValueBias)\n",
    "    self.weightsValue = torch.nn.Linear(inputDimension, outputDimension, bias = queryKeyValueBias)\n",
    "    self.outputProjection = torch.nn.Linear(outputDimension, outputDimension)\n",
    "    self.dropout = torch.nn.Dropout(dropoutProbability)\n",
    "    self.register_buffer(\"mask\", torch.triu(torch.ones(contextLength, contextLength), diagonal = 1))\n",
    "      \n",
    "    val forward = (batchedInputs: TorchTensor) => {\n",
    "      val (batchesCount, tokensCount, tokenDimension) = batchedInputs.shape.as[(Int, Int, Int)]\n",
    "      val queries = self.weightsQuery(batchedInputs)\n",
    "        .view(batchesCount, tokensCount, headsCount, headDimension)\n",
    "        .transpose(1, 2)\n",
    "      val keys = self.weightsKey(batchedInputs)\n",
    "        .view(batchesCount, tokensCount, headsCount, headDimension)\n",
    "        .transpose(1, 2)\n",
    "      val values = self.weightsValue(batchedInputs)\n",
    "        .view(batchesCount, tokensCount, headsCount, headDimension)\n",
    "        .transpose(1, 2)\n",
    "      val attentionScores = py\"$queries @ $keys.transpose(2, 3)\"\n",
    "      attentionScores.masked_fill_(py\"${self.mask}.bool()[:$tokensCount, :$tokensCount]\", -torch.inf)\n",
    "      val attentionWeights = self.dropout(torch.softmax(py\"$attentionScores / $headDimension**0.5\", dim = -1))\n",
    "      self.outputProjection(\n",
    "        py\"$attentionWeights @ $values\"\n",
    "          .transpose(1, 2)\n",
    "          .contiguous()\n",
    "          .view(batchesCount, tokensCount, outputDimension)\n",
    "      )\n",
    "    }\n",
    "    self.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.MultiHeadAttention(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a52671-21de-47de-87a0-4894d7c2a0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mGELU\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "// Because it mostly uses Python operators, it's implemented fully in Python\n",
    "py.exec {\n",
    "  s\"\"\"import torch\n",
    "     |import torch.nn as nn\n",
    "     |\n",
    "     |class GELU(nn.Module):\n",
    "     |  def __init__(self):\n",
    "     |    super().__init__()\n",
    "     |\n",
    "     |  def forward(self, inputs):\n",
    "     |    return 0.5 * inputs * (\n",
    "     |      1 + torch.tanh(\n",
    "     |        torch.sqrt(torch.tensor(2.0 / torch.pi)) * (inputs + 0.044715 * torch.pow(inputs, 3))\n",
    "     |      )\n",
    "     |    )\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def GELU() = py.Dynamic.global.GELU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7bb1c31-b837-4a87-acb5-fda694b34cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mFeedForward\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class FeedForward(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def FeedForward(\n",
    "  embeddingDimension: Int\n",
    "): py.Dynamic = {\n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.layers = torch.nn.Sequential(\n",
    "      torch.nn.Linear(embeddingDimension, 4 * embeddingDimension),\n",
    "      GELU(),\n",
    "      torch.nn.Linear(4 * embeddingDimension, embeddingDimension)\n",
    "    )\n",
    "      \n",
    "    val forward = (inputs: TorchTensor) => self.layers(inputs)\n",
    "    self.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.FeedForward(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a379f7b-a8e9-42a3-99c6-7fa0dc23faab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mNormalizationLayer\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class NormalizationLayer(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def NormalizationLayer(\n",
    "  embeddingDimension: Int\n",
    "): py.Dynamic = {\n",
    "  val epsilon = 1e-5\n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.scale = torch.nn.Parameter(torch.ones(embeddingDimension))\n",
    "    self.shift = torch.nn.Parameter(torch.zeros(embeddingDimension))\n",
    "      \n",
    "    val forward = (inputs: TorchTensor) => {\n",
    "      val mean = inputs.mean(dim = -1, keepdim = true)\n",
    "      val variance = inputs.`var`(dim = -1, keepdim = true, unbiased = false)\n",
    "      val normalizedInputs = py\"($inputs - $mean) / torch.sqrt($variance + $epsilon)\"\n",
    "      py\"${self.scale} * $normalizedInputs + ${self.shift}\"\n",
    "    }\n",
    "    self.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.NormalizationLayer(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d3fab4f-b15a-4795-a3a6-a3f9e4546839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.chaining._\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mTransformerBlock\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.chaining._\n",
    "\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class TransformerBlock(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def TransformerBlock(\n",
    "  config: GPTConfig\n",
    "): py.Dynamic = {\n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.multiHeadAttention = MultiHeadAttention(\n",
    "      inputDimension = config.embeddingDimension,\n",
    "      outputDimension = config.embeddingDimension,\n",
    "      dropoutProbability = config.dropoutRate,\n",
    "      contextLength = config.contextLength,\n",
    "      headsCount = config.attentionHeadsCount,\n",
    "      queryKeyValueBias = config.queryKeyValueBias\n",
    "    )\n",
    "    self.feedForward = FeedForward(config.embeddingDimension)\n",
    "    self.normalization1 = NormalizationLayer(config.embeddingDimension)\n",
    "    self.normalization2 = NormalizationLayer(config.embeddingDimension)\n",
    "    self.dropoutShortcut = torch.nn.Dropout(config.dropoutRate)\n",
    "    \n",
    "    val forward = (inputs: TorchTensor) => {\n",
    "      val shortcut = inputs\n",
    "      val newShortcut = inputs\n",
    "        .pipe(self.normalization1(_))\n",
    "        .pipe(self.multiHeadAttention(_))\n",
    "        .pipe(self.dropoutShortcut(_))\n",
    "        .pipe(o => py\"$o + $shortcut\")\n",
    "      newShortcut\n",
    "        .pipe(self.normalization2(_))\n",
    "        .pipe(self.feedForward(_))\n",
    "        .pipe(self.dropoutShortcut(_))\n",
    "        .pipe(o => py\"$o + $newShortcut\")\n",
    "    }\n",
    "    self.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.TransformerBlock(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f56aad56-185d-49ff-a49c-37f029462671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mtype\u001b[39m \u001b[36mModel\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mGPTModel\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class GPTModel(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "type Model = py.Dynamic\n",
    "def GPTModel(\n",
    "  config: GPTConfig\n",
    "): Model = {\n",
    "  val transformerBlocks = Seq.fill(config.layersCount)(TransformerBlock(config))\n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.tokenEmbeddingLayer = torch.nn.Embedding(config.vocabularySize, config.embeddingDimension)\n",
    "    self.positionEmbeddingLayer = torch.nn.Embedding(config.contextLength, config.embeddingDimension)\n",
    "    self.dropoutEmbeddingLayer = torch.nn.Dropout(config.dropoutRate)\n",
    "    self.transformerBlocksLayer = py\"nn.Sequential(*${transformerBlocks.toPythonProxy})\"\n",
    "    self.finalNormalizationLayer = NormalizationLayer(config.embeddingDimension)\n",
    "    self.outputLayer = torch.nn.Linear(config.embeddingDimension, config.vocabularySize, bias = false)\n",
    "      \n",
    "    val forward = (batchedInputs: TorchTensor) => {\n",
    "      val (_, sequenceLength) = batchedInputs.shape.as[(Int, Int)]\n",
    "      val tokenEmbeddings = self.tokenEmbeddingLayer(batchedInputs)\n",
    "      val positionEmbeddings = self.positionEmbeddingLayer(torch.arange(sequenceLength, device = batchedInputs.device))\n",
    "      py\"$tokenEmbeddings + $positionEmbeddings\"\n",
    "        .pipe(self.dropoutEmbeddingLayer(_))\n",
    "        .pipe(self.transformerBlocksLayer(_))\n",
    "        .pipe(self.finalNormalizationLayer(_))\n",
    "        .pipe(self.outputLayer(_))\n",
    "    }\n",
    "    self.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.GPTModel(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb87a3fa-e035-468c-83c2-6bcb13572f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 162419712\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres11_0\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch._C.Generator object at 0xffff64b4a7b0>\n",
       "\u001b[36mmodel\u001b[39m: \u001b[32mModel\u001b[39m = GPTModel(\n",
       "  (tokenEmbeddingLayer): Embedding(50257, 768)\n",
       "  (positionEmbeddingLayer): Embedding(256, 768)\n",
       "  (dropoutEmbeddingLayer): Dropout(p=0.1, inplace=False)\n",
       "  (transformerBlocksLayer): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (normalization1): NormalizationLayer()\n",
       "      (normalization2): NormalizationLayer()\n",
       "      (dropoutShortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "...\n",
       "\u001b[36mres11_3\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = GPTModel(\n",
       "  (tokenEmbeddingLayer): Embedding(50257, 768)\n",
       "  (positionEmbeddingLayer): Embedding(256, 768)\n",
       "  (dropoutEmbeddingLayer): Dropout(p=0.1, inplace=False)\n",
       "  (transformerBlocksLayer): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (normalization1): NormalizationLayer()\n",
       "      (normalization2): NormalizationLayer()\n",
       "      (dropoutShortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "val model = GPTModel(gptConfig)\n",
    "println(s\"Total trainable parameters: ${py\"sum(p.numel() for p in $model.parameters())\"}\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1044c6be-1d1c-43d6-8c03-573e6b67b7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mgenerateTextSimple\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generateTextSimple(\n",
    "  model: Model,\n",
    "  maxNewTokens: Int,\n",
    "  contextLength: Int\n",
    ")(\n",
    "  encodedInput: TorchTensor\n",
    "): TorchTensor =\n",
    "  LazyList.iterate(encodedInput) { currentEncodedOutput =>\n",
    "    val croppedInput = py\"$currentEncodedOutput[:, -$contextLength:]\"\n",
    "    val logits = py.`with`(torch.no_grad()) { _ =>\n",
    "      model(croppedInput)\n",
    "    }\n",
    "    py\"$logits[:, -1, :]\"\n",
    "      .pipe(torch.softmax(_, dim = -1))\n",
    "      .pipe(torch.argmax(_, dim = -1, keepdim = true))\n",
    "      .pipe(nextEncodedOutput => torch.cat((currentEncodedOutput, nextEncodedOutput), dim = 1))\n",
    "  }.drop(maxNewTokens).head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e439f782-3376-4acb-91d7-097b75bae34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mtype\u001b[39m \u001b[36mTokenizer\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtextToTokenIds\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtokenIdsToText\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type Tokenizer = py.Dynamic\n",
    "def textToTokenIds(\n",
    "  text: String, \n",
    "  tokenizer: Tokenizer\n",
    "): TorchTensor = {\n",
    "  val allowedSpecial = py.Dynamic.global.set(Seq(\"<|endoftext|>\").toPythonProxy)\n",
    "  val encodedText = tokenizer.encode(text, allowed_special = allowedSpecial)\n",
    "  torch.tensor(encodedText).unsqueeze(0)\n",
    "}\n",
    "    \n",
    "def tokenIdsToText(\n",
    "  tokenIds: TorchTensor, \n",
    "  tokenizer: Tokenizer\n",
    "): String =\n",
    "  tokenizer.decode(tokenIds.squeeze(0).tolist()).as[String]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8be564df-4a19-4374-87cd-0a6b0a74d6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text: Every effort moves you impressed Arafootball beggreetings prev1980 reproduction Bang Whis\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtokenizer\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <Encoding 'gpt2'>\n",
       "\u001b[36mexampleText\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Every effort moves you\"\u001b[39m\n",
       "\u001b[36moutputTextIds\u001b[39m: \u001b[32mTorchTensor\u001b[39m = tensor([[ 6109,  3626,  6100,   345, 12617, 30574, 15914, 44887, 46648,  8654,\n",
       "         23664, 20728,  9801, 28424]])\n",
       "\u001b[36mdecodedOutputText\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Every effort moves you impressed Arafootball beggreetings prev1980 reproduction Bang Whis\"\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "val exampleText = \"Every effort moves you\"\n",
    "val outputTextIds = generateTextSimple(\n",
    "  model = model, \n",
    "  maxNewTokens = 10, \n",
    "  contextLength = gptConfig.contextLength\n",
    ")(\n",
    "  encodedInput = textToTokenIds(exampleText, tokenizer)\n",
    ")\n",
    "val decodedOutputText = tokenIdsToText(outputTextIds, tokenizer)\n",
    "println(s\"Output text: $decodedOutputText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa57b54b-f143-4435-900d-63e2e938455d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mscala.io.Source\u001b[39m\n",
       "\u001b[36mfilePath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"data/the_verdict.txt\"\u001b[39m\n",
       "\u001b[36mtextData\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
       "\n",
       "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\n",
       "\n",
       "Well!--even through the prism of Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own trade hardly a murmur. Professional jealousy? Perhaps. If it were, the honour of the craft was vindicated by little Claude Nutley, who, in all good faith, brought out in the Burlington a very handsome \"obituary\" on Jack--one of those showy articles stocked with random technicalities that I have heard (I won't say by whom) compared to Gisburn's painting. And so--his resolve being apparently irrevocable--the discussion gradually died out, and, as Mrs. Thwing had predicted, the price of \"Gisburns\" went up.\n",
       "\n",
       "It was not till three years later that, in the course of a few weeks' idling on the Riviera, it suddenly occurred to me to wonder why Gisburn had given up his painting. On reflection, it really was a tempting problem. To accuse his wife would have been too easy--his fair sitters had been denied the solace of saying that Mrs. Gisburn had \"dragged him down.\" For Mrs. Gisburn--as such--had not existed till nearly a year after Jack's resolve had been taken. It might be that he had married her--since he liked his ease--because he didn't want to go on painting; but it would have been hard to prove that he had given up his painting because he had married her.\n",
       "\n",
       "Of course, if she had not dragged him down, she had equally, as Miss Croft contended, failed to \"lift him up\"--she had not led him back to the easel. To put the\u001b[39m..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.io.Source\n",
    "\n",
    "val filePath = \"data/the_verdict.txt\"\n",
    "val textData = Source.fromFile(filePath).mkString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ba4355d-7766-428d-8085-54b343663c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtotalCharacters\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m20479\u001b[39m\n",
       "\u001b[36mtotalTokens\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m5145\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val totalCharacters = textData.length\n",
    "val totalTokens = tokenizer.encode(textData).as[Vector[Int]].length\n",
    "println(s\"Characters: $totalCharacters\")\n",
    "println(s\"Tokens: $totalTokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfecd895-25e9-487d-b30b-eef2c466c33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 18431\n",
      "Validation data size: 2048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainingRatio\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.9\u001b[39m\n",
       "\u001b[36msplitIndex\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m18431\u001b[39m\n",
       "\u001b[36mtrainingData\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
       "\n",
       "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\n",
       "\n",
       "Well!--even through the prism of Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own trade hardly a murmur. Professional jealousy? Perhaps. If it were, the honour of the craft was vindicated by little Claude Nutley, who, in all good faith, brought out in the Burlington a very handsome \"obituary\" on Jack--one of those showy articles stocked with random technicalities that I have heard (I won't say by whom) compared to Gisburn's painting. And so--his resolve being apparently irrevocable--the discussion gradually died out, and, as Mrs. Thwing had predicted, the price of \"Gisburns\" went up.\n",
       "\n",
       "It was not till three years later that, in the course of a few weeks' idling on the Riviera, it suddenly occurred to me to wonder why Gisburn had given up his painting. On reflection, it really was a tempting problem. To accuse his wife would have been too easy--his fair sitters had been denied the solace of saying that Mrs. Gisburn had \"dragged him down.\" For Mrs. Gisburn--as such--had not existed till nearly a year after Jack's resolve had been taken. It might be that he had married her--since he liked his ease--because he didn't want to go on painting; but it would have been hard to prove that he had given up his painting because he had married her.\n",
       "\n",
       "Of course, if she had not dragged him down, she had equally, as Miss Croft contended, failed to \"lift him up\"--she had not led him back to the easel. To put the\u001b[39m...\n",
       "\u001b[36mvalidationData\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"ue' collapsed like a house of cards. He didn't sneer, you understand, poor Stroud--he just lay there quietly watching, and on his lips, through the gray beard, I seemed to hear the question: 'Are you sure you know where you're coming out?'\n",
       "\n",
       "\"If I could have painted that face, with that question on it, I should have done a great thing. The next greatest thing was to see that I couldn't--and that grace was given me. But, oh, at that minute, Rickham, was there anything on earth I wouldn't have given to have Stroud alive before me, and to hear him say: 'It's not too late--I'll show you how'?\n",
       "\n",
       "\"It _was_ too late--it would have been, even if he'd been alive. I packed up my traps, and went down and told Mrs. Stroud. Of course I didn't tell her _that_--it would have been Greek to her. I simply said I couldn't paint him, that I was too moved. She rather liked the idea--she's so romantic! It was that that made her give me the donkey. But she was terribly upset at not getting the portrait--she did so want him 'done' by some one showy! At first I was afraid she wouldn't let me off--and at my wits' end I suggested Grindle. Yes, it was I who started Grindle: I told Mrs. Stroud he was the 'coming' man, and she told somebody else, and so it got to be true. . . . And he painted Stroud without wincing; and she hung the picture among her husband's things. . . .\"\n",
       "\n",
       "He flung himself down in the arm-chair near mine, laid back his head, and clasping his arms beneath it, looked up at the picture above the chimney-piece.\n",
       "\n",
       "\"I like to fancy that Stroud himself would have given it to me, if he'd been able to say what he thought that day.\"\n",
       "\n",
       "And, in answer to a question I put half-mechanically--\"Begin again?\" he flashed out. \"When the one thing that brings me anywhere near him is that I knew enough to leave off?\"\n",
       "\n",
       "He stood up and laid his hand on my shoulder with a laugh. \"Only the irony of it is that I _am_ still painting--since Grindle's doing it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\"\"\"\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainingRatio = 0.9\n",
    "val splitIndex = (totalCharacters * trainingRatio).toInt\n",
    "val (trainingData, validationData) = textData.splitAt(splitIndex)\n",
    "println(s\"Training data size: ${trainingData.size}\")\n",
    "println(s\"Validation data size: ${validationData.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e39f143b-4789-4a0e-b4ae-126ab1a745c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mGPTDatasetV1\u001b[39m\n",
       "defined \u001b[32mtype\u001b[39m \u001b[36mDataLoader\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcreateDataLoaderV1\u001b[39m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"from torch.utils.data import Dataset\n",
    "     |\n",
    "     |class GPTDatasetV1(Dataset):\n",
    "     |  def __init__(self, len, getItem):\n",
    "     |    self.len = len\n",
    "     |    self.getItem = getItem\n",
    "     |  \n",
    "     |  def __len__(self):\n",
    "     |    return self.len()\n",
    "     |\n",
    "     |  def __getitem__(self, index):\n",
    "     |    return self.getItem(index)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def GPTDatasetV1(\n",
    "  text: String,\n",
    "  tokenizer: Tokenizer,\n",
    "  maxLength: Int,\n",
    "  step: Int\n",
    "): py.Dynamic = {\n",
    "  val tokens = tokenizer.encode(text).as[Vector[Int]]\n",
    "  val (inputTokens, outputTokens) = (0 until (tokens.length - maxLength) by step).foldLeft(\n",
    "    (\n",
    "      Vector.empty[TorchTensor], \n",
    "      Vector.empty[TorchTensor]\n",
    "    )\n",
    "  ) {\n",
    "    case ((inputTokens, outputTokens), i) =>\n",
    "      val inputChunk = tokens.slice(i, i + maxLength)\n",
    "      val outputChunk = tokens.slice(i + 1, i + 1 + maxLength)\n",
    "      (\n",
    "       inputTokens :+ torch.tensor(inputChunk.toPythonProxy), \n",
    "       outputTokens :+ torch.tensor(outputChunk.toPythonProxy)\n",
    "      )\n",
    "  }\n",
    "  val len = () => inputTokens.length\n",
    "  val getItem = (index: Int) => (inputTokens(index), outputTokens(index))\n",
    "  py.Dynamic.global.GPTDatasetV1(len, getItem)\n",
    "}\n",
    "\n",
    "type DataLoader = py.Dynamic\n",
    "def createDataLoaderV1(\n",
    "  text: String, \n",
    "  batchSize: Int = 4,\n",
    "  maxLength: Int = 256,                       \n",
    "  step: Int = 128, \n",
    "  shuffle: Boolean = true, \n",
    "  dropLast: Boolean = true,\n",
    "  numWorkers: Int = 0 \n",
    "): DataLoader = {\n",
    "  val tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "  val dataset = GPTDatasetV1(text, tokenizer, maxLength, step)\n",
    "  torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size = batchSize,\n",
    "    shuffle = shuffle,\n",
    "    drop_last = dropLast,\n",
    "    num_workers = numWorkers\n",
    "  )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86347c1c-d8f1-4d39-8eea-57341bd1ae0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loader batches\n",
      "Batch 0: torch.Size([2, 256]) -> torch.Size([2, 256])\n",
      "Batch 1: torch.Size([2, 256]) -> torch.Size([2, 256])\n",
      "Batch 2: torch.Size([2, 256]) -> torch.Size([2, 256])\n",
      "Batch 3: torch.Size([2, 256]) -> torch.Size([2, 256])\n",
      "Batch 4: torch.Size([2, 256]) -> torch.Size([2, 256])\n",
      "Batch 5: torch.Size([2, 256]) -> torch.Size([2, 256])\n",
      "Batch 6: torch.Size([2, 256]) -> torch.Size([2, 256])\n",
      "Batch 7: torch.Size([2, 256]) -> torch.Size([2, 256])\n",
      "Batch 8: torch.Size([2, 256]) -> torch.Size([2, 256])\n",
      "\n",
      "Validation loader batches\n",
      "Batch 0: torch.Size([2, 256]) -> torch.Size([2, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainingLoader\u001b[39m: \u001b[32mDataLoader\u001b[39m = <torch.utils.data.dataloader.DataLoader object at 0xffff3f735ca0>\n",
       "\u001b[36mvalidationLoader\u001b[39m: \u001b[32mDataLoader\u001b[39m = <torch.utils.data.dataloader.DataLoader object at 0xffff3f711d30>\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mprintBatchShapes\u001b[39m"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainingLoader = createDataLoaderV1(\n",
    "  text = trainingData, \n",
    "  batchSize  = 2, \n",
    "  maxLength = gptConfig.contextLength,                       \n",
    "  step = gptConfig.contextLength, \n",
    "  shuffle = true, \n",
    "  dropLast = true,\n",
    "  numWorkers = 0 \n",
    ")\n",
    "val validationLoader = createDataLoaderV1(\n",
    "  text = validationData, \n",
    "  batchSize  = 2, \n",
    "  maxLength = gptConfig.contextLength,                       \n",
    "  step = gptConfig.contextLength, \n",
    "  shuffle = false, \n",
    "  dropLast = false,\n",
    "  numWorkers = 0 \n",
    ")\n",
    "\n",
    "def printBatchShapes(loader: DataLoader) = {\n",
    "  val loaderBatches = py\"[batch for batch in $loader]\".as[Seq[Seq[TorchTensor]]]\n",
    "  loaderBatches.zipWithIndex.foreach { case (Seq(inputBatch, targetBatch), i) =>\n",
    "    println(s\"Batch $i: ${inputBatch.shape} -> ${targetBatch.shape}\")\n",
    "  }\n",
    "}\n",
    "println(\"Training loader batches\")\n",
    "printBatchShapes(trainingLoader)\n",
    "println()\n",
    "println(\"Validation loader batches\")\n",
    "printBatchShapes(validationLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0335006a-0818-4028-8a2d-e654fc7ea85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mtype\u001b[39m \u001b[36mDevice\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcalculateBatchLoss\u001b[39m"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type Device = py.Dynamic\n",
    "def calculateBatchLoss(\n",
    "  model: Model,\n",
    "  device: Device\n",
    ")(\n",
    "  inputBatch: TorchTensor,\n",
    "  targetBatch: TorchTensor\n",
    "): TorchTensor = {\n",
    "  val outputs = model(inputBatch.to(device))\n",
    "  torch.nn.functional.cross_entropy(outputs.flatten(0, 1), targetBatch.flatten())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02576724-8ae5-442b-9e00-cfc700eb5590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcalculateDataLoaderLoss\u001b[39m"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculateDataLoaderLoss(\n",
    "  model: Model,\n",
    "  device: Device\n",
    ")(\n",
    "  dataLoader: DataLoader,\n",
    "  batchesCountOpt: Option[Int] = None\n",
    "): Double = { \n",
    "  val batchesCount = batchesCountOpt match {\n",
    "    case Some(batchesCount) => batchesCount\n",
    "    case None => py\"len($dataLoader)\".as[Int]\n",
    "  }\n",
    "  assert(batchesCount > 0, \"There were no batches to process\")\n",
    "  var totalLoss = 0.0\n",
    "  val batchesIterator = py\"iter($dataLoader)\"\n",
    "  var currentBatch = py\"next($batchesIterator, None)\"\n",
    "  var currentBatchIndex = 0\n",
    "  while (\n",
    "    currentBatchIndex < batchesCount && \n",
    "    currentBatch != py.Dynamic.global.None\n",
    "  ) {\n",
    "    py.local {\n",
    "      val Seq(inputBatch, targetBatch) = currentBatch.as[Seq[TorchTensor]]\n",
    "      totalLoss += calculateBatchLoss(model, device)(inputBatch, targetBatch).item().as[Double]\n",
    "    }\n",
    "    currentBatch = py\"next($batchesIterator, None)\"\n",
    "    currentBatchIndex += 1\n",
    "  }\n",
    "  totalLoss / batchesCount\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f82c0d33-96c4-46b5-9447-4c7645bb67f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating training loss...\n",
      "Training loss: 11.009115431043837\n",
      "Calculating validation loss...\n",
      "Validation loss: 11.044095993041992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdevice\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = cpu\n",
       "\u001b[36mres22_1\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = GPTModel(\n",
       "  (tokenEmbeddingLayer): Embedding(50257, 768)\n",
       "  (positionEmbeddingLayer): Embedding(256, 768)\n",
       "  (dropoutEmbeddingLayer): Dropout(p=0.1, inplace=False)\n",
       "  (transformerBlocksLayer): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (normalization1): NormalizationLayer()\n",
       "      (normalization2): NormalizationLayer()\n",
       "      (dropoutShortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val device = torch.device(if (torch.cuda.is_available().as[Boolean]) \"cuda\" else \"cpu\")\n",
    "model.to(device)\n",
    "py.`with`(torch.no_grad()) { _ =>\n",
    "  println(s\"Calculating training loss...\")\n",
    "  val trainingLoss = calculateDataLoaderLoss(model, device)(trainingLoader)\n",
    "  println(s\"Training loss: $trainingLoss\")\n",
    "  println(s\"Calculating validation loss...\")\n",
    "  val validationLoss = calculateDataLoaderLoss(model, device)(validationLoader)\n",
    "  println(s\"Validation loss: $validationLoss\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f40f2c2d-6ec1-483f-aafa-0607cdd7cf17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mLoss\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mTrainingStep\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mModelEvaluator\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mSampleGenerator\u001b[39m\n",
       "defined \u001b[32mtype\u001b[39m \u001b[36mOptimizer\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtrainModelSimple\u001b[39m"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Loss(\n",
    "  trainingLoss: Double,\n",
    "  validationLoss: Double\n",
    ")\n",
    "\n",
    "case class TrainingStep(\n",
    "  loss: Loss,\n",
    "  tokensSeen: Long\n",
    ")\n",
    "\n",
    "class ModelEvaluator(\n",
    "  device: Device,\n",
    "  trainingLoader: DataLoader,\n",
    "  validationLoader: DataLoader,\n",
    "  evaluationEpochsCount: Int,\n",
    "  evaluationFrequencySteps: Int\n",
    ") {\n",
    "\n",
    "  def evaluateCond(currentStep: Int)(model: Model): Option[Loss] =\n",
    "    Option.when(currentStep % evaluationFrequencySteps == 0) {\n",
    "      println(s\"Step $currentStep\")\n",
    "      evaluate(model)\n",
    "    }\n",
    "    \n",
    "  def evaluate(model: Model): Loss = {\n",
    "    model.eval()\n",
    "    py.`with`(torch.no_grad()) { _ =>\n",
    "      val trainingLoss = calculateDataLoaderLoss(model, device)(trainingLoader, batchesCountOpt = Some(evaluationEpochsCount))\n",
    "      val validationLoss = calculateDataLoaderLoss(model, device)(validationLoader, batchesCountOpt = Some(evaluationEpochsCount))\n",
    "      println(\n",
    "        s\"\"\"- training loss: $trainingLoss\n",
    "           |- validation loss: $validationLoss\"\"\".stripMargin\n",
    "      )\n",
    "      model.train()\n",
    "      Loss(trainingLoss, validationLoss)\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "class SampleGenerator(\n",
    "  device: Device,\n",
    "  tokenizer: Tokenizer,\n",
    "  text: String,\n",
    "  contextLength: Int\n",
    ") {\n",
    "\n",
    "  def generateAndPrintSample(model: Model): Unit = {\n",
    "    model.eval()\n",
    "    val encodedText = textToTokenIds(text, tokenizer)\n",
    "    py.`with`(torch.no_grad()) { _ =>\n",
    "      val tokenIds = generateTextSimple(\n",
    "        model = model, \n",
    "        maxNewTokens = 50, \n",
    "        contextLength = contextLength\n",
    "      )(encodedText)\n",
    "      val decodedText = tokenIdsToText(tokenIds, tokenizer).replace(\"\\n\", \" \")\n",
    "      println(s\"Sample text for '$text':\\n'$decodedText'\")\n",
    "      model.train()\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "type Optimizer = py.Dynamic\n",
    "def trainModelSimple(\n",
    "  model: Model,\n",
    "  device: Device,\n",
    "  trainingLoader: DataLoader,\n",
    "  validationLoader: DataLoader,\n",
    "  optimizer: Optimizer,\n",
    "  epochsCount: Int,\n",
    "  modelEvaluator: ModelEvaluator,\n",
    "  sampleGenerator: SampleGenerator\n",
    "): List[TrainingStep] = {\n",
    "  var stepsCount = 0\n",
    "  var tokensSeen = 0L\n",
    "  val losses =\n",
    "    for {\n",
    "      epoch <- 1 to epochsCount\n",
    "    } yield py.local {\n",
    "      println(s\"=> Epoch $epoch\")\n",
    "      model.train()\n",
    "      val batchesIterator = py\"iter($trainingLoader)\"\n",
    "      val losses = LazyList\n",
    "        .continually(py\"next($batchesIterator, None)\")\n",
    "        .takeWhile(_ != py.Dynamic.global.None)\n",
    "        .map(_.as[Seq[TorchTensor]])\n",
    "        .map { batch =>\n",
    "          py.local {\n",
    "            val Seq(inputBatch, targetBatch) = batch\n",
    "            optimizer.zero_grad()\n",
    "            val loss = calculateBatchLoss(model, device)(inputBatch, targetBatch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            stepsCount += 1\n",
    "            tokensSeen += inputBatch.numel().as[Long]\n",
    "            modelEvaluator.evaluateCond(stepsCount)(model).map { loss =>\n",
    "              TrainingStep(loss, tokensSeen)\n",
    "            }\n",
    "          }\n",
    "        }.flatten.toList\n",
    "      sampleGenerator.generateAndPrintSample(model)\n",
    "      losses\n",
    "    }\n",
    "  losses.toList.flatten\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efd69a4a-de29-41d7-a6cf-d55480d28b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres24_0\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch._C.Generator object at 0xffff64b4a7b0>\n",
       "\u001b[36mmodel\u001b[39m: \u001b[32mModel\u001b[39m = GPTModel(\n",
       "  (tokenEmbeddingLayer): Embedding(50257, 768)\n",
       "  (positionEmbeddingLayer): Embedding(256, 768)\n",
       "  (dropoutEmbeddingLayer): Dropout(p=0.1, inplace=False)\n",
       "  (transformerBlocksLayer): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (normalization1): NormalizationLayer()\n",
       "      (normalization2): NormalizationLayer()\n",
       "      (dropoutShortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "...\n",
       "\u001b[36mres24_2\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = GPTModel(\n",
       "  (tokenEmbeddingLayer): Embedding(50257, 768)\n",
       "  (positionEmbeddingLayer): Embedding(256, 768)\n",
       "  (dropoutEmbeddingLayer): Dropout(p=0.1, inplace=False)\n",
       "  (transformerBlocksLayer): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (normalization1): NormalizationLayer()\n",
       "      (normalization2): NormalizationLayer()\n",
       "      (dropoutShortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "...\n",
       "\u001b[36moptimizer\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.0004\n",
       "    maximize: False\n",
       "    weight_decay: 0.1\n",
       ")\n",
       "\u001b[36mepochsCount\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m10\u001b[39m\n",
       "\u001b[36mmodelEvaluator\u001b[39m: \u001b[32mModelEvaluator\u001b[39m = ammonite.$sess.cmd23$Helper$ModelEvaluator@16d3a7f8\n",
       "\u001b[36msampleGenerator\u001b[39m: \u001b[32mSampleGenerator\u001b[39m = ammonite.$sess.cmd23$Helper$SampleGenerator@c7d8159"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "val model = GPTModel(gptConfig)\n",
    "model.to(device)\n",
    "val optimizer = torch.optim.AdamW(model.parameters(), lr = 0.0004, weight_decay = 0.1)\n",
    "val epochsCount = 10\n",
    "val modelEvaluator = new ModelEvaluator(device, trainingLoader, validationLoader, evaluationEpochsCount = 5, evaluationFrequencySteps = 5)\n",
    "val sampleGenerator = new SampleGenerator(device, tokenizer, text = \"Every effort moves you\", contextLength = gptConfig.contextLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5764a795-d757-4c40-8f65-56f298d228c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch 1\n",
      "Step 5\n",
      "- training loss: 8.465807342529297\n",
      "- validation loss: 1.7353384017944335\n",
      "Sample text for 'Every effort moves you':\n",
      "'Every effort moves you,,,,,,,,,,,,,,.                                   '\n",
      "=> Epoch 2\n",
      "Step 10\n",
      "- training loss: 7.041613388061523\n",
      "- validation loss: 1.451545238494873\n",
      "Step 15\n",
      "- training loss: 6.1582635879516605\n",
      "- validation loss: 1.326794147491455\n",
      "Sample text for 'Every effort moves you':\n",
      "'Every effort moves you, and,,,,,,,,,, and, and,,,, and,,, and,, and,, and,,,, and,, and,,,,, and,,,,, and'\n",
      "=> Epoch 3\n",
      "Step 20\n",
      "- training loss: 5.621779441833496\n",
      "- validation loss: 1.3087495803833007\n",
      "Step 25\n",
      "- training loss: 5.312926006317139\n",
      "- validation loss: 1.277871799468994\n",
      "Sample text for 'Every effort moves you':\n",
      "'Every effort moves you know to           \"I, and he was, and, and I had the his, and I had the the of the, and I had been, and he was, and, and I had'\n",
      "=> Epoch 4\n",
      "Step 30\n",
      "- training loss: 4.527096509933472\n",
      "- validation loss: 1.2614811897277831\n",
      "Step 35\n",
      "- training loss: 3.955209732055664\n",
      "- validation loss: 1.2633899688720702\n",
      "Sample text for 'Every effort moves you':\n",
      "'Every effort moves you know.    \".     \"Oh, and he had been the picture.    \"Oh, he had the picture--and.   \"Oh, and he had been the fact.'\n",
      "=> Epoch 5\n",
      "Step 40\n",
      "- training loss: 3.2153188228607177\n",
      "- validation loss: 1.2380556106567382\n",
      "Step 45\n",
      "- training loss: 2.982280135154724\n",
      "- validation loss: 1.237734317779541\n",
      "Sample text for 'Every effort moves you':\n",
      "'Every effort moves you know to see it--Ied by a little, and Mrs.  \"I looked up, and to me, the to see a little, and his pictures--I looked up, I had the donkey, and were, I had been'\n",
      "=> Epoch 6\n",
      "Step 50\n",
      "- training loss: 2.508208131790161\n",
      "- validation loss: 1.245980453491211\n",
      "Sample text for 'Every effort moves you':\n",
      "'Every effort moves you?\" I was not that he had been the to the fact with a little: \"Yes--and by me!\" \"--and I was his he had the _rose Dubarry_ his painting, and I had been the room, with his'\n",
      "=> Epoch 7\n",
      "Step 55\n",
      "- training loss: 1.866141676902771\n",
      "- validation loss: 1.2473424911499023\n",
      "Step 60\n",
      "- training loss: 1.523775839805603\n",
      "- validation loss: 1.2566779136657715\n",
      "Sample text for 'Every effort moves you':\n",
      "'Every effort moves you?\"  \"Yes--I glanced after him, and uncertain.  \"I looked up the fact, and I felt to see a smile behind his close grayish beard--as if he had the donkey. \"There were days when I'\n",
      "=> Epoch 8\n",
      "Step 65\n",
      "- training loss: 1.039507508277893\n",
      "- validation loss: 1.2677906036376954\n",
      "Step 70\n",
      "- training loss: 0.6775010704994202\n",
      "- validation loss: 1.277448558807373\n",
      "Sample text for 'Every effort moves you':\n",
      "'Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his glory, he had again. I had the donkey. \"There were days when I'\n",
      "=> Epoch 9\n",
      "Step 75\n",
      "- training loss: 0.6307951152324677\n",
      "- validation loss: 1.2843694686889648\n",
      "Step 80\n",
      "- training loss: 0.40227590799331664\n",
      "- validation loss: 1.306808090209961\n",
      "Sample text for 'Every effort moves you':\n",
      "'Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I'\n",
      "=> Epoch 10\n",
      "Step 85\n",
      "- training loss: 0.3334305465221405\n",
      "- validation loss: 1.3093819618225098\n",
      "Step 90\n",
      "- training loss: 0.19166191816329955\n",
      "- validation loss: 1.3363248825073242\n",
      "Sample text for 'Every effort moves you':\n",
      "'Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainingSteps\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mTrainingStep\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[33mTrainingStep\u001b[39m(\n",
       "    loss = \u001b[33mLoss\u001b[39m(\n",
       "      trainingLoss = \u001b[32m8.465807342529297\u001b[39m,\n",
       "      validationLoss = \u001b[32m1.7353384017944335\u001b[39m\n",
       "    ),\n",
       "    tokensSeen = \u001b[32m2560L\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mTrainingStep\u001b[39m(\n",
       "    loss = \u001b[33mLoss\u001b[39m(\n",
       "      trainingLoss = \u001b[32m7.041613388061523\u001b[39m,\n",
       "      validationLoss = \u001b[32m1.451545238494873\u001b[39m\n",
       "    ),\n",
       "    tokensSeen = \u001b[32m5120L\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mTrainingStep\u001b[39m(\n",
       "    loss = \u001b[33mLoss\u001b[39m(\n",
       "      trainingLoss = \u001b[32m6.1582635879516605\u001b[39m,\n",
       "      validationLoss = \u001b[32m1.326794147491455\u001b[39m\n",
       "    ),\n",
       "    tokensSeen = \u001b[32m7680L\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mTrainingStep\u001b[39m(\n",
       "    loss = \u001b[33mLoss\u001b[39m(\n",
       "      trainingLoss = \u001b[32m5.621779441833496\u001b[39m,\n",
       "      validationLoss = \u001b[32m1.3087495803833007\u001b[39m\n",
       "    ),\n",
       "    tokensSeen = \u001b[32m10240L\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mTrainingStep\u001b[39m(\n",
       "    loss = \u001b[33mLoss\u001b[39m(\n",
       "      trainingLoss = \u001b[32m5.312926006317139\u001b[39m,\n",
       "      validationLoss = \u001b[32m1.277871799468994\u001b[39m\n",
       "    ),\n",
       "    tokensSeen = \u001b[32m12800L\u001b[39m\n",
       "  ),\n",
       "  \u001b[33mTrainingStep\u001b[39m(\n",
       "    loss = \u001b[33mLoss\u001b[39m(\n",
       "      trainingLoss = \u001b[32m4.527096509933472\u001b[39m,\n",
       "..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainingSteps = trainModelSimple(model, device, trainingLoader, validationLoader, optimizer, epochsCount, modelEvaluator, sampleGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "841174e5-a3a8-4b1e-a387-e2e7fd581e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib==3.9.* in /usr/local/lib/python3.12/site-packages (3.9.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/site-packages (from matplotlib==3.9.*) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/site-packages (from matplotlib==3.9.*) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/site-packages (from matplotlib==3.9.*) (4.55.8)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/site-packages (from matplotlib==3.9.*) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/site-packages (from matplotlib==3.9.*) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/site-packages (from matplotlib==3.9.*) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/site-packages (from matplotlib==3.9.*) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/site-packages (from matplotlib==3.9.*) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/site-packages (from matplotlib==3.9.*) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib==3.9.*) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "Magic.!(\"pip\", \"install\", \"matplotlib==3.9.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1abe3fc3-3f07-427e-918a-20cf3b092f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling /workspace/DisplaySupport.sc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$\u001b[39m"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $file.^.DisplaySupport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1999f05c-ad20-4e53-ac9d-80c1d3f2af3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mplot\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mModule\u001b[39m = <module 'matplotlib.pyplot' from '/usr/local/lib/python3.12/site-packages/matplotlib/pyplot.py'>\n",
       "\u001b[36mticker\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mModule\u001b[39m = <module 'matplotlib.ticker' from '/usr/local/lib/python3.12/site-packages/matplotlib/ticker.py'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val plot = py.module(\"matplotlib.pyplot\")\n",
    "val ticker = py.module(\"matplotlib.ticker\")\n",
    "\n",
    "try {\n",
    "  type Figure = py.Dynamic\n",
    "  type Axis = py.Dynamic\n",
    "    \n",
    "  val (tokensSeen, trainingLoss, validationLoss) = trainingSteps.map {\n",
    "    case TrainingStep(Loss(trainingLoss, validationLoss), tokensSeen) => (tokensSeen, trainingLoss, validationLoss)\n",
    "  }.unzip3\n",
    "  val epochs = torch.linspace(0, epochsCount, tokensSeen.length)\n",
    "  val (figure, axis1) = plot.subplots(figsize = (5, 3)).as[(Figure, Axis)]\n",
    "  axis1.plot(epochs, trainingLoss.toPythonProxy, label = \"Training loss\")\n",
    "  axis1.plot(epochs, validationLoss.toPythonProxy, linestyle = \"-.\", label = \"Validation loss\")\n",
    "  axis1.set_xlabel(\"Epochs\")\n",
    "  axis1.set_ylabel(\"Loss\")\n",
    "  axis1.legend(loc = \"upper right\")\n",
    "  axis1.xaxis.set_major_locator(ticker.MaxNLocator(integer = true))\n",
    "  val axis2 = axis1.twiny()\n",
    "  axis2.plot(tokensSeen.toPythonProxy, trainingLoss.toPythonProxy, alpha = 0)\n",
    "  axis2.set_xlabel(\"Tokens seen\")\n",
    "  figure.tight_layout()\n",
    "  DisplaySupport.showPlot(plot)\n",
    "} catch {\n",
    "  case e: py.PythonException =>\n",
    "    println(\"(!) If the exception below says 'Numpy is not available', restart the Jupyter kernel. It's an issue with Matplotlib in Jupyter.\\n\")\n",
    "    throw e\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e0546bb-fbdf-4a2e-b39d-8480ead2bcfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres29\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = GPTModel(\n",
       "  (tokenEmbeddingLayer): Embedding(50257, 768)\n",
       "  (positionEmbeddingLayer): Embedding(256, 768)\n",
       "  (dropoutEmbeddingLayer): Dropout(p=0.1, inplace=False)\n",
       "  (transformerBlocksLayer): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (normalization1): NormalizationLayer()\n",
       "      (normalization2): NormalizationLayer()\n",
       "      (dropoutShortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b94bab5-1181-423b-b87b-f7b46314d2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mgenerateText\u001b[39m"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generateText(\n",
    "  model: Model,\n",
    "  maxNewTokens: Int,\n",
    "  contextLength: Int,\n",
    "  temperature: Double,\n",
    "  topK: Option[Int] = None\n",
    ")(\n",
    "  encodedInput: TorchTensor\n",
    "): TorchTensor =\n",
    "  LazyList.iterate(encodedInput) { currentEncodedOutput =>\n",
    "    val croppedInput = py\"$currentEncodedOutput[:, -$contextLength:]\"\n",
    "    val logits = py.`with`(torch.no_grad()) { _ =>\n",
    "      model(croppedInput)\n",
    "    }\n",
    "    py\"$logits[:, -1, :]\"\n",
    "      .pipe { logits =>\n",
    "        topK match {\n",
    "          case Some(topK) =>\n",
    "            val (topLogits, _) = torch.topk(logits, topK).as[(TorchTensor, TorchTensor)]\n",
    "            val minValue = py\"$topLogits[:, -1]\"\n",
    "            torch.where(\n",
    "              py\"$logits < $minValue\",\n",
    "              torch.tensor(-torch.inf).to(logits.device),\n",
    "              logits\n",
    "            )\n",
    "          case None => logits\n",
    "        }\n",
    "      }\n",
    "      .pipe { logits =>\n",
    "        if (temperature > 0)\n",
    "          logits\n",
    "            .pipe(logits => py\"$logits / $temperature\")\n",
    "            .pipe(torch.softmax(_, dim = -1))\n",
    "            .pipe(torch.multinomial(_, num_samples = 1))\n",
    "        else \n",
    "          logits\n",
    "            .pipe(torch.argmax(_, dim = -1, keepdim = true))\n",
    "      }\n",
    "      .pipe(nextEncodedOutput => torch.cat((currentEncodedOutput, nextEncodedOutput), dim = 1))\n",
    "  }.drop(maxNewTokens).head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc821cae-356e-4eb1-9331-92bb475b3cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text: Every effort moves you?\" I meant to surprise, for he answered with a deprecating laugh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres31_0\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch._C.Generator object at 0xffff64b4a7b0>\n",
       "\u001b[36mexampleText\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Every effort moves you\"\u001b[39m\n",
       "\u001b[36moutputTextIds\u001b[39m: \u001b[32mTorchTensor\u001b[39m = tensor([[6109, 3626, 6100,  345, 1701,  314, 4001,  284, 5975,   11,  329,  339,\n",
       "         9373,  351,  257, 1207, 8344,  803, 6487]])\n",
       "\u001b[36mdecodedOutputText\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Every effort moves you?\\\" I meant to surprise, for he answered with a deprecating laugh\"\u001b[39m"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "val exampleText = \"Every effort moves you\"\n",
    "val outputTextIds = generateText(\n",
    "  model = model, \n",
    "  maxNewTokens = 15,\n",
    "  contextLength = gptConfig.contextLength,\n",
    "  temperature = 1.4,\n",
    "  topK = Some(25)\n",
    ")(\n",
    "  encodedInput = textToTokenIds(exampleText, tokenizer)\n",
    ")\n",
    "val decodedOutputText = tokenIdsToText(outputTextIds, tokenizer)\n",
    "println(s\"Output text: $decodedOutputText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f269b61d-71b7-4ba4-b7dc-2226a8a6b6f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmodelStateKey\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"model\"\u001b[39m\n",
       "\u001b[36moptimizerStateKey\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"optimizer\"\u001b[39m\n",
       "\u001b[36mstatesMap\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = {'model': OrderedDict({'tokenEmbeddingLayer.weight': tensor([[ 1.1318, -1.0575,  0.2020,  ..., -1.1474, -1.1797,  0.3015],\n",
       "        [ 0.0191, -0.6840, -0.2143,  ...,  1.5910,  0.2862, -1.1013],\n",
       "        [-0.6644, -1.4701,  0.3903,  ..., -1.1785,  0.3644, -0.4615],\n",
       "        ...,\n",
       "        [ 0.2090,  1.9848,  1.5054,  ...,  0.1136,  0.1309,  0.2593],\n",
       "        [ 1.0055, -1.2059,  1.8960,  ..., -1.1787, -0.8087, -0.0541],\n",
       "        [-0.4318,  1.2109, -0.4254,  ...,  0.3160,  0.9786,  0.5312]]), 'positionEmbeddingLayer.weight': tensor([[-0.2724,  0.4232,  2.8003,  ...,  1.2376,  0.9877,  0.8252],\n",
       "        [-1.9044, -0.7268,  0.5185,  ...,  0.1794, -0.0683,  0.1098],\n",
       "        [-1.2739, -0.1384, -1.1305,  ..., -0.8418,  1.3725,  0.4875],\n",
       "        ...,\n",
       "        [-0.2139,  0.3632,  0.8600,  ..., -0.2214,  0.7290, -0.6409],\n",
       "        [-1.4700, -1.3954,  1.6163,  ..., -0.3895,  0.1911,  1.3728],\n",
       "        [-1.5702,  0.5587,  1.7373,  ..., -1.1094, -0.2214, -1.0023]]), 'transformerBlocksLayer.0.multiHeadAttention.mask': tensor([[0., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'transformerBlocksLayer.0.multiHeadAttention.weightsQuery.weight': tensor([[-1.1463e-02,  1.7480e-03, -1.8711e-02,  ...,  3.4494e-02,\n",
       "          2.1541e-02,  8.4472e-03],\n",
       "        [-1.2598e-02, -1.9503e-02, -1.4481e-02,  ...,  5.5506e-03,\n",
       "          3.3536e-03,  2.0759e-02],\n",
       "        [ 1.1155e-02, -1.0575e-02, -2.3106e-03,  ...,  4.2440e-02,\n",
       "          5.4993e-03, -2.3092e-03],\n",
       "        ...,\n",
       "        [ 2.8657e-02, -2.6470e-02, -2.4321e-05,  ..., -1.1274e-02,\n",
       "         -4.2524e-03, -2.4981e-02],\n",
       "        [ 2.0739e-02, -2.2790e-02, -2.7510e-02,  ..., -1.0917e-02,\n",
       "          1.9160e-02, -1.2060e-02],\n",
       "        [ 2.0135e-02,  8.7224e-03,  1.1067e-02,  ...,  2.8885e-02,\n",
       "          3.9478e-02,  1.2743e-02]]), 'transformerBlocksLayer.0.multiHeadAttention.weightsKey.weight': tensor([[ 0.0104, -0.0087,  0.0260,  ...,  0.0129, -0.004...\n",
       "\u001b[36mres32_3\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = None"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val modelStateKey = \"model\"\n",
    "val optimizerStateKey = \"optimizer\"\n",
    "val statesMap = py\"{$modelStateKey: ${model.state_dict()}, $optimizerStateKey: ${optimizer.state_dict()}}\"\n",
    "torch.save(statesMap, \"model_and_optimizer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4edcb189-d4ed-494b-94c7-44cd60b22b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcheckpoint\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = {'model': OrderedDict({'tokenEmbeddingLayer.weight': tensor([[ 1.1318, -1.0575,  0.2020,  ..., -1.1474, -1.1797,  0.3015],\n",
       "        [ 0.0191, -0.6840, -0.2143,  ...,  1.5910,  0.2862, -1.1013],\n",
       "        [-0.6644, -1.4701,  0.3903,  ..., -1.1785,  0.3644, -0.4615],\n",
       "        ...,\n",
       "        [ 0.2090,  1.9848,  1.5054,  ...,  0.1136,  0.1309,  0.2593],\n",
       "        [ 1.0055, -1.2059,  1.8960,  ..., -1.1787, -0.8087, -0.0541],\n",
       "        [-0.4318,  1.2109, -0.4254,  ...,  0.3160,  0.9786,  0.5312]]), 'positionEmbeddingLayer.weight': tensor([[-0.2724,  0.4232,  2.8003,  ...,  1.2376,  0.9877,  0.8252],\n",
       "        [-1.9044, -0.7268,  0.5185,  ...,  0.1794, -0.0683,  0.1098],\n",
       "        [-1.2739, -0.1384, -1.1305,  ..., -0.8418,  1.3725,  0.4875],\n",
       "        ...,\n",
       "        [-0.2139,  0.3632,  0.8600,  ..., -0.2214,  0.7290, -0.6409],\n",
       "        [-1.4700, -1.3954,  1.6163,  ..., -0.3895,  0.1911,  1.3728],\n",
       "        [-1.5702,  0.5587,  1.7373,  ..., -1.1094, -0.2214, -1.0023]]), 'transformerBlocksLayer.0.multiHeadAttention.mask': tensor([[0., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'transformerBlocksLayer.0.multiHeadAttention.weightsQuery.weight': tensor([[-1.1463e-02,  1.7480e-03, -1.8711e-02,  ...,  3.4494e-02,\n",
       "          2.1541e-02,  8.4472e-03],\n",
       "        [-1.2598e-02, -1.9503e-02, -1.4481e-02,  ...,  5.5506e-03,\n",
       "          3.3536e-03,  2.0759e-02],\n",
       "        [ 1.1155e-02, -1.0575e-02, -2.3106e-03,  ...,  4.2440e-02,\n",
       "          5.4993e-03, -2.3092e-03],\n",
       "        ...,\n",
       "        [ 2.8657e-02, -2.6470e-02, -2.4321e-05,  ..., -1.1274e-02,\n",
       "         -4.2524e-03, -2.4981e-02],\n",
       "        [ 2.0739e-02, -2.2790e-02, -2.7510e-02,  ..., -1.0917e-02,\n",
       "          1.9160e-02, -1.2060e-02],\n",
       "        [ 2.0135e-02,  8.7224e-03,  1.1067e-02,  ...,  2.8885e-02,\n",
       "          3.9478e-02,  1.2743e-02]]), 'transformerBlocksLayer.0.multiHeadAttention.weightsKey.weight': tensor([[ 0.0104, -0.0087,  0.0260,  ...,  0.0129, -0.004...\n",
       "\u001b[36mnewModel\u001b[39m: \u001b[32mModel\u001b[39m = GPTModel(\n",
       "  (tokenEmbeddingLayer): Embedding(50257, 768)\n",
       "  (positionEmbeddingLayer): Embedding(256, 768)\n",
       "  (dropoutEmbeddingLayer): Dropout(p=0.1, inplace=False)\n",
       "  (transformerBlocksLayer): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (normalization1): NormalizationLayer()\n",
       "      (normalization2): NormalizationLayer()\n",
       "      (dropoutShortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (weightsQuery): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsKey): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (weightsValue): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (outputProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feedForward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "...\n",
       "\u001b[36mres33_2\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <All keys matched successfully>\n",
       "\u001b[36mnewOptimizer\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.0004\n",
       "    maximize: False\n",
       "    weight_decay: 0.1\n",
       ")\n",
       "\u001b[36mres33_4\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = None"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val checkpoint = torch.load(\"model_and_optimizer.pth\", map_location = device)\n",
    "val newModel = GPTModel(gptConfig)\n",
    "newModel.load_state_dict(checkpoint.bracketAccess(modelStateKey))\n",
    "val newOptimizer = torch.optim.AdamW(newModel.parameters(), lr = 0.0004, weight_decay = 0.1)\n",
    "newOptimizer.load_state_dict(checkpoint.bracketAccess(optimizerStateKey))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263e67e7-43a5-41b0-9585-7f5baa250e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Epoch 1\n"
     ]
    }
   ],
   "source": [
    "// Exercise 5.4\n",
    "trainModelSimple(newModel, device, trainingLoader, validationLoader, newOptimizer, epochsCount = 1, modelEvaluator, sampleGenerator)\n",
    "\n",
    "val exampleText = \"Every effort moves you\"\n",
    "val outputTextIds = generateText(\n",
    "  model = newModel, \n",
    "  maxNewTokens = 15,\n",
    "  contextLength = gptConfig.contextLength,\n",
    "  temperature = 1.4,\n",
    "  topK = Some(25)\n",
    ")(\n",
    "  encodedInput = textToTokenIds(exampleText, tokenizer)\n",
    ")\n",
    "val decodedOutputText = tokenIdsToText(outputTextIds, tokenizer)\n",
    "println(s\"Output text: $decodedOutputText\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.13.14",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
