{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dcbe069-f199-45af-99a0-57bfa2df6e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $file.^.Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd9bd9ae-61c1-403a-9871-fb89a11647bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.4.* in /usr/local/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch==2.4.*) (75.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->torch==2.4.*) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/site-packages (from sympy->torch==2.4.*) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken==0.7.* in /usr/local/lib/python3.12/site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/site-packages (from tiktoken==0.7.*) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/site-packages (from tiktoken==0.7.*) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.7.*) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.7.*) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.7.*) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.7.*) (2024.12.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "Magic.!(\"pip\", \"install\", \"torch==2.4.*\")\n",
    "Magic.!(\"pip\", \"install\", \"tiktoken==0.7.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34306985-2652-45df-b0ce-bd3c1a880e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mme.shadaj.scalapy.py\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mpy.SeqConverters\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mpy.PyQuote\u001b[39m\n",
       "\u001b[36mtorch\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mModule\u001b[39m = <module 'torch' from '/usr/local/lib/python3.12/site-packages/torch/__init__.py'>\n",
       "\u001b[36mtiktoken\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mModule\u001b[39m = <module 'tiktoken' from '/usr/local/lib/python3.12/site-packages/tiktoken/__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`dev.scalapy::scalapy-core:0.5.3`\n",
    "\n",
    "import me.shadaj.scalapy.py\n",
    "import py.SeqConverters\n",
    "import py.PyQuote\n",
    "\n",
    "val torch = py.module(\"torch\")\n",
    "val tiktoken = py.module(\"tiktoken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "359a8e96-546e-476e-b635-3ade29735b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mGPTConfig\u001b[39m\n",
       "\u001b[36mgptConfig\u001b[39m: \u001b[32mGPTConfig\u001b[39m = \u001b[33mGPTConfig\u001b[39m(\n",
       "  vocabularySize = \u001b[32m50257\u001b[39m,\n",
       "  contextLength = \u001b[32m256\u001b[39m,\n",
       "  embeddingDimension = \u001b[32m768\u001b[39m,\n",
       "  attentionHeadsCount = \u001b[32m12\u001b[39m,\n",
       "  layersCount = \u001b[32m12\u001b[39m,\n",
       "  dropoutRate = \u001b[32m0.1\u001b[39m,\n",
       "  queryKeyValueBias = \u001b[32mfalse\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class GPTConfig(\n",
    "  vocabularySize: Int,\n",
    "  contextLength: Int,\n",
    "  embeddingDimension: Int,\n",
    "  attentionHeadsCount: Int,\n",
    "  layersCount: Int,\n",
    "  dropoutRate: Double,\n",
    "  queryKeyValueBias: Boolean\n",
    ")\n",
    "\n",
    "val gptConfig = GPTConfig(\n",
    "  vocabularySize = 50_257,\n",
    "  contextLength = 256,\n",
    "  embeddingDimension = 768,\n",
    "  attentionHeadsCount = 12,\n",
    "  layersCount = 12,\n",
    "  dropoutRate = 0.1,\n",
    "  queryKeyValueBias = false\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30b2b4e1-5ddb-44a2-9acc-a304ce5c0e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mtype\u001b[39m \u001b[36mTorchTensor\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mMultiHeadAttention\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class MultiHeadAttention(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "type TorchTensor = py.Dynamic\n",
    "def MultiHeadAttention(\n",
    "  inputDimension: Int,\n",
    "  outputDimension: Int,\n",
    "  dropoutProbability: Double,\n",
    "  contextLength: Int,\n",
    "  headsCount: Int,\n",
    "  queryKeyValueBias: Boolean\n",
    "): py.Dynamic = {\n",
    "  assert(outputDimension % headsCount == 0, \"Output dimension must be a multiple of heads count\")\n",
    "  val headDimension = outputDimension / headsCount\n",
    "    \n",
    "  val weightsQuery = torch.nn.Linear(inputDimension, outputDimension, bias = queryKeyValueBias)\n",
    "  val weightsKey = torch.nn.Linear(inputDimension, outputDimension, bias = queryKeyValueBias)\n",
    "  val weightsValue = torch.nn.Linear(inputDimension, outputDimension, bias = queryKeyValueBias)\n",
    "  val outputProjection = torch.nn.Linear(outputDimension, outputDimension)\n",
    "  val dropout = torch.nn.Dropout(dropoutProbability)\n",
    "    \n",
    "  val init = (self: py.Dynamic) => {\n",
    "    self.register_buffer(\"mask\", torch.triu(torch.ones(contextLength, contextLength), diagonal = 1))\n",
    "    val mask = self.mask\n",
    "      \n",
    "    val forward = (batchedInputs: TorchTensor) => {\n",
    "      val (batchesCount, tokensCount, tokenDimension) = batchedInputs.shape.as[(Int, Int, Int)]\n",
    "      val queries = weightsQuery(batchedInputs)\n",
    "        .view(batchesCount, tokensCount, headsCount, headDimension)\n",
    "        .transpose(1, 2)\n",
    "      val keys = weightsKey(batchedInputs)\n",
    "        .view(batchesCount, tokensCount, headsCount, headDimension)\n",
    "        .transpose(1, 2)\n",
    "      val values = weightsValue(batchedInputs)\n",
    "        .view(batchesCount, tokensCount, headsCount, headDimension)\n",
    "        .transpose(1, 2)\n",
    "      val attentionScores = py\"$queries @ $keys.transpose(2, 3)\"\n",
    "      attentionScores.masked_fill_(py\"$mask.bool()[:$tokensCount, :$tokensCount]\", -torch.inf)\n",
    "      val attentionWeights = dropout(torch.softmax(py\"$attentionScores / $outputDimension**0.5\", dim = -1))\n",
    "      outputProjection(\n",
    "        py\"$attentionWeights @ $values\"\n",
    "          .transpose(1, 2)\n",
    "          .contiguous()\n",
    "          .view(batchesCount, tokensCount, outputDimension)\n",
    "      )\n",
    "    }\n",
    "    py.Dynamic.global.MultiHeadAttention.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.MultiHeadAttention(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a52671-21de-47de-87a0-4894d7c2a0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mGELU\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "// Because it mostly uses Python operators, it's implemented fully in Python\n",
    "py.exec {\n",
    "  s\"\"\"import torch\n",
    "     |import torch.nn as nn\n",
    "     |\n",
    "     |class GELU(nn.Module):\n",
    "     |  def __init__(self):\n",
    "     |    super().__init__()\n",
    "     |\n",
    "     |  def forward(self, inputs):\n",
    "     |    return 0.5 * inputs * (\n",
    "     |      1 + torch.tanh(\n",
    "     |        torch.sqrt(torch.tensor(2.0 / torch.pi)) * (inputs + 0.044715 * torch.pow(inputs, 3))\n",
    "     |      )\n",
    "     |    )\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def GELU() = py.Dynamic.global.GELU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7bb1c31-b837-4a87-acb5-fda694b34cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mFeedForward\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class FeedForward(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def FeedForward(\n",
    "  embeddingDimension: Int\n",
    "): py.Dynamic = {\n",
    "  val layers = torch.nn.Sequential(\n",
    "    torch.nn.Linear(embeddingDimension, 4 * embeddingDimension),\n",
    "    GELU(),\n",
    "    torch.nn.Linear(4 * embeddingDimension, embeddingDimension)\n",
    "  )\n",
    "  val init = (self: py.Dynamic) => {\n",
    "    val forward = (inputs: TorchTensor) => layers(inputs)\n",
    "    py.Dynamic.global.FeedForward.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.FeedForward(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a379f7b-a8e9-42a3-99c6-7fa0dc23faab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mNormalizationLayer\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class NormalizationLayer(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def NormalizationLayer(\n",
    "  embeddingDimension: Int\n",
    "): py.Dynamic = {\n",
    "  val epsilon = 1e-5\n",
    "  val scale = torch.nn.Parameter(torch.ones(embeddingDimension))\n",
    "  val shift = torch.nn.Parameter(torch.zeros(embeddingDimension))\n",
    "  val init = (self: py.Dynamic) => {\n",
    "    val forward = (inputs: TorchTensor) => {\n",
    "      val mean = inputs.mean(dim = -1, keepdim = true)\n",
    "      val variance = inputs.`var`(dim = -1, keepdim = true, unbiased = false)\n",
    "      val normalizedInputs = py\"($inputs - $mean) / torch.sqrt($variance + $epsilon)\"\n",
    "      py\"$scale * $normalizedInputs + $shift\"\n",
    "    }\n",
    "    py.Dynamic.global.NormalizationLayer.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.NormalizationLayer(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d3fab4f-b15a-4795-a3a6-a3f9e4546839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.chaining._\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mTransformerBlock\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.chaining._\n",
    "\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class TransformerBlock(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def TransformerBlock(\n",
    "  config: GPTConfig\n",
    "): py.Dynamic = {\n",
    "  val multiHeadAttention = MultiHeadAttention(\n",
    "    inputDimension = config.embeddingDimension,\n",
    "    outputDimension = config.embeddingDimension,\n",
    "    dropoutProbability = config.dropoutRate,\n",
    "    contextLength = config.contextLength,\n",
    "    headsCount = config.attentionHeadsCount,\n",
    "    queryKeyValueBias = config.queryKeyValueBias\n",
    "  )\n",
    "  val feedForward = FeedForward(config.embeddingDimension)\n",
    "  val normalization1 = NormalizationLayer(config.embeddingDimension)\n",
    "  val normalization2 = NormalizationLayer(config.embeddingDimension)\n",
    "  val dropoutShortcut = torch.nn.Dropout(config.dropoutRate)\n",
    "  val init = (self: py.Dynamic) => {\n",
    "    val forward = (inputs: TorchTensor) => {\n",
    "      val shortcut = inputs\n",
    "      val newShortcut = inputs\n",
    "        .pipe(normalization1(_))\n",
    "        .pipe(multiHeadAttention(_))\n",
    "        .pipe(dropoutShortcut(_))\n",
    "        .pipe(o => py\"$o + $shortcut\")\n",
    "      newShortcut\n",
    "        .pipe(normalization2(_))\n",
    "        .pipe(feedForward(_))\n",
    "        .pipe(dropoutShortcut(_))\n",
    "        .pipe(o => py\"$o + $newShortcut\")\n",
    "    }\n",
    "    py.Dynamic.global.TransformerBlock.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.TransformerBlock(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f56aad56-185d-49ff-a49c-37f029462671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mtype\u001b[39m \u001b[36mModel\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mGPTModel\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"import torch.nn as nn\n",
    "     |\n",
    "     |class GPTModel(nn.Module):\n",
    "     |  def __init__(self, init):\n",
    "     |    super().__init__()\n",
    "     |    init(self)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "type Model = py.Dynamic\n",
    "def GPTModel(\n",
    "  config: GPTConfig\n",
    "): Model = {\n",
    "  val tokenEmbeddingLayer = torch.nn.Embedding(config.vocabularySize, config.embeddingDimension)\n",
    "  val positionEmbeddingLayer = torch.nn.Embedding(config.contextLength, config.embeddingDimension)\n",
    "  val dropoutEmbeddingLayer = torch.nn.Dropout(config.dropoutRate)\n",
    "  val transformerBlocks = Seq.fill(config.layersCount)(TransformerBlock(config))\n",
    "  val transformerBlocksLayer = py\"nn.Sequential(*${transformerBlocks.toPythonProxy})\"\n",
    "  val finalNormalizationLayer = NormalizationLayer(config.embeddingDimension)\n",
    "  val outputLayer = torch.nn.Linear(config.embeddingDimension, config.vocabularySize, bias = false)\n",
    "  val init = (self: py.Dynamic) => {\n",
    "    val forward = (batchedInputs: TorchTensor) => {\n",
    "      val (_, sequenceLength) = batchedInputs.shape.as[(Int, Int)]\n",
    "      val tokenEmbeddings = tokenEmbeddingLayer(batchedInputs)\n",
    "      val positionEmbeddings = positionEmbeddingLayer(torch.arange(sequenceLength, device = batchedInputs.device))\n",
    "      py\"$tokenEmbeddings + $positionEmbeddings\"\n",
    "        .pipe(dropoutEmbeddingLayer(_))\n",
    "        .pipe(transformerBlocksLayer(_))\n",
    "        .pipe(finalNormalizationLayer(_))\n",
    "        .pipe(outputLayer(_))\n",
    "    }\n",
    "    py.Dynamic.global.GPTModel.forward = forward\n",
    "  }\n",
    "  py.Dynamic.global.GPTModel(init)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb87a3fa-e035-468c-83c2-6bcb13572f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres11_0\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <torch._C.Generator object at 0xfffea4151370>\n",
       "\u001b[36mmodel\u001b[39m: \u001b[32mModel\u001b[39m = GPTModel()\n",
       "\u001b[36mres11_2\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = GPTModel()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "val model = GPTModel(gptConfig)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1044c6be-1d1c-43d6-8c03-573e6b67b7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mgenerateTextSimple\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generateTextSimple(\n",
    "  model: Model,\n",
    "  maxNewTokens: Int,\n",
    "  contextLength: Int\n",
    ")(\n",
    "  encodedInput: TorchTensor\n",
    "): TorchTensor =\n",
    "  LazyList.iterate(encodedInput) { currentEncodedOutput =>\n",
    "    val croppedInput = py\"$currentEncodedOutput[:, -$contextLength:]\"\n",
    "    val logits = py.`with`(torch.no_grad()) { _ =>\n",
    "      model(croppedInput)\n",
    "    }\n",
    "    py\"$logits[:, -1, :]\"\n",
    "      .pipe(torch.softmax(_, dim = -1))\n",
    "      .pipe(torch.argmax(_, dim = -1, keepdim = true))\n",
    "      .pipe(nextEncodedOutput => torch.cat((currentEncodedOutput, nextEncodedOutput), dim = 1))\n",
    "  }.drop(maxNewTokens).head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e439f782-3376-4acb-91d7-097b75bae34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mtype\u001b[39m \u001b[36mTokenizer\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtextToTokenIds\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtokenIdsToText\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type Tokenizer = py.Dynamic\n",
    "def textToTokenIds(\n",
    "  text: String, \n",
    "  tokenizer: Tokenizer\n",
    "): TorchTensor = {\n",
    "  val allowedSpecial = py.Dynamic.global.set(Seq(\"<|endoftext|>\").toPythonProxy)\n",
    "  val encodedText = tokenizer.encode(text, allowed_special = allowedSpecial)\n",
    "  torch.tensor(encodedText).unsqueeze(0)\n",
    "}\n",
    "    \n",
    "def tokenIdsToText(\n",
    "  tokenIds: TorchTensor, \n",
    "  tokenizer: Tokenizer\n",
    "): String =\n",
    "  tokenizer.decode(tokenIds.squeeze(0).tolist()).as[String]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8be564df-4a19-4374-87cd-0a6b0a74d6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text: Every effort moves you haw questioning + Vaj370ILSnorth degener dynamicistle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtokenizer\u001b[39m: \u001b[32mpy\u001b[39m.\u001b[32mDynamic\u001b[39m = <Encoding 'gpt2'>\n",
       "\u001b[36mexampleText\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Every effort moves you\"\u001b[39m\n",
       "\u001b[36moutputTextIds\u001b[39m: \u001b[32mTorchTensor\u001b[39m = tensor([[ 6109,  3626,  6100,   345, 23185, 14085,  1343, 39838, 20167, 45484,\n",
       "         43588, 25419,  8925, 12535]])\n",
       "\u001b[36mdecodedOutputText\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Every effort moves you haw questioning + Vaj370ILSnorth degener dynamicistle\"\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "val exampleText = \"Every effort moves you\"\n",
    "val outputTextIds = generateTextSimple(\n",
    "  model = model, \n",
    "  maxNewTokens = 10, \n",
    "  contextLength = gptConfig.contextLength\n",
    ")(\n",
    "  encodedInput = textToTokenIds(exampleText, tokenizer)\n",
    ")\n",
    "val decodedOutputText = tokenIdsToText(outputTextIds, tokenizer)\n",
    "println(s\"Output text: $decodedOutputText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa57b54b-f143-4435-900d-63e2e938455d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mscala.io.Source\u001b[39m\n",
       "\u001b[36mfilePath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"data/the_verdict.txt\"\u001b[39m\n",
       "\u001b[36mtextData\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
       "\n",
       "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\n",
       "\n",
       "Well!--even through the prism of Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own trade hardly a murmur. Professional jealousy? Perhaps. If it were, the honour of the craft was vindicated by little Claude Nutley, who, in all good faith, brought out in the Burlington a very handsome \"obituary\" on Jack--one of those showy articles stocked with random technicalities that I have heard (I won't say by whom) compared to Gisburn's painting. And so--his resolve being apparently irrevocable--the discussion gradually died out, and, as Mrs. Thwing had predicted, the price of \"Gisburns\" went up.\n",
       "\n",
       "It was not till three years later that, in the course of a few weeks' idling on the Riviera, it suddenly occurred to me to wonder why Gisburn had given up his painting. On reflection, it really was a tempting problem. To accuse his wife would have been too easy--his fair sitters had been denied the solace of saying that Mrs. Gisburn had \"dragged him down.\" For Mrs. Gisburn--as such--had not existed till nearly a year after Jack's resolve had been taken. It might be that he had married her--since he liked his ease--because he didn't want to go on painting; but it would have been hard to prove that he had given up his painting because he had married her.\n",
       "\n",
       "Of course, if she had not dragged him down, she had equally, as Miss Croft contended, failed to \"lift him up\"--she had not led him back to the easel. To put the\u001b[39m..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.io.Source\n",
    "\n",
    "val filePath = \"data/the_verdict.txt\"\n",
    "val textData = Source.fromFile(filePath).mkString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ba4355d-7766-428d-8085-54b343663c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: $totalCharacters\n",
      "Tokens: $totalTokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtotalCharacters\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m20479\u001b[39m\n",
       "\u001b[36mtotalTokens\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m5145\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val totalCharacters = textData.length\n",
    "val totalTokens = tokenizer.encode(textData).as[Vector[Int]].length\n",
    "println(\"Characters: $totalCharacters\")\n",
    "println(\"Tokens: $totalTokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfecd895-25e9-487d-b30b-eef2c466c33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 18431\n",
      "Validation data size: 2048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainingRatio\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.9\u001b[39m\n",
       "\u001b[36msplitIndex\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m18431\u001b[39m\n",
       "\u001b[36mtrainingData\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
       "\n",
       "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\n",
       "\n",
       "Well!--even through the prism of Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own trade hardly a murmur. Professional jealousy? Perhaps. If it were, the honour of the craft was vindicated by little Claude Nutley, who, in all good faith, brought out in the Burlington a very handsome \"obituary\" on Jack--one of those showy articles stocked with random technicalities that I have heard (I won't say by whom) compared to Gisburn's painting. And so--his resolve being apparently irrevocable--the discussion gradually died out, and, as Mrs. Thwing had predicted, the price of \"Gisburns\" went up.\n",
       "\n",
       "It was not till three years later that, in the course of a few weeks' idling on the Riviera, it suddenly occurred to me to wonder why Gisburn had given up his painting. On reflection, it really was a tempting problem. To accuse his wife would have been too easy--his fair sitters had been denied the solace of saying that Mrs. Gisburn had \"dragged him down.\" For Mrs. Gisburn--as such--had not existed till nearly a year after Jack's resolve had been taken. It might be that he had married her--since he liked his ease--because he didn't want to go on painting; but it would have been hard to prove that he had given up his painting because he had married her.\n",
       "\n",
       "Of course, if she had not dragged him down, she had equally, as Miss Croft contended, failed to \"lift him up\"--she had not led him back to the easel. To put the\u001b[39m...\n",
       "\u001b[36mvalidationData\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"ue' collapsed like a house of cards. He didn't sneer, you understand, poor Stroud--he just lay there quietly watching, and on his lips, through the gray beard, I seemed to hear the question: 'Are you sure you know where you're coming out?'\n",
       "\n",
       "\"If I could have painted that face, with that question on it, I should have done a great thing. The next greatest thing was to see that I couldn't--and that grace was given me. But, oh, at that minute, Rickham, was there anything on earth I wouldn't have given to have Stroud alive before me, and to hear him say: 'It's not too late--I'll show you how'?\n",
       "\n",
       "\"It _was_ too late--it would have been, even if he'd been alive. I packed up my traps, and went down and told Mrs. Stroud. Of course I didn't tell her _that_--it would have been Greek to her. I simply said I couldn't paint him, that I was too moved. She rather liked the idea--she's so romantic! It was that that made her give me the donkey. But she was terribly upset at not getting the portrait--she did so want him 'done' by some one showy! At first I was afraid she wouldn't let me off--and at my wits' end I suggested Grindle. Yes, it was I who started Grindle: I told Mrs. Stroud he was the 'coming' man, and she told somebody else, and so it got to be true. . . . And he painted Stroud without wincing; and she hung the picture among her husband's things. . . .\"\n",
       "\n",
       "He flung himself down in the arm-chair near mine, laid back his head, and clasping his arms beneath it, looked up at the picture above the chimney-piece.\n",
       "\n",
       "\"I like to fancy that Stroud himself would have given it to me, if he'd been able to say what he thought that day.\"\n",
       "\n",
       "And, in answer to a question I put half-mechanically--\"Begin again?\" he flashed out. \"When the one thing that brings me anywhere near him is that I knew enough to leave off?\"\n",
       "\n",
       "He stood up and laid his hand on my shoulder with a laugh. \"Only the irony of it is that I _am_ still painting--since Grindle's doing it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\"\"\"\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainingRatio = 0.9\n",
    "val splitIndex = (totalCharacters * trainingRatio).toInt\n",
    "val (trainingData, validationData) = textData.splitAt(splitIndex)\n",
    "println(s\"Training data size: ${trainingData.size}\")\n",
    "println(s\"Validation data size: ${validationData.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e39f143b-4789-4a0e-b4ae-126ab1a745c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mGPTDatasetV1\u001b[39m\n",
       "defined \u001b[32mtype\u001b[39m \u001b[36mDataLoader\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcreateDataLoaderV1\u001b[39m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Workaround to define a class that inherits from a Python class\n",
    "py.exec {\n",
    "  s\"\"\"from torch.utils.data import Dataset\n",
    "     |\n",
    "     |class GPTDatasetV1(Dataset):\n",
    "     |  def __init__(self, len, getItem):\n",
    "     |    self.len = len\n",
    "     |    self.getItem = getItem\n",
    "     |  \n",
    "     |  def __len__(self):\n",
    "     |    return self.len()\n",
    "     |\n",
    "     |  def __getitem__(self, index):\n",
    "     |    return self.getItem(index)\n",
    "     |\"\"\".stripMargin\n",
    "}\n",
    "def GPTDatasetV1(\n",
    "  text: String,\n",
    "  tokenizer: Tokenizer,\n",
    "  maxLength: Int,\n",
    "  step: Int\n",
    "): py.Dynamic = {\n",
    "  val tokens = tokenizer.encode(text).as[Vector[Int]]\n",
    "  val (inputTokens, outputTokens) = (0 until tokens.length by step).foldLeft(\n",
    "    (\n",
    "      Vector.empty[TorchTensor], \n",
    "      Vector.empty[TorchTensor]\n",
    "    )\n",
    "  ) {\n",
    "    case ((inputTokens, outputTokens), i) =>\n",
    "      val inputChunk = tokens.slice(i, i + maxLength)\n",
    "      val outputChunk = tokens.slice(i + 1, i + 1 + maxLength)\n",
    "      (\n",
    "       inputTokens :+ torch.tensor(inputChunk.toPythonProxy), \n",
    "       outputTokens :+ torch.tensor(outputChunk.toPythonProxy)\n",
    "      )\n",
    "  }\n",
    "  val len = () => inputTokens.length\n",
    "  val getItem = (index: Int) => (inputTokens(index), outputTokens(index))\n",
    "  py.Dynamic.global.GPTDatasetV1(len, getItem)\n",
    "}\n",
    "\n",
    "type DataLoader = py.Dynamic\n",
    "def createDataLoaderV1(\n",
    "  text: String, \n",
    "  batchSize: Int = 4,\n",
    "  maxLength: Int = 256,                       \n",
    "  step: Int = 128, \n",
    "  shuffle: Boolean = true, \n",
    "  dropLast: Boolean = true,\n",
    "  numWorkers: Int = 0 \n",
    "): DataLoader = {\n",
    "  val tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "  val dataset = GPTDatasetV1(text, tokenizer, maxLength, step)\n",
    "  torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size = batchSize,\n",
    "    shuffle = shuffle,\n",
    "    drop_last = dropLast,\n",
    "    num_workers = numWorkers\n",
    "  )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86347c1c-d8f1-4d39-8eea-57341bd1ae0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loader batches\n"
     ]
    },
    {
     "ename": "me.shadaj.scalapy.py.PythonException",
     "evalue": "Traceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n    data = self._next_data()\n           ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 673, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 317, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 174, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 214, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n<class 'RuntimeError'> stack expects each tensor to be equal size, but got [4] at entry 0 and [256] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31mme.shadaj.scalapy.py.PythonException: Traceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n    data = self._next_data()\n           ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 673, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 317, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 174, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 214, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n<class 'RuntimeError'> stack expects each tensor to be equal size, but got [4] at entry 0 and [256] at entry 1\u001b[39m",
      "  me.shadaj.scalapy.interpreter.CPythonInterpreter$.$anonfun$throwErrorIfOccured$2(\u001b[32mCPythonInterpreter.scala\u001b[39m:\u001b[32m390\u001b[39m)",
      "  me.shadaj.scalapy.interpreter.Platform$.Zone(\u001b[32mPlatform.scala\u001b[39m:\u001b[32m10\u001b[39m)",
      "  me.shadaj.scalapy.interpreter.CPythonInterpreter$.$anonfun$throwErrorIfOccured$1(\u001b[32mCPythonInterpreter.scala\u001b[39m:\u001b[32m362\u001b[39m)",
      "  scala.runtime.java8.JFunction0$mcV$sp.apply(\u001b[32mJFunction0$mcV$sp.scala\u001b[39m:\u001b[32m18\u001b[39m)",
      "  me.shadaj.scalapy.interpreter.CPythonInterpreter$.withGil(\u001b[32mCPythonInterpreter.scala\u001b[39m:\u001b[32m161\u001b[39m)",
      "  me.shadaj.scalapy.interpreter.CPythonInterpreter$.throwErrorIfOccured(\u001b[32mCPythonInterpreter.scala\u001b[39m:\u001b[32m361\u001b[39m)",
      "  me.shadaj.scalapy.interpreter.CPythonInterpreter$.$anonfun$load$2(\u001b[32mCPythonInterpreter.scala\u001b[39m:\u001b[32m400\u001b[39m)",
      "  me.shadaj.scalapy.interpreter.CPythonInterpreter$.withGil(\u001b[32mCPythonInterpreter.scala\u001b[39m:\u001b[32m161\u001b[39m)",
      "  me.shadaj.scalapy.interpreter.CPythonInterpreter$.$anonfun$load$1(\u001b[32mCPythonInterpreter.scala\u001b[39m:\u001b[32m398\u001b[39m)",
      "  me.shadaj.scalapy.interpreter.Platform$.Zone(\u001b[32mPlatform.scala\u001b[39m:\u001b[32m10\u001b[39m)",
      "  me.shadaj.scalapy.interpreter.CPythonInterpreter$.load(\u001b[32mCPythonInterpreter.scala\u001b[39m:\u001b[32m396\u001b[39m)",
      "  me.shadaj.scalapy.py.package$.eval(\u001b[32mpackage.scala\u001b[39m:\u001b[32m53\u001b[39m)",
      "  me.shadaj.scalapy.py.package$PyQuote$.py$extension(\u001b[32mpackage.scala\u001b[39m:\u001b[32m86\u001b[39m)",
      "  ammonite.$sess.cmd23$Helper.printBatchShapes(\u001b[32mcmd23.sc\u001b[39m:\u001b[32m21\u001b[39m)",
      "  ammonite.$sess.cmd23$Helper.<init>(\u001b[32mcmd23.sc\u001b[39m:\u001b[32m27\u001b[39m)",
      "  ammonite.$sess.cmd23$.<clinit>(\u001b[32mcmd23.sc\u001b[39m:\u001b[32m7\u001b[39m)"
     ]
    }
   ],
   "source": [
    "val trainingLoader = createDataLoaderV1(\n",
    "  text = trainingData, \n",
    "  batchSize  = 2, \n",
    "  maxLength = gptConfig.contextLength,                       \n",
    "  step = gptConfig.contextLength, \n",
    "  shuffle = true, \n",
    "  dropLast = true,\n",
    "  numWorkers = 0 \n",
    ")\n",
    "val validationLoader = createDataLoaderV1(\n",
    "  text = validationData, \n",
    "  batchSize  = 2, \n",
    "  maxLength = gptConfig.contextLength,                       \n",
    "  step = gptConfig.contextLength, \n",
    "  shuffle = false, \n",
    "  dropLast = false,\n",
    "  numWorkers = 0 \n",
    ")\n",
    "\n",
    "def printBatchShapes(loader: DataLoader) = {\n",
    "  val loaderBatches = py\"[batch for batch in $loader]\".as[Seq[Seq[TorchTensor]]]\n",
    "  loaderBatches.zipWithIndex.foreach { case (Seq(inputBatch, targetBatch), i) =>\n",
    "    println(s\"Batch $i: ${inputBatch.shape} -> ${targetBatch.shape}\")\n",
    "  }\n",
    "}\n",
    "println(\"Training loader batches\")\n",
    "printBatchShapes(trainingLoader)\n",
    "println()\n",
    "println(\"Validation loader batches\")\n",
    "printBatchShapes(validationLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0335006a-0818-4028-8a2d-e654fc7ea85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type Device = py.Dynamic\n",
    "def calculateBatchLoss(\n",
    "  model: Model,\n",
    "  device: Device\n",
    ")(\n",
    "  inputBatch: TorchTensor,\n",
    "  targetBatch: TorchTensor\n",
    "): TorchTensor = {\n",
    "  val outputs = model(inputBatch.to(device))\n",
    "  torch.nn.functional.cross_entropy(outputs.flatten(0, 1), targetBatch.flatten())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02576724-8ae5-442b-9e00-cfc700eb5590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateDataLoaderLoss(\n",
    "  model: Model,\n",
    "  device: Device\n",
    ")(\n",
    "  dataLoader: DataLoader,\n",
    "  batchesCountOpt: Option[Int] = None\n",
    "): Double = { \n",
    "  val batchesCount = batchesCountOpt match {\n",
    "    case Some(batchesCount) => batchesCount\n",
    "    case None => py\"len($dataLoader)\".as[Int]\n",
    "  }\n",
    "  assert(batchesCount > 0, \"There were no batches to process\")\n",
    "  var totalLoss = 0.0\n",
    "  val batchesIterator = py\"iter($dataLoader)\"\n",
    "  var currentBatch = py\"next($batchesIterator, None)\"\n",
    "  var currentBatchIndex = 0\n",
    "  while (\n",
    "    currentBatchIndex < batchesCount && \n",
    "    currentBatch != py.Dynamic.global.None\n",
    "  ) {\n",
    "    val Seq(inputBatch, targetBatch) = currentBatch.as[Seq[TorchTensor]]\n",
    "    totalLoss += calculateBatchLoss(model, device)(inputBatch, targetBatch).item().as[Double]\n",
    "    currentBatchIndex += 1\n",
    "  }\n",
    "  totalLoss / batchesCount\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c0d33-96c4-46b5-9447-4c7645bb67f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val device = torch.device(if (torch.cuda.is_available().as[Boolean]) \"cuda\" else \"cpu\")\n",
    "model.to(device)\n",
    "py.`with`(torch.no_grad()) { _ =>\n",
    "  val trainingLoss = calculateDataLoaderLoss(model, device)(trainingLoader)\n",
    "  println(s\"Training loss: $trainingLoss\")\n",
    "  val validationLoss = calculateDataLoaderLoss(model, device)(validationLoader)\n",
    "  println(s\"Validation loss: $validationLoss\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c822210c-fa2d-42c6-88a1-08f1361e7512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.13.14",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
